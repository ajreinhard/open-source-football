[
  {
    "path": "posts/2020-08-29-player-density-and-completion-surface-estimates/",
    "title": "Player Density and Completion Surface Estimates",
    "description": "Methods for modeling density estimates and expected completion percentages across the football field for individual players.",
    "author": [
      {
        "name": "Ethan Douglas",
        "url": "https://twitter.com/ChiefsAnalytics"
      }
    ],
    "date": "2021-08-29",
    "categories": [
      "nflfastR",
      "python"
    ],
    "contents": "\nTable of Contents\nDensity Estimates and Expected Completion SurfacesDensity Estimates\nExpected Completion Surfaces\n\nDensity Estimates and Expected Completion Surfaces\nIn this post I will cover\nUsing the scipy library to create your own kernel density estimator\nUsing this estimator to easily compare the densities of two players\nModeling the expected completion % of a pass, and plotting these probabilities as a surface over the field\nModeling the expected completion % of a pass for a particular player or team, and comparing that to the rest of the league\nIn my last post I gave some examples of how you can use the seaborn library in python to plot heat maps of NFL passing locations. For this post I’m going to pick right back up where we left off - performing kernal density estimates (KDEs) with the scipy library rather than relying on the seaborn library. The advantage here is that we can get an estimate of the density of the passes, and then “slice and dice” that estimate however we want, performing calculations on the output. I’ll show you why that can be useful.\nAs a reminder, we wanted to compare the densities of Patrick Mahomes and Derek Carr. Our graph worked fine - but what if we wanted to overlay those densities, so that the true differences were more apparent? That’s where the more manual (but still very much not manual) KDE comes in. One important note before we begin: while I’ve since modified the code a bit, as I mentioned in the previous post this Next Gen Stats scraper was first created by Sarah Mallepalle et al. (2019). I cannot recommend reading this paper enough!\nDensity Estimates\n\n\n#imports\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom pygam import LogisticGAM, s, f, te\n#I'm surpressing warnings here because the PyGAM library warns you that the p-values are smaller than likely, which I am not concerned with.\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#load passing location data\ndf = pd.read_csv('https://raw.githubusercontent.com/ArrowheadAnalytics/next-gen-scrapy-2.0/master/pass_and_game_data.csv', low_memory=False)\n#There's an additional index row we don't need, so I am getting rid of it here\ndf = df.iloc[0:,1:]\ndf.dropna(inplace=True)\n\nWhat we’re going to do here is create a helper function, which will allow is to perform the kernal density. The key to making these KDEs comparable between players is the grid size. By keeping these constant between players we are comparing apples to apples.\n\n\n#Function that will help us get our data in the right shape every time we want to do this estimate\ndef kde_helper(df,name):\n    '''Function to get data in the correct form for the KDE function\n    inputs: dataframe, player name\n    output: KDE applied to mesh grid, ready for plotting'''\n    #Creating a mesh grid dividing each yard in half (so 4 units in a square yard),\n    #between the boundaries of the x and y coordinates (the min and max of our data) supplied.\n    m1 = df['x_coord'].loc[(df['name'].str.contains(name))]\n    m2 = df['y_coord'].loc[(df['name'].str.contains(name))]\n    #By using the same size grid each time we perform these estimates, we can make direct apples to apples comparisons between players. \n    #What we're doing with this line is creating a \"mesh grid\" (think matrix) which we'll eventually evaluate the KDE on\n    X, Y = np.mgrid[-30:30:121j, -10:60:141j]\n    #flatten and stack these grids, giving a 2xn array of positions where n = 121*141 (the # of steps for each direction)\n    #Basically what we are getting here is a \"coordinate\" for every single step we've created.\n    #We start the x min at -30, so there will be 141 -30s - because -30 will be paired with every step we've created in the y direction.\n    positions = np.vstack([X.ravel(), Y.ravel()])\n    #Stack the values we care about in a 2xm array (basically transposing them here), where m is just the length of our supplied data\n    values = np.vstack([m1, m2])\n    #Perform the kernel estimation on the values we care about - you can think of this as \"training\" the kernel estimator\n    kernel = stats.gaussian_kde(values)\n    #Generate probabilities at the positions specified, transpose them, and put them back into the grid shape for plotting\n    Z = np.reshape(kernel(positions).T, X.shape)\n    return Z\n\nNow that we’ve got the helper function, we can try it out!\n\n\n#We'll start with Mahomes\nname='Mahomes'\nmahomes_kde = kde_helper(df,name)\n#That was easy! Now Carr\nname='Carr'\ncarr_kde = kde_helper(df,name)\n\nSo we’ve got our two estimates. There are different ways you can play around with these estimates to find insights but what I’m going to focus on is the difference between the two estimates. The nice thing is this is really easy to do, you just subtract them.\n\n\n#again - simple!\ndiff_kde = mahomes_kde - carr_kde\n\nThe plotting here is a bit different than what we did in the previous post. We’ll be using matplotlib’s imshow() function, which is what you use to display image files like JPEGs. imshow accepts a matrix as an input, either MxN (what we have with our kde - a colormap), MxNx3 (how a traditional “picture” is stored - RGB values for each pixel location), or MxNx4 (adding an additional layer of our matrix to control the transparency level)\n\n\n#Set our style\nplt.style.use('seaborn-talk')\n\nfig, ax1 =plt.subplots(1,1)\n\n#This line is where the magic happens. Because of the way we performed the KDE, we have to rotate our data 270 degrees to plot in the orientation we want (np.rot90)\n#Next, we want to make sure a pixel in the left direction is the same coordinate distance as a pixel in the vertical direction, so we set aspect to equal\n#The extent is setting the coordinate system of the displayed image (along with the \"origin\" parameter). This is necessary to make sure that what we are indicating is the 20 yard line shows up as the 20 yardline in the pic\n#Next, we want to normalize our colormap so that 0 is in the exact middle of the colormap. We can do this by having vmin and vmax have the same absolute value\n#Lastly, we set the colormap parameter. I like \"diverging\" colormaps that have white in the middle for comparison plots, so it is clear which values are positive, negative, and 0.\nplt.imshow(np.fliplr(np.rot90(diff_kde,3)),\n           origin='lower', aspect='equal',\n           extent=[-30, 30, -10, 60],\n           norm = mpl.colors.Normalize(vmin=-0.0005, vmax=0.0005),\n           cmap='RdBu_r')\n#Add a \"colorbar\", a scale so people know what color represents what\ncbar = plt.colorbar()\ncbar.set_label(\"\\nMahomes (Red) - Carr (Blue) passing densities\")\n#We don't really care about the values here, only the relative differences. \n#The values will change depending on how small we slice up our field. So, I only want to show the viewer what 0 is.\ncbar.set_ticks([0])\n#Set title, remove ticks and labels\nax1.set_title('Mahomes vs Carr - NFL Passing Densities')\nax1.set_xlabel('')\nax1.set_xticks([])\n\nax1.set_yticks([])\n\nax1.set_ylabel('')\n\n#Remove any part of the plot that is out of bounds\nax1.set_xlim(-53.3333/2, 53.3333/2)\n\nax1.set_ylim(-10,60)\n\n\n#Plot all of the field markings (line of scrimmage, hash marks, etc.)\n\nfor j in range(-10,60,1):\n    ax1.annotate('--', (-3.1,j-0.5),\n                 ha='center',fontsize =10)\n    ax1.annotate('--', (3.1,j-0.5),\n                 ha='center',fontsize =10)\n    \nfor i in range(-10,60,5):\n    ax1.axhline(i,c='k',ls='-',alpha=0.5, lw=1.5)\n    \nfor i in range(-10,60,10):\n    ax1.axhline(i,c='k',ls='-',alpha=0.7, lw=1.5)\n    \nfor i in range(10,60-1,10):\n    ax1.annotate(str(i), (-12.88,i-1.15),\n            ha='center',fontsize =15,\n                rotation=270)\n    \n    ax1.annotate(str(i), (12.88,i-0.65),\n            ha='center',fontsize =15,\n                rotation=90)\n\nax1.annotate('Line of Scrimmage', (16,0),\n             textcoords=\"offset points\", # how to position the text\n                 xytext=(0,5), # distance from text to points (x,y)\n                 ha='center',fontsize = 9) # horizontal alignment can be left, right or center\n\n\n\nThis plot lets us see the differences in the densities between the two players, but there’s a lot of color there. Depending on the device you’re viewing this chart on, it may be hard to know what areas of the field to focus on. In order to help better direct the viewer to the most prominent differences, we can “mask” the image so that we only show the extreme differences.\n\n\n#Here's our mask. It may seem weird to use \"masked_inside\" here when we want the values on the extremes (outside these numbers), but keep in mind this is the \"masked\" array - so the mask_inside will hide all values inside these boundaries\n#You can manually set these numbers, but for simplicity and consistency I'm going to go with the top and bottom quartiles of our differences. Show I'm showing the top 25% units where Mahomes has higher density than Carr, and the top 25% where Carr has higher density than Mahomse\ndiff_masked = np.ma.masked_inside(diff_kde, np.percentile(diff_kde, 25), np.percentile(diff_kde, 75))\n\nplt.style.use('seaborn-talk')\n\nfig, ax1 =plt.subplots(1,1)\n\n\nplt.imshow(np.fliplr(np.rot90(diff_masked,3)),\n           origin='lower', aspect='equal',\n           extent=[-30, 30, -10, 60],\n           norm = mpl.colors.Normalize(vmin=-0.0005, vmax=0.0005),\n           cmap='RdBu_r')\n\n#Set title, remove ticks and labels\nax1.set_title('Mahomes (red) vs Carr (blue) - NFL Passing Densities')\nax1.set_xlabel('')\nax1.set_xticks([])\n\nax1.set_yticks([])\n\nax1.set_ylabel('')\n\n#Remove any part of the plot that is out of bounds\nax1.set_xlim(-53.3333/2, 53.3333/2)\n\nax1.set_ylim(-10,60)\n\n\n#Plot all of the field markings (line of scrimmage, hash marks, etc.)\n\nfor j in range(-10,60,1):\n    ax1.annotate('--', (-3.1,j-0.5),\n                 ha='center',fontsize =10)\n    ax1.annotate('--', (3.1,j-0.5),\n                 ha='center',fontsize =10)\n    \nfor i in range(-10,60,5):\n    ax1.axhline(i,c='k',ls='-',alpha=0.5, lw=1.5)\n    \nfor i in range(-10,60,10):\n    ax1.axhline(i,c='k',ls='-',alpha=0.7, lw=1.5)\n    \nfor i in range(10,60-1,10):\n    ax1.annotate(str(i), (-12.88,i-1.15),\n            ha='center',fontsize =15,\n                rotation=270)\n    \n    ax1.annotate(str(i), (12.88,i-0.65),\n            ha='center',fontsize =15,\n                rotation=90)\n\nax1.annotate('Line of Scrimmage', (16,0),\n             textcoords=\"offset points\", # how to position the text\n                 xytext=(0,5), # distance from text to points (x,y)\n                 ha='center',fontsize = 9) # horizontal alignment can be left, right or center\n\n\nSo now a viewer can pretty easily see the most relevant differences between two players. In this case, Carr is far more likely to target players around the line of scrimmage, while Mahomes is more likely to do “deep” screens (<-5 yards) or passes past the 10 yardline. If you aren’t a fan of the red and blue, you can play around with all of the available matplotlib colormaps. Again, I recommend a diverging map for this kind of plot but you can certainly get creative.\nExpected Completion Surfaces\nWhile densities can help tell us tendencies, they don’t tell us how well a player performed when targeting a certain area of the field. Ideally, we’d like to match this pass location data to play by play data and look at the expected points added of each throw, but due to the inconsistincies with the way different stadiums record air yards that’s quite difficult to do (though I highly encourage any ambitious reader to try. You’d add a lot to this field if you can pull it off). Since we don’t have expected points, we’ll try the next best thing: expected completion percentage.\nIn our dataset, we have the x and y coordinate of the pass, the player who threw the ball, the team they threw it against, some final game information (final score, game location) and whether or not the pass was completed. For now we’ll just estimate probabilities for the whole league, focusing on just the x and y coordinate of the pass. Now I’m not a statistician, so I can’t say for certain what model is best for this task. Thankfully, the amazing creators of the original NGS scraper are trained statisticians, and they’ve laid out in their paper why generalized additive models would be a good choice for this task. To quickly summarise, they allow us to both capture potential nonlinearities while also giving us a very smooth output, which is both nice for plotting and likely matches the reality of throwing a football (it is unlikely that there are very jagged differences or harsh cutoffs in difficulty as pass locations move throughout the field, but rather we’d expect the change in the “true” completion percentage to be smooth).\n\n\n#We have to do a bit of cleaning to get the data in a form we can use for the model. First, we need to convert out pass_type column into a binary variable instead of the categorical complete, incomplete, touchdown, and interception. \ndf['is_complete'] = 0\ndf.loc[((df['pass_type']=='COMPLETE') | (df['pass_type']=='TOUCHDOWN')), 'is_complete'] = 1\n\n#Now let's see the distribution of our outcome\nprint(df.is_complete.mean())\n\n0.6532767626998791\n\nOur classes are a bit unbalanced. We have more complete passes than incomplete, though not too drastically so. This class imbalance would be more important if we had imbalanced penalties for assigning incorrect classes. In other words, if we cared more about false negatives than false positives. In this case, it is no worse to predict an incomplete pass complete, than it is to predict a complete pass incomplete (unlike many systems we may try to model in the medical field). So the main reason we care about class imbalance here is when it comes to assessing the performance of our model; because 65% of our passes are complete, just predicting every single pass will be complete will already get us to 65% accuracy. This post isn’t meant to be a deep dive in classification, so we’re not going to address the class imbalance further.\nLet’s use a similar model structure to the one introduced by Mallepalle et al. (2019) Sticking with python, we’ll take advantage of the PyGAM library here.\n\n\n#Get the features and outcomes we care about\nX = df[['x_coord','y_coord']]\ny = df[['is_complete']]\n#Fit our model\ngam = LogisticGAM().fit(X, y)\n#Test the accuracy of our model\ngam.summary()\n\nLogisticGAM                                                                                               \n=============================================== ==========================================================\nDistribution:                      BinomialDist Effective DoF:                                     30.9458\nLink Function:                        LogitLink Log Likelihood:                                -25666.4423\nNumber of Samples:                        43839 AIC:                                            51394.7762\n                                                AICc:                                           51394.8243\n                                                UBRE:                                               3.1729\n                                                Scale:                                                 1.0\n                                                Pseudo R-Squared:                                   0.0928\n==========================================================================================================\nFeature Function                  Lambda               Rank         EDoF         P > x        Sig. Code   \n================================= ==================== ============ ============ ============ ============\ns(0)                              [0.6]                20           17.1         0.00e+00     ***         \ns(1)                              [0.6]                20           13.8         0.00e+00     ***         \nintercept                                              1            0.0          7.85e-05     ***         \n==========================================================================================================\nSignificance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWARNING: Fitting splines and a linear function to a feature introduces a model identifiability problem\n         which can cause p-values to appear significant when they are not.\n\nWARNING: p-values calculated in this manner behave correctly for un-penalized models or models with\n         known smoothing parameters, but when smoothing parameters have been estimated, the p-values\n         are typically lower than they should be, meaning that the tests reject the null too readily.\n\ngam.accuracy(X,y)\n\n0.7013389903966788\n\nSo with a quick simple model we’ve improved the accuracy of just assuming every pass will be complete, but we’re still incorrectly classifying 30% of passes. This isn’t too surprising though - we’ve got many different quarterbacks throwing the ball to many different wide receivers against many different defenses. Just including the location of the pass shouldn’t get us too accurate of a model, or we’d start to think that players don’t matter!\nOne very useful aspect of GAMs is that because they are an additive model, we can explore how each feature is influencing the model output by holding the other features constant at their average value. Let’s plot what that looks like.\n\n\n##I'll confess I just copy and pasted this straight from the pygam documentation, you could definitely clean these up further and add relevant titles. \nfor i, term in enumerate(gam.terms):\n    if term.isintercept:\n        continue\n        \n\n    XX = gam.generate_X_grid(term=i)\n    pdep, confi = gam.partial_dependence(term=i, X=XX, width=0.95)\n\n    plt.figure()\n    plt.plot(XX[:, term.feature], pdep)\n    plt.plot(XX[:, term.feature], confi, c='r', ls='--')\n    plt.title(repr(term))\n    if i == 0:\n      plt.show()\n\n\nI find these plots to be super cool (yes I’m a nerd but hey, you’re the one reading an open-source football post!) because they can let us easily see where the decreased probability in throwing a pass in certain areas of the field come from. One thing that immediately jumps out from these plots is that the only real influence of the x coordinate is passes close to either sideline. Otherwise, it is the depth of the pass (y coordinate) that is the driver behind the difficulty in completing it. This is exactly why a linear model would not do well here - the difference in completion probability from x coordinate 28 to x coordinate 25 is far different than the difference in cp from x coordinate 18 to x coordinate 15.\nOkay so we’ve fit our model and explored the feature dependencies a bit, but how do we go about visualizing this?\nSimilar to our KDE plotting, we’ll build a helper function for this\n\n\ndef gam_helper(df):\n    x = df[['x_coord','y_coord']]\n    y = df['is_complete']\n        #Similar to our KDE helper, we want a mesh grid that we will eventually evaluate the model on\n    X, Y = np.mgrid[-30:30:121j, -10:60:141j]\n        #Once again we want to flatten and stack our coordinates\n    positions = np.vstack([X.ravel(), Y.ravel()])\n        #Instead of a kde we fit a gam. Here I'm adjusting the number of splines to avoid overfitting, since we aren't doing any sort of hold out or cross validation in this post\n    gam = LogisticGAM(s(0, n_splines=8) + s(1, n_splines=8) + te(0,1)).fit(x, y)\n        #Generate probabilities at the positions specified, transpose them, and put them back into the grid shape for plotting\n    Z = np.reshape(gam.predict_mu(positions.T).T, X.shape)\n    return Z\n    \n#Call our function\npass_gam = gam_helper(df)\n\n\n#Plot our output, same code as before\nplt.style.use('seaborn-talk')\n\nfig, ax1 =plt.subplots(1,1)\n\n#This is where the magic happens here. Because of the way we performed the KDE, we have to rotate our data 270 degrees to plot in the orientation we want (np.rot90)\n#Next, we want to make sure a pixel in the left direction is the same coordinate distance as a pixel in the vertical direction, so we set aspect to equal\n#The extent is setting the coordinate system of the displayed image (along with the \"origin\" parameter). This is necessary to make sure that what we are indicating is the 20 yard line shows up as the 20 yardline in the pic\n#Next, we want to normalize our colormap so that 0 is in the exact middle of the colormap. We can do this by having vmin and vmax have the same absolute value\n#Lastly, we set the colormap parameter. I like \"diverging\" colormaps that have white in the middle for comparison plots, so it is clear which values are positive, negative, and 0.\nplt.imshow(np.fliplr(np.rot90(pass_gam,3)),\n           origin='lower', aspect='equal',\n           extent=[-30, 30, -10, 60],\n           norm = mpl.colors.Normalize(vmin=0, vmax=1),\n           cmap='PiYG')\n#Add a \"colorbar\", a scale so people know what color represents what\ncbar = plt.colorbar()\ncbar.set_label(\"\\nEstimated Completion Probability\")\n\n#Set title, remove ticks and labels\nax1.set_title('League-wide Estimated Completion Probability')\nax1.set_xlabel('')\nax1.set_xticks([])\n\nax1.set_yticks([])\n\nax1.set_ylabel('')\n\n#Remove any part of the plot that is out of bounds\nax1.set_xlim(-53.3333/2, 53.3333/2)\n\nax1.set_ylim(-10,60)\n\n\n#Plot all of the field markings (line of scrimmage, hash marks, etc.)\n\nfor j in range(-10,60,1):\n    ax1.annotate('--', (-3.1,j-0.5),\n                 ha='center',fontsize =10)\n    ax1.annotate('--', (3.1,j-0.5),\n                 ha='center',fontsize =10)\n    \nfor i in range(-10,60,5):\n    ax1.axhline(i,c='k',ls='-',alpha=0.5, lw=1.5)\n    \nfor i in range(-10,60,10):\n    ax1.axhline(i,c='k',ls='-',alpha=0.7, lw=1.5)\n    \nfor i in range(10,60-1,10):\n    ax1.annotate(str(i), (-12.88,i-1.15),\n            ha='center',fontsize =15,\n                rotation=270)\n    \n    ax1.annotate(str(i), (12.88,i-0.65),\n            ha='center',fontsize =15,\n                rotation=90)\n\nax1.annotate('Line of Scrimmage', (16,0),\n             textcoords=\"offset points\", # how to position the text\n                 xytext=(0,5), # distance from text to points (x,y)\n                 ha='center',fontsize = 9) # horizontal alignment can be left, right or center\n\n\nThere are a few different ways we can expand on this. First, we could play around with the model more. We didn’t do any hold out or cross validation in our model, we just checked the accuracy of the model on the data it was trained on.\nAdditionally, the original expected completion surface model introduced by Mallepalle et. al (2019) used smooth tensor products (ti) for all terms, whereas the python GAM library does not have this functionality - instead I just used spline terms and a tensor product term. So, our results differ a bit (though they should be expected to differ some because I’ve included the 2019 season which was not in the original paper). In general for statistical modeling I prefer and recommend using R, however I wanted to try keeping this post all in python.\nA logical next step is to estimate completion probabilities for a given QB or against a given defense. The simple way of doing this is very straightforward. You just filter your dataframe using .loc to get the QB or team you want, and repeat the process above. However, you’re going to be left with a model that is very likely to be “overfit” (admittedly I’ve done this quite a bit on twitter, but as I said before - I’m not a statistician!). Derek Carr for instance only has a handful of passes deep, but we would not expect that small sample size to be representative of the “true” completion percentage if Carr threw to every square yard on the field thousands of times. To combat that, one approach you can use is what Mallepalle et. al did and use a 2-Dimensional Naive Bayesian approach where you leverage the sample size of the entire league but give it less weight as the QB of interest has completed a greater number of passes in a given location of the field. Source code for this from the Mallepalle et al. paper can be found here, which is what I drew from below (though the source code is in R, it’s quite straightforward to adapt to python.)\n\n\n#league-wide data\n#median number of passes \nmed_n_passes = df.groupby(by='name')['x_coord'].count().median()\n#league-wide comp. probability estimates\nleague_gam = gam_helper(df)\n#league-wide kde\nleague_kde = kde_helper(df, '')\n\n#QB data\nqb_name = 'Mahomes'\nqb_df = df.loc[(df['name'].str.contains(qb_name))]\n#Qb passes\nn_qb = len(qb_df)\n#QB comp. prob\nqb_gam = gam_helper(qb_df)\n#QB kde\nqb_kde = kde_helper(df, qb_name)\n\n#Everyone's favorite phrase - regress to the mean!\nregressed_model = (med_n_passes*league_gam*league_kde + n_qb*qb_gam*qb_kde) / (med_n_passes*league_kde + n_qb*qb_kde)\n\nPlotting this using the exact same code as before should show us how like Mahomes is to complete a pass at any part of the field, accounting for how little we know about his true ability in each area of the field.\n\n\nplt.style.use('seaborn-talk')\nfig, ax1 =plt.subplots(1,1)\n\nplt.imshow(np.fliplr(np.rot90(regressed_model,3)),\n           origin='lower', aspect='equal',\n           extent=[-30, 30, -10, 60],\n           norm = mpl.colors.Normalize(vmin=0, vmax=1),\n           cmap='PiYG')\n#Add a \"colorbar\", a scale so people know what color represents what\ncbar = plt.colorbar()\ncbar.set_label(\"\\nMahomes Estimated Completion Probability\")\n\n#Set title, remove ticks and labels\nax1.set_title('Mahomes Estimated Completion Probability')\nax1.set_xlabel('')\nax1.set_xticks([])\n\nax1.set_yticks([])\n\nax1.set_ylabel('')\n\n#Remove any part of the plot that is out of bounds\nax1.set_xlim(-53.3333/2, 53.3333/2)\n\nax1.set_ylim(-10,60)\n\n\n#Plot all of the field markings (line of scrimmage, hash marks, etc.)\n\nfor j in range(-10,60,1):\n    ax1.annotate('--', (-3.1,j-0.5),\n                 ha='center',fontsize =10)\n    ax1.annotate('--', (3.1,j-0.5),\n                 ha='center',fontsize =10)\n    \nfor i in range(-10,60,5):\n    ax1.axhline(i,c='k',ls='-',alpha=0.5, lw=1.5)\n    \nfor i in range(-10,60,10):\n    ax1.axhline(i,c='k',ls='-',alpha=0.7, lw=1.5)\n    \nfor i in range(10,60-1,10):\n    ax1.annotate(str(i), (-12.88,i-1.15),\n            ha='center',fontsize =15,\n                rotation=270)\n    \n    ax1.annotate(str(i), (12.88,i-0.65),\n            ha='center',fontsize =15,\n                rotation=90)\n\nax1.annotate('Line of Scrimmage', (16,0),\n             textcoords=\"offset points\", # how to position the text\n                 xytext=(0,5), # distance from text to points (x,y)\n                 ha='center',fontsize = 9) # horizontal alignment can be left, right or center\n\n\nThis is definitely a different shape than the league-wide model we plotted. But exactly how does it differ? Once again we can answer this by subtracting our two models.\n\n\ndiff_gam = regressed_model - league_gam\n\nplt.style.use('seaborn-talk')\nfig, ax1 =plt.subplots(1,1)\n#Remember to change the min and max so again 0 is the midpoint, but the scale is more reasonable for the completion % data\nplt.imshow(np.fliplr(np.rot90(diff_gam,3)),\n           origin='lower', aspect='equal',\n           extent=[-30, 30, -10, 60],\n           norm = mpl.colors.Normalize(vmin=-0.5, vmax=0.5),\n           cmap='PiYG')\n\ncbar = plt.colorbar()\ncbar.set_label(\"\\n Completion Prob over Leage Avg\")\n\n#Set title, remove ticks and labels\nax1.set_title('Mahomes Estimated Completion Probability Over Avg')\nax1.set_xlabel('')\nax1.set_xticks([])\n\nax1.set_yticks([])\n\nax1.set_ylabel('')\n\n#Remove any part of the plot that is out of bounds\nax1.set_xlim(-53.3333/2, 53.3333/2)\n\nax1.set_ylim(-10,60)\n\n\n#Plot all of the field markings (line of scrimmage, hash marks, etc.)\n\nfor j in range(-10,60,1):\n    ax1.annotate('--', (-3.1,j-0.5),\n                 ha='center',fontsize =10)\n    ax1.annotate('--', (3.1,j-0.5),\n                 ha='center',fontsize =10)\n    \nfor i in range(-10,60,5):\n    ax1.axhline(i,c='k',ls='-',alpha=0.5, lw=1.5)\n    \nfor i in range(-10,60,10):\n    ax1.axhline(i,c='k',ls='-',alpha=0.7, lw=1.5)\n    \nfor i in range(10,60-1,10):\n    ax1.annotate(str(i), (-12.88,i-1.15),\n            ha='center',fontsize =15,\n                rotation=270)\n    \n    ax1.annotate(str(i), (12.88,i-0.65),\n            ha='center',fontsize =15,\n                rotation=90)\n\nax1.annotate('Line of Scrimmage', (16,0),\n             textcoords=\"offset points\", # how to position the text\n                 xytext=(0,5), # distance from text to points (x,y)\n                 ha='center',fontsize = 9) # horizontal alignment can be left, right or center\n\n\nMahomes has clearly had more success than most completing passes to the deep middle of the field.\nWe could go further here by utilizing the mask we used previously and only showing extreme differences (maybe greater or less than 10% above average), but I think this is a good place to stop for this post. I was incredibly pleased with the amount of people who played around with this data and code after the last post, and hopefully this inspires even more. But please remember to credit and cite Sarah Mallepalle and her team at CMU, since so much of this code and the original scraper came from them!\n\n\n",
    "preview": "posts/2020-08-29-player-density-and-completion-surface-estimates/player-density-and-completion-surface-estimates_files/figure-html5/plotting-1.png",
    "last_modified": "2020-10-29T09:08:01+00:00",
    "input_file": {},
    "preview_width": 6240,
    "preview_height": 4290
  },
  {
    "path": "posts/2020-08-29-adding-espn-and-538-game-predictions-to-nflfastr-data/",
    "title": "Adding ESPN and 538 Game Predictions to nflfastR Data",
    "description": "Here, we'll look at how to scrape ESPN's and 538's pregame predictions and merge them into nflfastR data",
    "author": [
      {
        "name": "Jonathan Goldberg",
        "url": "https://twitter.com/gberg1303"
      }
    ],
    "date": "2020-08-29",
    "categories": [
      "Scraping",
      "Game Predicitions",
      "nflfastR"
    ],
    "contents": "\nIn this article, we are going to (1) take a look at how to scrape pregame predictions from 538 and ESPN and (2) how to merge those predictions into nflfastR’s dataset.\nLet’s start by loading up the nflfastR data. To save some time, we’re only going to load the schedules for the last two seasons (2018 and 2019).\n\n\nlibrary(tidyverse)\nNFL_Games <- nflfastR::fast_scraper_schedules(2018:2019) %>%\n  dplyr::mutate(\n    gameday = lubridate::as_date(gameday)\n  )\n\n538:\n538’s data is super easy to grab. They have a repo of NFL games, each team’s ELO ratings before the game, and their predictions for the outcome of the game.\n\n\nFiveThirtyEight_Predictions <- read_csv(\"https://projects.fivethirtyeight.com/nfl-api/nfl_elo.csv\")\n\nThe last thing we need to do is to change some of their labels for NFL teams. Washington and Oakland (Las Vegas) are the two problem children.\n\n\nFiveThirtyEight_Predictions <- FiveThirtyEight_Predictions %>%\n  dplyr::mutate(\n    team1 = gsub(\"WSH\", \"WAS\", team1),\n    team2 = gsub(\"WSH\", \"WAS\", team2),\n    team1 = gsub(\"LAR\", \"LA\", team1),\n    team2 = gsub(\"LAR\", \"LA\", team2)\n  ) %>%\n  dplyr::select(date, season, team1, team2, elo_prob1) %>%\n  dplyr::rename(fivethirtyeight_home_wp = elo_prob1)\n\nNow we can merge!\n\n\nNFL_Games <- NFL_Games %>%\n  left_join(\n    FiveThirtyEight_Predictions,\n    by = c(\"home_team\" = \"team1\", \"away_team\" = \"team2\", \"gameday\" = \"date\", \"season\")\n  )\n\nESPN:\nESPN is going to be pretty tricky, but they also include moneyline odds from some sportsbooks. It may be worth the extra effort to convert them to probabilities.\nUnfortunately, the data on ESPN’s pregame predictions cannot be simply scraped from their website nor are they in a repo. To complicate this even more, ESPN has a unique identification system that does not follow a reproducible pattern. To get their game IDs, you have to scrape them from the schedule page for each week of the NFL season.\nI’ve built a function that will help with exactly that.\n\n\nget_game_ids <- function(season, season_type = c(\"preseason\", \"regular\", \"postseason\")) {\n  current_year <- as.double(substr(Sys.Date(), 1, 4))\n  espn_game_ids <- data.frame()\n\n  if (!season_type %in% c(\"preseason\", \"regular\", \"postseason\", \"all\")) {\n    stop(\"Please choose season_type of 'regular',  'playoffs', 'postseason', or 'all'\")\n  }\n\n  if (!dplyr::between(as.numeric(season), 2002, current_year)) {\n    stop(paste(\"Please choose season between 2002 and\", current_year))\n  }\n\n  if (lubridate::month(Sys.Date()) < 12 & lubridate::month(Sys.Date()) > 2 & season_type == \"postseason\" & current_year == season | season_type == \"postseason\" & lubridate::month(Sys.Date()) <= 2 & current_year == season) {\n    stop(paste(\"Unfortunately, the NFL Playoff Games have not been determined yet\"))\n  }\n\n  message(\n    dplyr::if_else(\n      season_type == \"regular\",\n      glue::glue(\"Scraping from {season} {season_type} season!\"),\n      glue::glue(\"Scraping from {season} {season_type}!\")\n    )\n  )\n\n  season_type <- ifelse(season_type == \"preseason\", \"1\", season_type)\n  season_type <- ifelse(season_type == \"regular\", \"2\", season_type)\n  season_type <- ifelse(season_type == \"postseason\", \"3\", season_type)\n\n  weeks <- ifelse(season_type == \"2\", 17, 5)\n\n  espn_game_ids <- purrr::map_df(1:weeks, function(week) {\n    url <- glue::glue(\"https://www.espn.com/nfl/schedule/_/week/{week}/year/{season}/seasontype/{season_type}\")\n\n    webpage <- xml2::read_html(url)\n\n    links <- webpage %>%\n      rvest::html_nodes(\"a\") %>%\n      rvest::html_attr(\"href\")\n\n    espn_gameid <- links %>%\n      as.tibble() %>%\n      dplyr::filter(str_detect(value, \"gameId\") == TRUE) %>%\n      dplyr::pull(value) %>%\n      stringr::str_remove(., \"/nfl/game/_/gameId/\")\n\n    bye_teams <- webpage %>%\n      rvest::html_nodes(\".odd.byeweek\") %>%\n      rvest::html_nodes(\"abbr\") %>%\n      rvest::html_text()\n\n    home_team <- webpage %>%\n      rvest::html_nodes(\".home-wrapper\") %>%\n      rvest::html_nodes(\"abbr\") %>%\n      rvest::html_text()\n\n    away_team <- webpage %>%\n      rvest::html_nodes(\"abbr\") %>%\n      rvest::html_text()\n    away_team <- away_team[!away_team %in% home_team]\n    away_team <- away_team[!away_team %in% bye_teams]\n\n    placeholder <- data.frame(\n      home_team,\n      away_team,\n      espn_gameid\n    ) %>%\n      dplyr::mutate(\n        season_type = season_type,\n        season = season,\n        week = ifelse(season_type == 3, 17 + week, week)\n      )\n\n    espn_game_ids <- dplyr::bind_rows(espn_game_ids, placeholder)\n    return(espn_game_ids)\n  })\n\n  ### Fix Several Names for Compatibility with nflfastR Data game_ids\n  espn_game_ids <- espn_game_ids %>%\n    dplyr::mutate(\n      home_team = gsub(\"WSH\", \"WAS\", home_team),\n      away_team = gsub(\"WSH\", \"WAS\", away_team),\n      home_team = gsub(\"LAR\", \"LA\", home_team),\n      away_team = gsub(\"LAR\", \"LA\", away_team)\n    ) %>%\n    # Add nflfastR game_ids\n    dplyr::mutate(\n      week = ifelse(week == 22, week - 1, week),\n      alt_gameid = paste0(season, \"_\", ifelse(week >= 10, paste0(week), paste0(0, week)), \"_\", away_team, \"_\", home_team)\n    )\n\n  return(espn_game_ids)\n}\n\nThis is only step one, but pulling the pregame predictions from ESPN’s API is rather easy now that we have the IDs for each game. We simply plug them into the link to their json file, do some cleaning along the way, and extract the prediction.\n\n\n# Get Game IDs\nESPN_Games <- purrr::map_df(2018:2019, function(x) {\n  get_game_ids(x, season_type = \"regular\")\n})\n\nhead(ESPN_Games)\n\n  home_team away_team espn_gameid season_type season week\n1       PHI       ATL   401030710           2   2018    1\n2       CLE       PIT   401030718           2   2018    1\n3       IND       CIN   401030717           2   2018    1\n4       MIA       TEN   401030716           2   2018    1\n5       MIN        SF   401030715           2   2018    1\n6        NE       HOU   401030714           2   2018    1\n       alt_gameid\n1 2018_01_ATL_PHI\n2 2018_01_PIT_CLE\n3 2018_01_CIN_IND\n4 2018_01_TEN_MIA\n5  2018_01_SF_MIN\n6  2018_01_HOU_NE\n\n\n\n# Pull Pregame Predictions\nESPN_Game_Predictions <- purrr::map_df(ESPN_Games$espn_gameid, function(espn_game_id) {\n  pregame_predictions <- data.frame(espn_gameid = espn_game_id)\n\n  # Pull the JSon\n  game_json <- httr::GET(url = glue::glue(\"http://site.api.espn.com/apis/site/v2/sports/football/nfl/summary?event={espn_game_id}\")) %>%\n    httr::content(as = \"text\", encoding = \"UTF-8\") %>%\n    jsonlite::fromJSON(flatten = TRUE)\n\n\n  # Pull the game data from the ID dataframe\n  if (\"gameProjection\" %in% names(game_json[[\"predictor\"]][[\"homeTeam\"]]) == TRUE) {\n    pregame_predictions <- pregame_predictions %>%\n      dplyr::mutate(\n        espn_home_wp = as.numeric(game_json[[\"predictor\"]][[\"homeTeam\"]][[\"gameProjection\"]]) / 100\n      )\n    message(\n      paste(\"Pulling predictions for\", pregame_predictions$alt_gameid)\n    )\n  }\n\n\n  # Grab and convert the Moneylines from Oddsmakers\n  if (\"pickcenter\" %in% names(game_json) == TRUE &\n    \"provider.name\" %in% names(game_json[[\"pickcenter\"]]) == TRUE &\n    \"homeTeamOdds.moneyLine\" %in% names(game_json[[\"pickcenter\"]]) == TRUE\n  ) {\n    vegas_odds <- data.frame(\n      providers = game_json[[\"pickcenter\"]][[\"provider.name\"]],\n      odds = ifelse(game_json[[\"pickcenter\"]][[\"homeTeamOdds.moneyLine\"]] > 0, 100 / (game_json[[\"pickcenter\"]][[\"homeTeamOdds.moneyLine\"]] + 100), game_json[[\"pickcenter\"]][[\"homeTeamOdds.moneyLine\"]] / (game_json[[\"pickcenter\"]][[\"homeTeamOdds.moneyLine\"]] - 100))\n    ) %>%\n      tidyr::pivot_wider(names_from = providers, values_from = odds)\n\n    pregame_predictions <- cbind(\n      pregame_predictions, vegas_odds\n    )\n  }\n\n  return(pregame_predictions)\n})\n\n# Merge ESPN Data together\nESPN_Games <- ESPN_Games %>%\n  left_join(\n    ESPN_Game_Predictions,\n    by = c(\"espn_gameid\" = \"espn_gameid\")\n  )\n\n# Merge back to main data\nNFL_Games <- NFL_Games %>%\n  left_join(\n    ESPN_Games %>% select(alt_gameid, espn_home_wp, Caesars, numberfire, teamrankings, consensus),\n    by = c(\"game_id\" = \"alt_gameid\")\n  )\n\nNow that it’s all together, let’s take a look at the accuracy of ESPN’s, 538’s, Numberfire’s predictions over the last two seasons.\n\n\n### Do some data wrangling first\nNFL_Games <- NFL_Games %>%\n  mutate(\n    home_win = ifelse(home_score > away_score, 1, 0),\n    correct_espn = ifelse(ifelse(espn_home_wp > .5, 1, 0) == home_win, 1, 0),\n    correct_numberfire = ifelse(ifelse(numberfire > .5, 1, 0) == home_win, 1, 0),\n    correct_fivethirtyeight = ifelse(ifelse(fivethirtyeight_home_wp > .5, 1, 0) == home_win, 1, 0)\n  )\nAccuracy_Dataset <- NFL_Games %>%\n  # Filter out Playoff Games\n  filter(game_type == \"REG\") %>%\n  # Pivot Longer to allow group_by and summarize\n  pivot_longer(\n    cols = starts_with(\"correct_\"),\n    names_to = \"predictor\",\n    names_prefix = \"correct_\",\n    values_to = \"correct\"\n  ) %>%\n  # Group and Summarize\n  group_by(predictor) %>%\n  summarise(\n    games = sum(!is.na(correct)),\n    games_correct = sum(correct, na.rm = TRUE),\n    percent_correct = round(mean(correct, na.rm = TRUE), 3)\n  )\n\n### Merge Over Brier Scores\nAccuracy_Dataset <- Accuracy_Dataset %>%\n  left_join(data.frame(\n    predictor = c(\"espn\", \"fivethirtyeight\", \"numberfire\"),\n    brier_score = c(\n      DescTools::BrierScore(NFL_Games %>% filter(!is.na(espn_home_wp)) %>% pull(home_win), NFL_Games %>% filter(!is.na(espn_home_wp)) %>% pull(espn_home_wp)),\n      DescTools::BrierScore(NFL_Games$home_win, NFL_Games$fivethirtyeight_home_wp),\n      DescTools::BrierScore(NFL_Games %>% filter(!is.na(numberfire)) %>% pull(home_win), NFL_Games %>% filter(!is.na(espn_home_wp)) %>% pull(numberfire))\n    )\n  ),\n  by = \"predictor\"\n  ) %>%\n  mutate(brier_score = round(brier_score, 3))\n\n\n\n{\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"predictor\":[\"numberfire\",\"espn\",\"fivethirtyeight\"],\"games\":[512,512,512],\"games_correct\":[335,326,326],\"percent_correct\":[0.654,0.637,0.637],\"brier_score\":[0.212,0.213,0.223]},\"columns\":[{\"accessor\":\"predictor\",\"name\":\"Predictor\",\"type\":\"character\",\"minWidth\":110,\"align\":\"left\"},{\"accessor\":\"games\",\"name\":\"Predictions\",\"type\":\"numeric\",\"minWidth\":100,\"align\":\"center\"},{\"accessor\":\"games_correct\",\"name\":\"Correct Predictions\",\"type\":\"numeric\",\"minWidth\":100,\"align\":\"center\"},{\"accessor\":\"percent_correct\",\"name\":\"% Correct\",\"type\":\"numeric\",\"minWidth\":100,\"align\":\"center\",\"className\":\"border-left\",\"style\":[{\"background\":\"#00B0A7\"},{\"background\":\"#0089BA\"},{\"background\":\"#0089BA\"}]},{\"accessor\":\"brier_score\",\"name\":\"Brier Score\",\"type\":\"numeric\",\"minWidth\":100,\"align\":\"center\",\"style\":[{\"background\":\"#0089BA\"},{\"background\":\"#008CB8\"},{\"background\":\"#00B0A7\"}]}],\"defaultPageSize\":10,\"paginationType\":\"numbers\",\"showPageInfo\":true,\"minRows\":1,\"compact\":true,\"theme\":{\"headerStyle\":{\"&:hover[aria-sort]\":{\"background\":\"hsl(0, 0%, 96%)\"},\"&[aria-sort='ascending'], &[aria-sort='descending']\":{\"background\":\"hsl(0, 0%, 96%)\"},\"borderColor\":\"#555\"}},\"dataKey\":\"d8de4b8852ab4402043c5b1982e6717d\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}\nIt’s unsurprising that Numberfire is significantly more accurate than the ESPN or 538 predictions. As an oddsmaker, they can see where bets are placed and update their lines accordingly. Essentially, Numberfire benefits from more information.\nI’m looking forward to the future investigations into predicting NFL games that may come from this data!\n\n\n",
    "preview": {},
    "last_modified": "2020-10-29T09:08:01+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-08-29-faceted-and-animated-heatmaps/",
    "title": "Faceted and Animated Heatmaps",
    "description": "Combining lessons from multiple posts to create faceted or animated heatmaps.",
    "author": [
      {
        "name": "Analytics Darkweb",
        "url": "https://twitter.com/footballdaRkweb"
      }
    ],
    "date": "2020-08-29",
    "categories": [
      "Figures",
      "Animation",
      "nflfastR"
    ],
    "contents": "\nThomas Mock from Rstudio has done it again and shown us how to pull in different heatmap options into R. You can see his blogpost here!\nEarlier I had posted a Gist talking about animated plots, but decided why not add the two together and make some faceted and animated heatmaps to really let us dig down into whatever subsets we want. Maybe we want to see some season splits or changes year over year?\nFirst, let’s pull in our data.\n\n\nlibrary(tidyverse)\nlibrary(arrow)\n\nsouce_url <- \"https://raw.githubusercontent.com/ArrowheadAnalytics/next-gen-scrapy-2.0/master/pass_and_game_data.csv\"\n\npass_map_df <- \n  data.table::fread(souce_url) %>%\n  na.omit() %>%\n  select(-V1)\n\npbp <- \n  open_dataset(\"D:/nflfastR/\", format = \"feather\") %>% \n  filter(season >= 2017, play_type == \"pass\") %>% \n  collect()\n\nSince this dataset is pretty small I won’t convert the NGS scrappy data (shouts to OG scrappy legend Sarah Mallepalle) to feather, but you can learn how here.\nNext, let’s figure out a good way to merge our data, we’ll need to do some aggrigation of the pbp to get something useful here. Luckly, we’ve already trimmed it down to a small size.\n\n\nepa_yac <- \n  pbp %>%\n  group_by(old_game_id, passer_player_name, posteam) %>%\n  summarise(\n    mean_epa = mean(epa, na.rm = TRUE),\n    mean_YAC = mean(yards_after_catch, na.rm = TRUE)\n  )\n\nLet’s say we didn’t want to just look at one player, but we wanted to look at the passing patters of every QB since 2017 who meets some threshold, then we could learn something about the underlying nature of EPA, or YAC.\nNow, if you’ll excuse me here I’m going to lift a little bit of code from Thomas’ blog post I linked above since he already gave us the structure to plot on the field as a function. We all stand on the shoulder of giants.\n\n\n#### Code blog from Thomas Mock\nback_col <- \"white\"\nfront_col <- \"black\"\n\nnot_div_5 <- function(x) {\n  # select only elements of the vector not divisible by 5\n  x[x %% 5 != 0]\n}\n\ncenter_df <- tibble(\n  x_coord = c(rep(-3.1, 60), rep(3.1, 60)),\n  y_coord = seq(-14, 59, 1) %>% rep(2) %>% not_div_5(),\n  text = \"--\"\n)\n\n# line labels\nannotate_df <- tibble(\n  x_coord = c(12.88, -12.88) %>% rep(each = 5),\n  y_coord = seq(10, 50, 10) %>% rep(2),\n  text = seq(10, 50, 10) %>% rep(2) %>% str_replace(\"(.)(.)\", \"\\\\1 \\\\2\"),\n  rotation = c(90, 270) %>% rep(each = 5)\n)\n\n# yardlines\nyardline_df <- tibble(\n  y = seq(-15, 60, 5),\n  yend = seq(-15, 60, 5),\n  x = rep(-56 / 2, 16),\n  xend = rep(56 / 2, 16)\n)\n\n# sidelines\nsideline_df <- tibble(\n  y = c(-15.15, -15.15),\n  yend = c(60.15, 60.15),\n  x = c(-56 / 2, 56 / 2),\n  xend = c(-56 / 2, 56 / 2)\n)\n\nadd_field <- function() {\n  list(\n    coord_cartesian(\n      xlim = c(-53.333 / 2, 53.333 / 2),\n      ylim = c(-15, 60)\n    ),\n    geom_text(\n      data = annotate_df, aes(label = text, angle = rotation),\n      color = front_col, size = 8\n    ),\n    geom_segment(\n      data = yardline_df, color = front_col, size = 1,\n      aes(x = x, y = y, xend = xend, yend = yend)\n    ),\n    geom_segment(\n      x = -56 / 2, y = 0, xend = 56 / 2, yend = 0,\n      color = \"blue\", size = 1, alpha = 0.5\n    ),\n    geom_segment(\n      data = sideline_df, color = front_col, size = 2,\n      aes(x = x, y = y, xend = xend, yend = yend)\n    ),\n    geom_text(\n      data = center_df,\n      aes(label = text), color = front_col, vjust = 0.32\n    ),\n    theme_void(),\n    theme(\n      strip.text = element_text(size = 20, color = front_col),\n      plot.background = element_rect(fill = back_col, color = NA),\n      legend.position = \"none\",\n      plot.margin = unit(c(2, 1, 0.5, 1), unit = \"cm\"),\n      plot.caption = element_text(color = front_col),\n      plot.title = element_text(color = front_col),\n      plot.subtitle = element_text(color = front_col),\n      panel.background = element_rect(fill = back_col, color = NA),\n      panel.border = element_blank()\n    )\n  )\n}\n\nNext we need to do a little rangling here to get our player names to match. We don’t have id numbers here, but since we are only dealing with QBs we should be able to join on name and team, unless someone knows of two QBs on the same team with the same names!\n\n\npass_map_df %>%\n  separate(name, into = c(\"first\", \"last\"), \"\\\\s\") %>%\n  mutate(\n    passer_player_name = paste0(str_extract(first, \"\\\\w\"), \".\", last)\n  ) %>%\n  inner_join(epa_yac, by = c(\"passer_player_name\", \"team\" = \"posteam\", \"game_id\" = \"old_game_id\"))\n\n          game_id  first   last  pass_type team week x_coord y_coord\n    1: 2017091004 Carson Palmer   COMPLETE  ARI    1   -23.5    14.6\n    2: 2017091004 Carson Palmer   COMPLETE  ARI    1     2.8     9.3\n    3: 2017091004 Carson Palmer   COMPLETE  ARI    1    18.6    -1.1\n    4: 2017091004 Carson Palmer   COMPLETE  ARI    1    -8.4     8.3\n    5: 2017091004 Carson Palmer   COMPLETE  ARI    1   -15.5     8.0\n   ---                                                              \n37817: 2019122904  Casey Keenum INCOMPLETE  WAS   17    14.6     2.0\n37818: 2019122904  Casey Keenum INCOMPLETE  WAS   17    24.5    14.8\n37819: 2019122904  Casey Keenum INCOMPLETE  WAS   17    11.7    -7.6\n37820: 2019122904  Casey Keenum INCOMPLETE  WAS   17    10.9    14.1\n37821: 2019122904  Casey Keenum INCOMPLETE  WAS   17    -9.8    12.3\n       type home_team away_team season\n    1:  reg       DET       ARI   2017\n    2:  reg       DET       ARI   2017\n    3:  reg       DET       ARI   2017\n    4:  reg       DET       ARI   2017\n    5:  reg       DET       ARI   2017\n   ---                                \n37817:  reg       DAL       WAS   2019\n37818:  reg       DAL       WAS   2019\n37819:  reg       DAL       WAS   2019\n37820:  reg       DAL       WAS   2019\n37821:  reg       DAL       WAS   2019\n                                                                       game_url\n    1: http://www.nfl.com/liveupdate/game-center/2017091004/2017091004_gtd.json\n    2: http://www.nfl.com/liveupdate/game-center/2017091004/2017091004_gtd.json\n    3: http://www.nfl.com/liveupdate/game-center/2017091004/2017091004_gtd.json\n    4: http://www.nfl.com/liveupdate/game-center/2017091004/2017091004_gtd.json\n    5: http://www.nfl.com/liveupdate/game-center/2017091004/2017091004_gtd.json\n   ---                                                                         \n37817: http://www.nfl.com/liveupdate/game-center/2019122904/2019122904_gtd.json\n37818: http://www.nfl.com/liveupdate/game-center/2019122904/2019122904_gtd.json\n37819: http://www.nfl.com/liveupdate/game-center/2019122904/2019122904_gtd.json\n37820: http://www.nfl.com/liveupdate/game-center/2019122904/2019122904_gtd.json\n37821: http://www.nfl.com/liveupdate/game-center/2019122904/2019122904_gtd.json\n       home_score away_score passer_player_name   mean_epa mean_YAC\n    1:         35         23           C.Palmer -0.2199358 4.407407\n    2:         35         23           C.Palmer -0.2199358 4.407407\n    3:         35         23           C.Palmer -0.2199358 4.407407\n    4:         35         23           C.Palmer -0.2199358 4.407407\n    5:         35         23           C.Palmer -0.2199358 4.407407\n   ---                                                             \n37817:         47         16           C.Keenum -0.6127941 5.833333\n37818:         47         16           C.Keenum -0.6127941 5.833333\n37819:         47         16           C.Keenum -0.6127941 5.833333\n37820:         47         16           C.Keenum -0.6127941 5.833333\n37821:         47         16           C.Keenum -0.6127941 5.833333\n\nLooks like we dropped a few entries, probably a team name mismatch, so let’s fix that.\n\n\nfastR_teams <- epa_yac$posteam %>% unique()\n\nscrappy_teams <- pass_map_df$team %>% unique()\n\nsetdiff(fastR_teams, scrappy_teams)\n\n[1] \"LV\"\n\nAs suspected, the scrappy team names reflect OAK while nflfastR lists LV.\n\n\npass_map_df %>%\n  separate(name, into = c(\"first\", \"last\"), \"\\\\s\") %>%\n  mutate(\n    passer_player_name = paste0(str_extract(first, \"\\\\w\"), \".\", last),\n    team = ifelse(team == \"OAK\", \"LV\", team)\n  ) %>%\n  inner_join(epa_yac, by = c(\"passer_player_name\", \"team\" = \"posteam\", \"game_id\" = \"old_game_id\"))\n\n          game_id  first   last  pass_type team week x_coord y_coord\n    1: 2017091004 Carson Palmer   COMPLETE  ARI    1   -23.5    14.6\n    2: 2017091004 Carson Palmer   COMPLETE  ARI    1     2.8     9.3\n    3: 2017091004 Carson Palmer   COMPLETE  ARI    1    18.6    -1.1\n    4: 2017091004 Carson Palmer   COMPLETE  ARI    1    -8.4     8.3\n    5: 2017091004 Carson Palmer   COMPLETE  ARI    1   -15.5     8.0\n   ---                                                              \n39280: 2019122904  Casey Keenum INCOMPLETE  WAS   17    14.6     2.0\n39281: 2019122904  Casey Keenum INCOMPLETE  WAS   17    24.5    14.8\n39282: 2019122904  Casey Keenum INCOMPLETE  WAS   17    11.7    -7.6\n39283: 2019122904  Casey Keenum INCOMPLETE  WAS   17    10.9    14.1\n39284: 2019122904  Casey Keenum INCOMPLETE  WAS   17    -9.8    12.3\n       type home_team away_team season\n    1:  reg       DET       ARI   2017\n    2:  reg       DET       ARI   2017\n    3:  reg       DET       ARI   2017\n    4:  reg       DET       ARI   2017\n    5:  reg       DET       ARI   2017\n   ---                                \n39280:  reg       DAL       WAS   2019\n39281:  reg       DAL       WAS   2019\n39282:  reg       DAL       WAS   2019\n39283:  reg       DAL       WAS   2019\n39284:  reg       DAL       WAS   2019\n                                                                       game_url\n    1: http://www.nfl.com/liveupdate/game-center/2017091004/2017091004_gtd.json\n    2: http://www.nfl.com/liveupdate/game-center/2017091004/2017091004_gtd.json\n    3: http://www.nfl.com/liveupdate/game-center/2017091004/2017091004_gtd.json\n    4: http://www.nfl.com/liveupdate/game-center/2017091004/2017091004_gtd.json\n    5: http://www.nfl.com/liveupdate/game-center/2017091004/2017091004_gtd.json\n   ---                                                                         \n39280: http://www.nfl.com/liveupdate/game-center/2019122904/2019122904_gtd.json\n39281: http://www.nfl.com/liveupdate/game-center/2019122904/2019122904_gtd.json\n39282: http://www.nfl.com/liveupdate/game-center/2019122904/2019122904_gtd.json\n39283: http://www.nfl.com/liveupdate/game-center/2019122904/2019122904_gtd.json\n39284: http://www.nfl.com/liveupdate/game-center/2019122904/2019122904_gtd.json\n       home_score away_score passer_player_name   mean_epa mean_YAC\n    1:         35         23           C.Palmer -0.2199358 4.407407\n    2:         35         23           C.Palmer -0.2199358 4.407407\n    3:         35         23           C.Palmer -0.2199358 4.407407\n    4:         35         23           C.Palmer -0.2199358 4.407407\n    5:         35         23           C.Palmer -0.2199358 4.407407\n   ---                                                             \n39280:         47         16           C.Keenum -0.6127941 5.833333\n39281:         47         16           C.Keenum -0.6127941 5.833333\n39282:         47         16           C.Keenum -0.6127941 5.833333\n39283:         47         16           C.Keenum -0.6127941 5.833333\n39284:         47         16           C.Keenum -0.6127941 5.833333\n\nOkay we’re closer, but still missing some rows. Probably a QB name mismatch. Let’s find them.\n\n\npass_map_df <- \n  pass_map_df %>%\n  separate(name, into = c(\"first\", \"last\"), \"\\\\s\") %>%\n  mutate(\n    passer_player_name = paste0(str_extract(first, \"\\\\w\"), \".\", last),\n    team = ifelse(team == \"OAK\", \"LV\", team)\n  )\n\nfastR_qbs <- epa_yac$passer_player_name %>% unique()\n\nscrappy_qbs <- pass_map_df$passer_player_name %>% unique()\n\nsetdiff(fastR_qbs, scrappy_qbs)\n\n  [1] \"M.Stafford\"    \"B.Bortles\"     \"D.Prescott\"    \"R.Quigley\"    \n  [5] \"J.Landry\"      \"J.Hekker\"      \"C.Henne\"       \"R.Mallett\"    \n  [9] \"M.Haack\"       \"M.Cassel\"      \"L.Edwards\"     \"M.Gray\"       \n [13] \"T.McEvoy\"      \"P.O'Donnell\"   \"T.Cohen\"       \"C.Beathard\"   \n [17] \"M.Lee\"         \"R.Golden\"      \"C.Rush\"        \"G.Smith\"      \n [21] \"S.Koch\"        \"E.Decker\"      \"S.Vereen\"      \"T.Kelce\"      \n [25] \"K.Clemens\"     \"J.Ryan\"        \"C.Kupp\"        \"J.Rudock\"     \n [29] \"J.Webb\"        \"B.Nortman\"     \"M.Palardy\"     \"A.McCarron\"   \n [33] \"L.Fitzgerald\"  \"L.Jones\"       \"G.Tate\"        \"J.Callahan\"   \n [37] \"R.Cobb\"        \"T.Bray\"        \"W.Snead\"       \"T.Burton\"     \n [41] \"N.Agholor\"     \"D.Henry\"       \"K.Byard\"       \"A.Wilson\"     \n [45] \"J.Scott\"       \"C.Bojorquez\"   \"D.Hopkins\"     \"E.Sanders\"    \n [49] \"N.Mullens\"     \"J.Dobbs\"       \"B.Anger\"       \"C.Beasley\"    \n [53] \"D.Hilliard\"    \"L.Cooke\"       \"L.Thomas\"      \"D.Jennings\"   \n [57] \"T.Boyd\"        \"E.Ebron\"       \"C.McCoy\"       \"C.Wadman\"     \n [61] \"A.Miller\"      \"C.Daniel\"      \"A.Brown\"       \"C.Boswell\"    \n [65] \"R.Griffin III\" \"B.Ellington\"   \"M.Sanchez\"     \"Z.Jones\"      \n [69] \"J.Johnson\"     \"K.Lauletta\"    \"S.Martin\"      \"C.McCaffrey\"  \n [73] \"D.Westbrook\"   \"M.Darr\"        \"K.Stills\"      \"M.Prater\"     \n [77] \"G.Gilbert\"     \"T.Way\"         \"D.Pettis\"      \"J.Stidham\"    \n [81] \"P.Williams\"    \"J.Samuels\"     \"A.Kamara\"      \"J.Elliott\"    \n [85] \"G.Minshew II\"  \"Z.Pascal\"      \"B.Kern\"        \"M.Wishnowsky\" \n [89] \"Jos.Allen\"     \"R.Dixon\"       \"A.Lee\"         \"D.Colquitt\"   \n [93] \"K.Barner\"      \"C.Sutton\"      \"B.Powell\"      \"S.Sims\"       \n [97] \"T.Boyle\"       \"J.Brown\"       \"A.Erickson\"    \"J.White\"      \n[101] \"J.Gordon\"      \"A.Tanney\"      \"A.Beck\"        \"K.Hunt\"       \n[105] \"K.Harmon\"      \"S.Diggs\"       \"S.Watkins\"    \n\nIt appears as though a lot of names are non-QBs such as Sam Koch or Dustin Colquitt. Punters doing trick plays etc. But some of these are QBs we need to fix like RG3, Dak Prescott, and Matt Stafford.\n\n\nepa_yac <- \n  epa_yac %>%\n  mutate(\n    passer_player_name = ifelse(passer_player_name == \"Jos.Allen\", \"J.Allen\", passer_player_name),\n    passer_player_name = ifelse(passer_player_name == \"G.Minshew II\", \"G.Minshew\", passer_player_name),\n    passer_player_name = ifelse(passer_player_name == \"R.Griffin III\", \"R.Griffin\", passer_player_name)\n  )\n\npass_map_df <- \n  pass_map_df %>%\n  mutate(\n    passer_player_name = ifelse(passer_player_name == \"R.Prescott\", \"D.Prescott\", passer_player_name),\n    passer_player_name = ifelse(passer_player_name == \"J.Stafford\", \"M.Stafford\", passer_player_name),\n    passer_player_name = ifelse(passer_player_name == \"R.Bortles\", \"B.Bortles\", passer_player_name)\n  )\n\nYou get the point, you can use this code and repair more names to match but I am going to proclaim victory and move on.\n\n\npass_map_df <- \n  pass_map_df %>%\n  inner_join(epa_yac, by = c(\"passer_player_name\", \"team\" = \"posteam\", \"game_id\" = \"old_game_id\"))\n\nFor our first split, let’s leverage these two datasets and see if we can see some structural differences between EPA and YAC performance.\n\n\npass_map_df %>%\n  mutate(\n    epa_below_zero = ifelse(mean_epa <= 0, \"Negative EPA\", \"Postive EPA\"),\n    epa_below_zero = as.factor(epa_below_zero)\n  ) %>%\n  ggplot(aes(x = x_coord, y = y_coord)) +\n  geom_density_2d_filled(\n    aes(fill = ..level.., color = ..level..),\n    contour_var = \"ndensity\", # normalize across facets\n    breaks = seq(0.1, 1.0, length.out = 10)\n  ) +\n  facet_wrap(~epa_below_zero) + \n  add_field()\n\n\nLet’s try the same thing but for YAC!\n\n\npass_map_df %>%\n  mutate(\n    yac_below_zero = ifelse(mean_YAC <= 0, \"Negative YAC\", \"Postive YAC\"),\n    yac_below_zero = as.factor(yac_below_zero)\n  ) %>%\n  ggplot(aes(x = x_coord, y = y_coord)) +\n  geom_density_2d_filled(\n    aes(fill = ..level.., color = ..level..),\n    contour_var = \"ndensity\", # normalize across facets\n    breaks = seq(0.1, 1.0, length.out = 10)\n  ) +\n  facet_wrap(~yac_below_zero) + \n  add_field()\n\n\nThe finding structure and drawing any conclusions is left as an exersize for the reader.\nBut let’s now take a look at QB performance over time.\n\n\npass_map_df %>%\n  filter(passer_player_name == \"D.Prescott\") %>%\n  ggplot(aes(x = x_coord, y = y_coord)) +\n  geom_density_2d_filled(\n    aes(fill = ..level.., color = ..level..),\n    contour_var = \"ndensity\", # normalize across facets\n    breaks = seq(0.1, 1.0, length.out = 10)\n  ) +\n  facet_wrap(~season) + \n  add_field() + \n  labs(title = \"Dak Prescott Targets by year, 2017-2019\")\n\n\nIt apears that as the years have gone on, DAK as less likely to throw to his left. Interesting, maybe something is there. Maybe the departure of personnel, or it could simply be new play designs. But what if we wanted to take this and make an animation instead of looking at these plots side by side.\n\n\nlibrary(gganimate)\n\np <- \n  pass_map_df %>%\n  filter(passer_player_name == \"D.Prescott\") %>%\n  ggplot(aes(x = x_coord, y = y_coord)) +\n  geom_density_2d_filled(\n    aes(fill = ..level.., color = ..level..),\n    contour_var = \"ndensity\", # normalize across facets\n    breaks = seq(0.1, 1.0, length.out = 10)\n  ) + \n  transition_states(season, transition_length = 2, state_length = 1) +\n  labs(\n    x = \"X Coordinate\",\n    y = \"Y coordinate relative to LOS\",\n    caption = \"Data: @nflfastR & next-gen-scrappy\",\n    title = \"{closest_state} Dak Prescott Targets\"\n  ) + \n  enter_fade()+\n  exit_fade() + \n  add_field()\n\nNow we’re ready to create our heatmap that changes over time. Imagine all the exploring we could do with this!\n\n\nanimate(p, width = 400, height = 600)\n\n\nWe could look at Patrick Mahomes over time, or Russell Wilson week to week. If we could get play_ids somehow we could improve our EPA plots above and look at negative EPA vs positive by down, distance. Perhaps if we assumed the order of the scrappy plays is in game order, we could match it to the nflfastR set and see what we find. Try it out and see!\nOne last thing, let’s combine what we’ve done and add in some of the work of Josh Hermsmeyer to do direct QB comparisons.\n\n\nqb_density_compare <- function(pass_df, qb1_name, qb2_name, n = 100){\n  \n  # filter to qb1\n  qb1 <- pass_df %>% \n    select(x_coord, y_coord, name) %>% \n    filter(str_detect(name, qb1_name))\n  \n  #filter to qb2\n  qb2 <- pass_df %>% \n    select(x_coord, y_coord, name) %>% \n    filter(str_detect(name, qb2_name))\n  \n  # get x/y coords as vectors\n  qb1_x <- pull(qb1, x_coord)\n  qb1_y <- pull(qb1, y_coord)\n  \n  # get x/y coords as vectors\n  qb2_x <- pull(qb2, x_coord)\n  qb2_y <- pull(qb2, y_coord)\n\n  # get x and y range to compute comparisons across\n  x_rng = range(c(qb1_x, qb2_x))\n  y_rng = range(c(qb1_y, qb2_y))\n  \n  # Calculate the 2d density estimate over the common range\n  d2_qb1 = MASS::kde2d(qb1_x, qb1_y, lims=c(x_rng, y_rng), n=n)\n  d2_qb2 = MASS::kde2d(qb2_x, qb2_y, lims=c(x_rng, y_rng), n=n)\n  \n  # create diff df\n  qb_diff <- d2_qb1\n  \n  # matrix subtraction density from qb2 from qb1\n  qb_diff$z <- d2_qb1$z - d2_qb2$z\n  \n  # add matrix col names\n  colnames(qb_diff$z) = qb_diff$y\n  \n  #### return tidy tibble ####\n  qb_diff$z %>% \n    # each col_name is actually the y_coord from the matrix\n    as_tibble() %>% \n    # add back the x_coord\n    mutate(x_coord= qb_diff$x) %>% \n    pivot_longer(-x_coord, names_to = \"y_coord\", values_to = \"z\") %>% \n    mutate(y_coord = as.double(y_coord))\n\n}\n\npass_map_df <- pass_map_df %>% rename(name = passer_player_name)\n\ncompared_z <- data.frame()\n\nfor (i in seq(2017, 2019, 1)) {\n  compared_z <- rbind(qb_density_compare(pass_map_df[pass_map_df$season == i], \"P.Mahomes\", \"A.Rodgers\", n = 200) %>% mutate(season = i), compared_z) \n}\n\nlibrary(scales)\np <-\n  compared_z %>%\n  # mutate(season = as.factor(season)) %>%\n  ggplot(aes(x_coord, y_coord)) +\n  # geom_tile(aes(x_coord, y_coord, fill=z))  +\n  stat_contour(geom = \"polygon\", \n                 aes(colour=..level.., z = z, fill = ..level..), \n               breaks = seq(min(compared_z$z), max(compared_z$z), length.out = 10)\n               ) +\n  scale_fill_gradient2(low=\"blue\",mid=\"white\", high=\"red\", midpoint=0) +\n  scale_colour_gradient2(low=muted(\"blue\"), mid=\"white\", high=muted(\"red\"), midpoint=0) +\n  add_field() +\n  theme(legend.position = \"bottom\", legend.key.width = unit(2, \"cm\"),\n        plot.title = element_text(size = 20, hjust = 0.5, face = \"bold\"),\n        plot.subtitle = element_text(size = 12, hjust = 0.5),\n        plot.caption = element_text(face = \"bold\")) +\n  labs(title = \"{current_frame}, Mahomes (QB1) vs Rodgers (QB2)\",\n       subtitle = \"Red = More by QB1, Blue = More by QB2\",\n       caption = \"\\n\\nPlot: @thomas_mock | Data: @ChiefsAnalytics\") +\n  guides(colour=FALSE) + \n  transition_manual(factor(season, levels = c('2017', '2018', '2019'))) +\n  enter_fade()+\n  exit_fade()\n\nanimate(p, width = 400, height = 600)\n\n\n\n\n",
    "preview": "posts/2020-08-29-faceted-and-animated-heatmaps/faceted-and-animated-heatmaps_files/figure-html5/unnamed-chunk-10-1.png",
    "last_modified": "2020-10-29T09:08:01+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-08-28-fast-data-loading/",
    "title": "Fast Data Loading",
    "description": "Loading your nfl data at 10x speed!",
    "author": [
      {
        "name": "Analytics Darkweb",
        "url": "https://twitter.com/footballdaRkweb"
      }
    ],
    "date": "2020-08-28",
    "categories": [
      "Efficiency"
    ],
    "contents": "\nMost of the time data loading isn’t something we think about when doing public data analysis. Datasets such as nflfastR aren’t that large in the grand scheme of things. But what if you’re looking to compete in the next big data bowl? Or what if you just need certain portions of nflfastR?\nThis walkthrough inspired by this NYR post: https://enpiar.com/talks/nyr-2020/#19\nAnd Thomas Mock from the Rstudio team: https://gist.github.com/jthomasmock/b8a1c6e90a199cf72c6c888bd899e84e\n\n\nlibrary(tidyverse)\nlibrary(arrow)\n\nWell, we can read in our data more efficiently to save ourselfs not only time but also RAM by not storing huge datasets in memory.\nIn order to run the arrow package as I have here, you will need the nightly build. See the above nyR post for details.\nFirst, let’s take a look at how fast we can pull down the latest pbp data from nflfastR.\n\n\nseasons <- 2010:2019\nsystem.time(\n  pbp <- \n    purrr::map_df(seasons, function(x) {\n      readr::read_csv(\n        glue::glue(\"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_{x}.csv.gz\")\n      )\n  })\n)\n\n   user  system elapsed \n  41.53   13.61   69.14 \n\nYour results may vary here, but for me? This process usually takes 30-45 seconds. Now, if you’re easily distracted like I am that’s just enough waiting around to be dangerous!\nOf course the alternative is to simply read in the data from a local copy, which is of course faster. We’ll use data.table’s fread here as it’s much faster than both the base read.csv and tidyverse’s read_csv.\n\n\nsystem.time(pbp <- data.table::fread(\"D:/Placeholder/nflfastR.csv\"))\n\n   user  system elapsed \n   7.38    0.53    2.98 \n\nBut what if we could do better? Or, what if we needed to load in much MUCH larger files? Perhaps gigabytes each? And what if we wanted to find a way to do some filtering as well?\nEnter the arrow package.\nArrow is a C++ backend that works across multiple languages to allow you incredibly fast load times and lets you conduct some of you first steps on disk. Meaning you’re not pulling the entire file into memory first.\nFirst we need to convert our data.frame into an arrow table. I found that using uncompressed made the process much faster.\n\n\nwrite_feather(pbp, \"D:/Placeholder/New/new_file\", compression = \"uncompressed\")\n\nds <- open_dataset(\"D:/Placeholder/New/\", format = \"feather\")\n\nsystem.time(open_dataset(\"D:/Placeholder/New/\", format = \"feather\") %>% collect())\n\n   user  system elapsed \n   2.14    3.04    2.14 \n\nWe can see that arrow loads our dataset pretty fast. A little faster than fread, but what if we could make it better?\nLets say I wanted to partitian the data by both season and play type. We can do this by converting our feather file to a dataset. We should choose ways to split the data that make the most sense given our usecase. For football, it may make sense to break things down by season and playtype since those are common splits to look at.\n\n\nfeather_dir <- \"D:/nflfastR/\"\nds %>%\n  group_by(season, play_type) %>%\n  write_dataset(feather_dir, format = \"feather\")\n\nNow for our last step, direct comparison!\nFor each test I am going to open a file, filter down to a particular season, play_type, then perform some summaries.\n\n\nsystem.time(\n  data.table::fread(\"D:/Placeholder/nflfastR.csv\") %>% \n    filter(season == 2019, play_type == \"pass\") %>% \n    group_by(posteam) %>% \n    summarise(epa = mean(epa, na.rm = TRUE))\n)\n\n   user  system elapsed \n   7.63    0.50    3.13 \n\nNotice below that I am using the collect() call between group_by and summarise!\n\n\nsystem.time(\n  open_dataset(\"D:/nflfastR/\", format = \"feather\") %>% \n    filter(season == 2019, play_type == \"pass\") %>% \n    group_by(posteam) %>% \n    collect() %>% \n    summarise(epa = mean(epa, na.rm = TRUE))\n)\n\n   user  system elapsed \n   0.25    0.05    0.29 \n\nThere you have it, to read in, filter, group, and summarise from data.table’s fread takes us significantly longer to read in than using arrow’s feather data type!\nWe’ve gone from loading online in about 60 seconds, to fread in 3-5 seconds, to feather around 2 seconds, but by saving our dataset in a novel way we can reduce our look ups to fractions of a second.\nThis 10x speed up might seem not worth the effort for this one file, but as these files get larger, as you merge more sources, these techniques can save a lot of time.\n\n\nmbm <- \n  microbenchmark::microbenchmark(\n  \"fread\" = {\n    data.table::fread(\"D:/Placeholder/nflfastR.csv\") %>% \n    filter(season == 2019, play_type == \"pass\") %>% \n    group_by(posteam) %>% \n    summarise(epa = mean(epa, na.rm = TRUE))\n  },\n  \"Naive Feather\" = {\n    open_dataset(\"D:/Placeholder/New/\", format = \"feather\") %>% \n    collect() %>% \n    filter(season == 2019, play_type == \"pass\") %>% \n    group_by(posteam) %>% \n    summarise(epa = mean(epa, na.rm = TRUE))\n  },\n  \"Custom Feather\" = {\n    open_dataset(\"D:/nflfastR/\", format = \"feather\") %>% \n    filter(season == 2019, play_type == \"pass\") %>% \n    group_by(posteam) %>% \n    collect() %>% \n    summarise(epa = mean(epa, na.rm = TRUE))\n  },\n  times = 5L\n  )\n\nHere is a plot showing the loading times for various methods.\n\n\nautoplot(mbm) + \n  labs(title = \"Data loading speed\")\n\n\n\n\n",
    "preview": "posts/2020-08-28-fast-data-loading/fast-data-loading_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2020-10-29T09:08:01+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-08-28-expected-completion-using-logistic-generalized-additive-mixed-models/",
    "title": "Individual Expected Completion using Logistic Generalized Additive Mixed Models",
    "description": "Case study how to leverage Generalized Additive Mixed Models (GAMM) to estimate the individual probability of completion per Quarterback as a random effect.",
    "author": [
      {
        "name": "Adrian Cadena",
        "url": "https://twitter.com/adrian_cadem"
      }
    ],
    "date": "2020-08-27",
    "categories": [
      "Logistic Generalized Additive Mixed Models",
      "Mixed Effects",
      "Completion Probability Intercept"
    ],
    "contents": "\nTable of Contents\nIntro\nPackages and Data Preparation\nModel\nSummary of Results\nRetrieving Estimates and Prepare Data for Plot\nPlotting\nInterpreting Results\nConclusion\n\nIntro\nMichael Lopez posted not long ago a great article explaining how Generalized Additive Models (GAMs) are a good way to measure non-linear effects of explanatory variables x on response variable y.\nLately, I’ve been playing around with linear and logistic mixed-effects models, so I thought about combining these with GAMs to estimate the probability of completion per Quarterback while accounting for non-linearities, especially on air yards.\nTo learn more about Logistic Mixed Effects I recommend https://stats.idre.ucla.edu/r/dae/mixed-effects-logistic-regression/\nFor a football case application of GAMs, nothing like M.Lopez’ post itself https://statsbylopez.netlify.app/post/plotting-air-yards/\nPackages and Data Preparation\nWe will be using the gamm4 library to fit our model.\n\n\nlibrary(gamm4)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(scales)\n\nWe’ll be working with data from 2016 to 2019. Because the NFL started to matter in 2016 when Dak Prescott was drafted.\nGAMMs can take a while to run since they also perform cross-validation. So I’ll do my best to filter-out data as much as possible without affecting results. Hold on.\n\n\n# Some stuff to filter later on\nnpass <- pbp %>%\n  dplyr::filter(\n    play_type == \"pass\",\n    season_type == \"REG\"\n  ) %>%\n  mutate(\n    play_in_19 = if_else(season == 2019, 1, 0)\n  ) %>%\n  group_by(passer_player_id) %>%\n  dplyr::summarise(\n    num_plays = n(),\n    last_seas = max(season),\n    plays_in_19 = sum(play_in_19)\n  )\n\npbp2 <- merge(pbp, npass, by = \"passer_player_id\", all.x = T, no.dups = T)\n\n# Mutations/data prep\npbp_mut <- pbp2 %>%\n  dplyr::filter(\n    season_type == \"REG\",\n    wp <= .85,\n    wp >= .15,\n    play_type == \"pass\",\n    !is.na(complete_pass),\n    penalty == 0,\n    num_plays >= 200\n  ) %>%\n  dplyr::mutate(\n    ayard_is_zero = if_else(air_yards == 0, 1, 0),\n    era1 = if_else(season %in% 2014:2017, 1, 0),\n    away = if_else(home_team == posteam, 0, 1),\n    id = passer_player_id,\n    # fixing some weird bugs I found with names bugs don't affect model, but mess with plot\n    passer_player_name = if_else(passer_player_name == \"Jos.Allen\", \"J.Allen\",\n      if_else(passer_player_name == \"R.Griffin\", \"R.Griffin III\",\n        if_else(passer_player_name == \"Matt.Moore\", \"M.Moore\",\n          if_else(passer_player_name == \"G.Minshew II\", \"G.Minshew\", passer_player_name)\n        )\n      )\n    )\n  ) %>%\n  dplyr::select(\n    id, passer_player_name, era1, season, away, wind, temp, complete_pass,\n    air_yards, qb_hit, ayard_is_zero, yardline_100, ydstogo, down,\n    plays_in_19, yardline_100\n  )\n\n# To map id's and Quarterback names\nnames <- pbp_mut %>%\n  group_by(id) %>%\n  dplyr::summarise(\n    Quarterback = unique(passer_player_name),\n    last_seas = max(season),\n    plays_in_19 = unique(plays_in_19)\n  )\n\nModel\nIt would be a good idea to add non-linear components to ydstogo and yardline_100, but I don’t want to slow down the model too much. If you have the time to do it, go for it! just do s(ydstogo) and s(yardline_100).\nSet family = binomial(link='logit') to make it a logistic binomial regression.\nOur random effect will be id since we want to look at every QB intercept.\nWe do nACG = 0 to speed up the process, it technically sacrifices accuracy, but for this exercise is no big deal.\nThis will take around 5 minutes to run, depending on your computer.\n\n\ngam_model <- gamm4(\n  complete_pass ~\n  era1 +\n    ydstogo +\n    yardline_100 +\n    down +\n    away +\n    qb_hit +\n    ayard_is_zero +\n    s(air_yards),\n  random = ~ (1 | id),\n  data = pbp_mut,\n  nAGQ = 0,\n  control = glmerControl(optimizer = \"nloptwrap\"),\n  family = binomial(link = \"logit\")\n)\n\nSummary of Results\ngamm4 returns two different summaries, one for the Generalized Additive part and one for the Mixed-Effects part. Feel free to look at the coefficients. You can also map IDs to the player’s name. I’ll do that later on. If interested, just copy the code.\nRetrieving Estimates and Prepare Data for Plot\nFirst, we will use broom.mixed to retrieve the random intercepts as well as their confidence intervals. We will also create a function to transform log-ods into probabilities. Just to make the plot easier to read.\nThen we map ids with player names using merge() and sort the data frame by descending estimate.\nFinally, we mutate confidence intervals and transform log-odds to probabilities. I am adding a threshold on the number of plays in 2019 just to make the plot clearer.\n\n\n# Retreive estimates and standard errors\nest <- broom.mixed::tidy(gam_model$mer, effects = \"ran_vals\") %>%\n  dplyr::rename(\"id\" = \"level\") %>%\n  dplyr::filter(term == \"(Intercept)\")\n\n# Function to convert logit to prob\nlogit2prob <- function(logit) {\n  odds <- exp(logit)\n  prob <- odds / (1 + odds)\n  return(prob)\n}\n\n# Prepare data for plot\nplot <- merge(est, names, by = \"id\", all.x = T, no.dups = T) %>%\n  arrange(estimate) %>%\n  mutate(\n    lci = estimate - 1.96 * std.error,\n    uci = estimate + 1.96 * std.error,\n    prob = logit2prob(estimate),\n    prob_uci = logit2prob(uci),\n    prob_lci = logit2prob(lci),\n  ) %>%\n  dplyr::filter(\n    plays_in_19 >= 100\n  )\n\nPlotting\nThe first plot includes the intercept estimate as well as confidence intervals.\n\n\nplot %>%\n  filter(\n    last_seas == 2019\n  ) %>%\n  ggplot(aes(x = factor(Quarterback, level = Quarterback), prob)) +\n  geom_point(size = .7) +\n  geom_linerange(size = .5, aes(\n    ymin = prob_lci,\n    ymax = prob_uci\n  )) +\n  coord_flip() +\n  theme_bw() +\n  labs(\n    y = \"iProbability of Completion\",\n    title = \"Individual Probability of Completion per Quarterback\",\n    subtitle = \"How each QB increases probability completion, controlling for situation | GAMM\",\n    caption = \"Data: nflfastR | Analysis by Adrian Cadena @adrian_cadem\"\n  ) +\n  theme(\n    plot.title = element_text(size = 15, hjust = .5),\n    plot.subtitle = element_text(size = 10, hjust = .5),\n    axis.title.y = element_blank(),\n  ) #+ ggsave('plot_gamm1.png', dpi=1100,width = 20, height = 15, units = \"cm\") \n\n\nThe second plot is a little easier to read and looks nice in terms of aesthetics. We are only adding the estimated probability intercept, no confidence interval.\n\n\nplot %>%\n  filter(last_seas == 2019) %>%\n  ggplot(aes(x = factor(Quarterback, level = Quarterback), prob)) +\n  geom_col(fill = \"grey20\") +\n  geom_text(aes(label = Quarterback, y = (((prob - .43) * .5)) + .43), color = \"white\", hjust = 1, size = 2.6, vjust = 0.3) +\n  coord_flip() +\n  theme_bw() +\n  labs(\n    y = \"iProbability of Completion\",\n    title = \"Individual Probability of Completion per Quarterback\",\n    subtitle = \"How each QB increases probability completion, controlling for situation | GAMM\",\n    caption = \"Data: nflfastR | Analysis by Adrian Cadena @adrian_cadem\"\n  ) +\n  scale_y_continuous(limits = c(.43, .565), oob = rescale_none, labels = scales::percent_format(accuracy = 1)) +\n  theme(\n    plot.title = element_text(size = 14, hjust = .5),\n    plot.subtitle = element_text(size = 10, hjust = .5),\n    axis.title.x = element_text(size = 12),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n  ) +\n  ggsave(\"plot_gamm2.png\", dpi = 1100, width = 20, height = 15, units = \"cm\")\n\n\nInterpreting Results\nWhen trying to predict the probability of a pass being completed, the main drivers are factors such as air yards, yard line, yards to go, down, etc. Each one of these variables increases or decreases the chances of a pass being completed. However, what is the starting probability of a pass being completed before we account for any of those variables? In other words, what is the intercept?\nBy using the passer as a random variable, we can assign an intercept for each Quarterback. This measures the individual effect on pass completion that each QB has. The higher the intercept, the higher the probability of a pass being completed, solely because of who is throwing the pass. This is an interesting alternative to CPOE because just like Expected Completion, it accounts for situational variables. However, this is just an inferential exercise, CPOE has proven to be the better predictive measure.\nConclusion\nThe purpose of this post is to invite people to play around with GAMMs. I didn’t want to break anyone’s computer so I kept it simple. Many variables could be added to the model to make it better, while also adding smoothing factors to some of the existing variables.\nThere are many other applications for GAMMs in football analysis, and I’m excited to see what you guys come up with. Reach out to me with any questions!\n\n\n",
    "preview": "posts/2020-08-28-expected-completion-using-logistic-generalized-additive-mixed-models/expected-completion-using-logistic-generalized-additive-mixed-models_files/figure-html5/Plot-1.png",
    "last_modified": "2020-10-29T09:08:01+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 2700
  },
  {
    "path": "posts/2020-08-25-open-source-fantasy-football-visualizing-trap-backs/",
    "title": "Open Source (Fantasy) Football: Visualizing TRAP Backs",
    "description": "Using nflfastR data to visualize where on the field running backs get their carries and how that translates to the Trivial Rush Attempt Percentage (TRAP) model.",
    "author": [
      {
        "name": "Sam Hoppen",
        "url": "https://twitter.com/SamHoppen"
      }
    ],
    "date": "2020-08-26",
    "categories": [
      "Figures",
      "nflfastR",
      "Fantasy Football"
    ],
    "contents": "\nTable of Contents\nIntro\nLoading Data and Packages\nAdding Player Positions\nVisualizing TB Touch Percent Based on Distance from the End Zone\nVisualizing High-Value Touches and the TRAP Model\nIntro\nIn this first post of mine, I am going to introduce the audience to open source fantasy football (a facet of football in which running backs DO matter), specifically the concept of TRAP backs.\nTRAP stands for Trivial Rush Attempt Percentage, which is a term popularized by Ben Gretch of CBS Sports. TRAP is meant to identify running backs who get the least-valuable touches in fantasy football by measuring a player’s percentage of total touches that are low-value rush attempts outside the 10-yard line.\nLoading Data and Packages\nThe first step in this analysis, as with many of these tutorials, is to load the data that we need. This includes the NFL play-by-play data, team colors and logos data (which will be used later), and NFL player positional data, along with the necessary libraries.\n\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggimage)\nlibrary(nflfastR)\n\nseasons <- 2019\n\npbp <- purrr::map_df(seasons, function(x) {\n  readRDS(\n    url(\n      glue::glue(\"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_{x}.rds\")\n    )\n  )\n})\n\nnfl_positions <- read_csv(url(\"https://raw.githubusercontent.com/samhoppen/NFL_Positions/master/nfl_positions_2011_2019.csv\"))\n\nAdding Player Positions\nIn order to get roster positions into nflfastR (which are not pre-populated), I built a repository that includes all players (from 2011-2019) and their respective position - that’s what the “nfl_positions” data frame is for. Since we’re only looking at running backs for this example, we want to filter out the other positions.\nTo add these to our pbp data, I used a sequence of left_join functions while adding in some fields that we’ll be using throughout this article. Additionally, because I’m doing this for fantasy football analysis, I want to filter out any non-fantasy-relevant plays, which is what the first filter is doing.\n\n\npbp <- pbp %>%\n  filter(season_type == \"REG\", down <= 4, play_type != \"no_play\") %>%\n  left_join(nfl_positions, by = c(\"passer_id\" = \"player_id\")) %>%\n  rename(\n    passer_full_name = full_player_name,\n    passer_position = position\n  ) %>%\n  left_join(nfl_positions, by = c(\"receiver_id\" = \"player_id\")) %>%\n  rename(\n    receiver_full_name = full_player_name,\n    receiver_position = position\n  ) %>%\n  left_join(nfl_positions, by = c(\"rusher_id\" = \"player_id\")) %>%\n  rename(\n    rusher_full_name = full_player_name,\n    rusher_position = position\n  ) %>%\n  select(-c(\"player.x\", \"player.y\", \"player\")) %>%\n  mutate(\n    ten_zone_rush = if_else(yardline_100 <= 10 & rush_attempt == 1, 1, 0),\n    ten_zone_pass = if_else(yardline_100 <= 10 & pass_attempt == 1 & sack == 0, 1, 0),\n    ten_zone_rec = if_else(yardline_100 <= 10 & complete_pass == 1, 1, 0),\n    field_touch = case_when(\n      yardline_100 <= 100 & yardline_100 >= 81 & (rush_attempt == 1 | complete_pass == 1) ~ \"touch_100_81\",\n      yardline_100 <= 80 & yardline_100 >= 61 & (rush_attempt == 1 | complete_pass == 1) ~ \"touch_80_61\",\n      yardline_100 <= 60 & yardline_100 >= 41 & (rush_attempt == 1 | complete_pass == 1) ~ \"touch_60_41\",\n      yardline_100 <= 40 & yardline_100 >= 21 & (rush_attempt == 1 | complete_pass == 1) ~ \"touch_40_21\",\n      yardline_100 <= 20 & yardline_100 >= 0 & (rush_attempt == 1 | complete_pass == 1) ~ \"touch_20_1\",\n      TRUE ~ \"other\"\n    )\n  )\n\nVisualizing TB Touch Percent Based on Distance from the End Zone\nNow that our play-by-play data has all of the information we need, we’re ready to start building new dataframes for our analysis.\nThe first piece of analysis is looking at the area of the field in which a running back’s rush attempts comes. This helps us get a high-level view of which running backs are getting touches closer to the goal line, which are the most valuable for fantasy football.\nIn this next block of code, we have a couple of things going on. First, as mentioned earlier, we’re filtering out only the running backs and grouping them in a way to get the total count of rushes for each area of the field, as defined above. Additionally, I’ve added an extra column in the second block of code to calculate the percent of rushes in each area of the field.\n\n\nrb_touches <- pbp %>%\n  filter(rusher_position == \"RB\") %>%\n  group_by(\n    rusher_full_name,\n    rusher_player_id,\n    field_touch\n  ) %>%\n  summarize(touches = n())\n\nrb_touches <- rb_touches %>%\n  group_by(rusher_full_name, rusher_player_id) %>%\n  mutate(\n    total_touches = sum(touches),\n    pct_touches = touches / total_touches\n  ) %>%\n  filter(total_touches >= 100)\n\nNow we have all of the data we need to build our first chart, but there are still a couple of small modifications to make in order to have our chart appear the way that we want it to.\nFirst, is creating a second dataframe that we’ll use to append to our primary dataframe - all I’m doing is pulling out each players’ red zone rush percent. I’m doing this because I eventually want to sort my chart by players’ red zone rushes as a percent of total touches, from highest to lowest. This may not be the most efficient way to add this data column, but it gets the job done.\n\n\nrb_touches_2 <- rb_touches %>%\n  filter(field_touch == \"touch_20_1\") %>%\n  select(rusher_full_name, rusher_player_id, pct_touches)\n\nrb_touches <- left_join(rb_touches,\n  rb_touches_2,\n  by = c(\n    \"rusher_full_name\" = \"rusher_full_name\",\n    \"rusher_player_id\" = \"rusher_player_id\"\n  )\n)\n\nSecond is a step I’m taking to use some custom colors from the Rcolorbrewer package, which will help us better visualize which running backs are getting the highest value touches (i.e. carries closer to the end zone). What I’m doing here is transforming our “field_touch” variable to a factor. We do this so that we can order the values in a way that aligns with the coloring we want, which is what we do in the second block of code below.\n\n\nlibrary(RColorBrewer)\nrb_touches$field_touch <- as.factor(rb_touches$field_touch)\nrb_touches$field_touch <- factor(rb_touches$field_touch, levels = c(\"touch_20_1\", \"touch_40_21\", \"touch_60_41\", \"touch_80_61\", \"touch_100_81\"))\n\ncolors <- brewer.pal(name = \"RdYlGn\", n = nlevels(rb_touches$field_touch))\nnames(colors) <- rev(levels(rb_touches$field_touch))\n\nNow that we have the data in the format that we want we’re ready to build our graph (using ggplot2, of course)!\n\n\nggplot() +\n  geom_col(\n    data = rb_touches,\n    aes(x = pct_touches.x, y = reorder(rusher_full_name, pct_touches.y), fill = field_touch)\n  ) +\n  scale_fill_manual(\n    values = colors,\n    limits = c(\"touch_100_81\", \"touch_80_61\", \"touch_60_41\", \"touch_40_21\", \"touch_20_1\"), labels = c(\"100 to 81 yds\", \"80 to 61 yds\", \"60 to 41 yds\", \"40 to 21 yds\", \"20 to 1 yds\")\n  ) +\n  labs(\n    x = \"Percent of plays\",\n    fill = \"Dist from end zone\",\n    title = \"RB touch % based on how far away from the goal line the touch was (min. 100 touches):\\nAlexander Mattison & Todd Gurley lead the league in % of touches in the red zone last year\",\n    caption = \"Figure: @SamHoppen | Data: @nflfastR\"\n  ) +\n  scale_x_continuous(\n    labels = scales::percent_format(accuracy = 1),\n    expand = c(0, 0.01)\n  ) +\n  theme(\n    axis.title.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.title.x = element_blank(),\n    legend.position = \"bottom\"\n  )\n\n\nVisualizing High-Value Touches and the TRAP Model\nThis isn’t all that we can do, though. Taking the next step, we focus on one of the tenets of the TRAP model: high-value touches (HVT). A high-value touch is defined as a rush attempt inside the 10 yard line or a reception anywhere on the field. To calculate a running back’s TRAP, we take the percent of a player’s non-HVTs as a percent of his total touches.\nSo, how do we do this? Well, using some of the fields that we’ve already added to the play-by-play data!\nWe’re going to start by making some new dataframes, though, so as not to get stuff mixed up. You’ll also notice that I removed Kenyan Drake’s time with the Dolphins so we can get a representation of his role with Arizona (and it makes the data a little messy).\n\n\nrb_hvt <- pbp %>%\n  filter(rusher_position == \"RB\") %>%\n  group_by(\n    rusher_full_name,\n    rusher_player_id,\n    posteam\n  ) %>%\n  summarize(\n    rush_attempts = sum(rush_attempt),\n    ten_zone_rushes = sum(ten_zone_rush),\n    receptions = sum(complete_pass),\n    total_touches = rush_attempts + receptions,\n    hvts = receptions + ten_zone_rushes,\n    non_hvts = total_touches - hvts,\n    hvt_pct = hvts / total_touches,\n    non_hvt_pct = non_hvts / total_touches\n  )\n\nrb_hvt <- rb_hvt[!(rb_hvt$rusher_full_name == \"Kenyan Drake\" & rb_hvt$posteam == \"MIA\"), ]\n\nSince the data isn’t ready-made in the correct format needed for the ggplot that we’ll be building, there are a couple minor modifications to do. The first of these is using pivot_longer to get our values matched up in the right way. Additionally, I’ve created a lookup dataframe. This is done in order to add an extra field to sort our ggplot from high to low, as we did earlier.\n\n\nrb_hvt <- rb_hvt %>%\n  pivot_longer(cols = c(hvt_pct, non_hvt_pct), names_to = \"hvt_type\", values_to = \"touch_pct\")\n\nhvt_lookup <- rb_hvt %>%\n  filter(hvt_type == \"hvt_pct\") %>%\n  select(rusher_full_name, rusher_player_id, hvt_type, touch_pct)\n\nrb_hvt <- left_join(rb_hvt,\n  hvt_lookup,\n  by = c(\n    \"rusher_full_name\" = \"rusher_full_name\",\n    \"rusher_player_id\" = \"rusher_player_id\"\n  )\n)\n\nHere, we also add the teams_colors_logos dataframe (which we loaded up earlier) as we’ll be using that as part of our visualization in the plot.\n\n\nrb_hvt <- left_join(rb_hvt,\n  teams_colors_logos,\n  by = c(\"posteam\" = \"team_abbr\")\n) %>%\n  filter(total_touches >= 100, hvt_type.x == \"hvt_pct\")\n\nNow we’ve got our data ready for visualization and are good to plot!\n\n\nggplot() +\n  geom_col(\n    data = rb_hvt,\n    aes(x = touch_pct.x, y = reorder(rusher_full_name, touch_pct.x)), fill = rb_hvt$team_color\n  ) +\n  geom_text() +\n  labs(\n    x = \"Percent of plays\",\n    fill = \"Distance from goal line\",\n    title = \"Visualization of TRAP backs, displaying RB high value touches (carries inside the 10\\nand catches) as a % of total touches (min 100 touches)\",\n    caption = \"Figure: @SamHoppen | Data: @nflfastR\"\n  ) +\n  scale_x_continuous(\n    labels = scales::percent_format(accuracy = 1),\n    limits = c(0, 0.165),\n    expand = c(0, 0)\n  ) +\n  theme(\n    axis.title.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.title.x = element_blank()\n  )\n\n\nVoila, that’s your intro to open source (fantasy) football! Hope you all enjoyed!\n\n\n",
    "preview": "posts/2020-08-25-open-source-fantasy-football-visualizing-trap-backs/open-source-fantasy-football-visualizing-trap-backs_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2020-10-29T09:08:01+00:00",
    "input_file": {},
    "preview_width": 5700,
    "preview_height": 3900
  },
  {
    "path": "posts/2020-08-25-expected-turnovers/",
    "title": "Expected Turnovers for Quarterbacks",
    "description": "Building expected interceptions and expected fumbles models to find QBs likely to increase or decrease their \ninterceptions and/or turnovers per dropback from 2019 to 2020.",
    "author": [
      {
        "name": "Anthony Gadaleta",
        "url": "https://twitter.com/AG_8"
      }
    ],
    "date": "2020-08-25",
    "categories": [
      "Figures",
      "nflfastR",
      "turnovers",
      "quarterbacks"
    ],
    "contents": "\nTable of Contents\nIntro\nLoad Packages, Get the Data\nExpected Interceptions\nExpected Fumbles\nPredictive Power?\nVisuals\nConclusion\nIntro\nOutside of actually scoring points, few events can swing an NFL game the way an interception or fumble can. Over the course of a season, if your quarterback is continuously giving the football away to the other team, your odds of a successful season are likely quite low.\nWith that said, all turnovers, and specifically QB turnovers, are not created equal. The goal of expected interceptions, expected fumbles and (by adding them together) expected turnovers is to measure how likely an incomplete pass or fumble is to be converted to an interception or lost fumble.1\nLoad Packages, Get the Data\n\n\nlibrary(nflfastR)\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(ggimage)\nlibrary(gt)\n\nDownload the latest play-by-play data for all plays from 2006-2019. We’ll be using 2006 as the start year because that’s the first year we have air yards data fully accessible.\n\n\nseasons <- 2006:2019\npbp <- purrr::map_df(seasons, function(x) {\n  readRDS(\n    url(\n      glue::glue(\"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_{x}.rds\")\n    )\n  )\n})\n\nDownload all NFL roster data from 1999-2019.\n\n\nroster <- readRDS(url(\"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/roster-data/roster.rds\"))\n\nExpected Interceptions\nWe’ll start with building the model for expected interceptions. Our independent variables will be air yards, pass location, qb hits, number of pass defenders and season.\nStart by creating an incompletions dataframe, which filters out all plays that do not result in incomplete passes or interceptions. Additionally, create the pass broken up (pbu) variable based on the number of pass defenders listed. The assumption being if more defenders are listed as defending a given pass, the more congested the throwing lane was.\n\n\nincompletions <- pbp %>%\n  filter(season_type == \"REG\" & season >= 2006 & (incomplete_pass == 1 | interception == 1)) %>%\n  select(\n    incomplete_pass, air_yards, pass_defense_1_player_id, pass_defense_2_player_id,\n    season, posteam, interception, qb_hit, week, defteam, passer, posteam, pass_location, desc\n  ) %>%\n  mutate(\n    pbu = case_when(\n      !is.na(pass_defense_1_player_id) & !is.na(pass_defense_2_player_id) &\n        (incomplete_pass == 1 | interception == 1) ~ 2,\n      !is.na(pass_defense_1_player_id) & is.na(pass_defense_2_player_id) &\n        (incomplete_pass == 1 | interception == 1) ~ 1,\n      TRUE ~ 0\n    ),\n  )\nincompletions$air_yards[is.na(incompletions$air_yards)] <- 0\nincompletions$pass_location[is.na(incompletions$pass_location)] <- \"None\"\n\nSplit into training and testing dataframes. I used 2006-2016 to train the model and 2017-2019 to test. The split comes out to approximately 79% training, 21% testing.\n\n\nint_train <- incompletions %>%\n  filter(season < 2017) %>%\n  select(-c(\n    pass_defense_1_player_id, pass_defense_2_player_id, incomplete_pass, posteam, week, defteam,\n    passer, posteam, desc\n  )) %>%\n  mutate(\n    interception = if_else(interception == 1, \"int\", \"no.int\"),\n  )\n\nint_test <- incompletions %>%\n  filter(season >= 2017)\n\nTrain the model using logistic regression, then add expected interceptions to the int_test dataframe.\n\n\nfitControl <- trainControl(\n  method = \"repeatedcv\",\n  number = 10,\n  repeats = 10,\n  classProbs = TRUE,\n  summaryFunction = twoClassSummary\n)\n\nset.seed(69) # nice\nint_model <- train(interception ~ .,\n  data = int_train,\n  method = \"glm\", preProcess = c(\"scale\", \"center\"),\n  metric = \"ROC\", trControl = fitControl\n)\nint_model\n\nGeneralized Linear Model \n\n74261 samples\n    5 predictor\n    2 classes: 'int', 'no.int' \n\nPre-processing: scaled (7), centered (7) \nResampling: Cross-Validated (10 fold, repeated 10 times) \nSummary of sample sizes: 66835, 66835, 66835, 66835, 66834, 66834, ... \nResampling results:\n\n  ROC        Sens       Spec     \n  0.9067668  0.1193285  0.9968662\n\nint_test$exp_int <- predict(int_model, int_test, type = \"prob\")\n\nLet’s take a look at the incompletions most likely have been intercepted from 2017-2019:\n\n\nint_test %>%\n  arrange(-exp_int) %>%\n  select(desc, exp_int, passer, posteam, defteam, season, week) %>%\n  head(5)\n\n# A tibble: 5 x 7\n  desc         exp_int$int $no.int passer posteam defteam season  week\n  <chr>              <dbl>   <dbl> <chr>  <chr>   <chr>    <int> <int>\n1 (:03) (Shot~       0.986  0.0141 D.Wat~ HOU     NE        2017     3\n2 (:03) (Shot~       0.985  0.0153 J.All~ BUF     JAX       2018    12\n3 (2:09) 73-C~       0.984  0.0158 M.Tru~ CHI     NO        2019     7\n4 (2:41) 5-J.~       0.983  0.0167 J.Fla~ BAL     TEN       2017     9\n5 (4:45) (Sho~       0.983  0.0175 M.Sta~ DET     NYG       2017     2\n\nExpected Fumbles\nNext up, we’ll build the expected fumbles model, using fumble forced vs not forced, fumble out of bounds, yards gained, sacks and aborted snaps as our independent variables.\nStart by creating a fumbles dataframe, which includes all plays where the ball hits the turf, regardless of which team recovers.\n\n\nfumbles <- pbp %>%\n  filter(season_type == \"REG\", season >= 2006, fumble == 1) %>%\n  select(\n    fumble_forced, fumble_not_forced, fumble_out_of_bounds, fumble_lost, fumble, yards_gained, sack,\n    aborted_play, season, posteam, week, defteam, fumbled_1_player_name, desc\n  )\n\nSplitting the same way as the incompletions, this time for an 80-20 train-test split.\n\n\nfumble_train <- fumbles %>%\n  filter(season < 2017) %>%\n  select(-c(season, posteam, week, fumble, defteam, fumbled_1_player_name, desc)) %>%\n  mutate(\n    fumble_lost = if_else(fumble_lost == 1, \"lost\", \"recovered\")\n  )\nfumble_test <- fumbles %>%\n  filter(season >= 2017)\n\nTrain the model using logistic regression (we can reuse the trControl from above) and then add expected fumbles to the fumble_test dataframe.\n\n\nset.seed(69) # nice\nfumble_model <- train(fumble_lost ~ .,\n  data = fumble_train,\n  method = \"glm\", preProcess = c(\"scale\", \"center\"),\n  trControl = fitControl, metric = \"ROC\"\n)\nfumble_model\n\nGeneralized Linear Model \n\n7642 samples\n   6 predictor\n   2 classes: 'lost', 'recovered' \n\nPre-processing: scaled (6), centered (6) \nResampling: Cross-Validated (10 fold, repeated 10 times) \nSummary of sample sizes: 6878, 6877, 6878, 6878, 6878, 6878, ... \nResampling results:\n\n  ROC        Sens       Spec     \n  0.7115302  0.7990853  0.5359961\n\nfumble_test$exp_fl <- predict(fumble_model, fumble_test, type = \"prob\")\n\nLet’s take a look at the fumbles most likely to have been lost from 2017-2019:\n\n\nfumble_test %>%\n  arrange(-exp_fl) %>%\n  select(desc, exp_fl, fumbled_1_player_name, posteam, defteam, season, week) %>%\n  head(5)\n\n# A tibble: 5 x 7\n  desc  exp_fl$lost $recovered fumbled_1_playe~ posteam defteam season\n  <chr>       <dbl>      <dbl> <chr>            <chr>   <chr>    <int>\n1 (:01~       0.852      0.148 T.Cohen          CHI     GB        2019\n2 (:05~       0.834      0.166 J.Witten         DAL     GB        2017\n3 (10:~       0.834      0.166 J.Jones          KC      LAC       2018\n4 (:02~       0.834      0.166 J.Edelman        NE      MIA       2019\n5 (:04~       0.824      0.176 J.Crowder        WAS     KC        2017\n# ... with 1 more variable: week <int>\n\nPredictive Power?\nNow it’s time to see if these new stats are actually useful for predicting future turnovers.\nFirst, modify roster names to allow us to use merge roster data with the dataframes we created above.\n\n\nroster$name <- paste0(substr(roster$teamPlayers.firstName, 1, 1), \".\", roster$teamPlayers.lastName)\n\nMerge roster data and group by passer and season to get total interceptions and expected interceptions and then total fumbles lost and expected fumbles lost for each passer in each season.\n\n\nxInt <- int_test %>%\n  filter(!is.na(passer)) %>%\n  left_join(roster[, c(\n    \"team.season\", \"name\", \"teamPlayers.position\", \"team.abbr\",\n    \"teamPlayers.headshot_url\"\n  )],\n  by = c(\"passer\" = \"name\", \"season\" = \"team.season\", \"posteam\" = \"team.abbr\")\n  ) %>%\n  rename(\n    position = teamPlayers.position,\n    player = passer,\n  ) %>%\n  filter(position == \"QB\") %>%\n  group_by(player, posteam, season, teamPlayers.headshot_url) %>%\n  summarise(Interceptions = sum(interception), xInt = sum(exp_int$int)) %>%\n  mutate(diff = Interceptions - xInt)\n\nxFmb <- fumble_test %>%\n  filter(!is.na(fumbled_1_player_name)) %>%\n  left_join(roster[, c(\n    \"team.season\", \"name\", \"teamPlayers.position\", \"team.abbr\",\n    \"teamPlayers.headshot_url\"\n  )],\n  by = c(\"fumbled_1_player_name\" = \"name\", \"season\" = \"team.season\", \"posteam\" = \"team.abbr\")\n  ) %>%\n  rename(\n    position = teamPlayers.position,\n    player = fumbled_1_player_name,\n  ) %>%\n  filter(position == \"QB\") %>%\n  group_by(player, posteam, season, teamPlayers.headshot_url) %>%\n  summarise(Fumbles_Lost = sum(fumble_lost), xFmb = sum(exp_fl$lost)) %>%\n  mutate(diff = Fumbles_Lost - xFmb)\n\nFind total dropbacks, epa per dropback and success rate on dropbacks for each passer. The latter two stats really aren’t necessary, but I thought it could be useful to show how well certain quarterbacks performed overall.\n\n\ndropbacks <- pbp %>%\n  filter(season_type == \"REG\" & season > 2016 & !is.na(passer)) %>%\n  group_by(passer, season) %>%\n  summarise(dropbacks = n(), epa = mean(epa, na.rm = TRUE), sr = mean(success, na.rm = TRUE))\n\nMerge the dropbacks dataframe with the xInt and xFmb dataframes. Then calc total turnovers, expected turnovers, turnovers per dropback, interceptions per dropback, differences between all of the actual and expected stats and the next season’s turnovers, interceptions and fumbles.\n\n\nxTO <- dropbacks %>%\n  inner_join(xInt, by = c(\"passer\" = \"player\", \"season\")) %>%\n  left_join(xFmb, by = c(\"passer\" = \"player\", \"posteam\", \"season\", \"teamPlayers.headshot_url\"))\n\nxTO$Fumbles_Lost[is.na(xTO$Fumbles_Lost)] <- 0\nxTO$xFmb[is.na(xTO$xFmb)] <- 0\nxTO$diff.y[is.na(xTO$diff.y)] <- 0\n\nxTO <- xTO %>%\n  mutate(\n    Turnovers = Interceptions + Fumbles_Lost,\n    xTO = xInt + xFmb,\n    diff = diff.x + diff.y,\n    to_pct = Turnovers / dropbacks,\n    int_pct = Interceptions / dropbacks,\n    xto_pct = xTO / dropbacks,\n    xint_pct = xInt / dropbacks,\n    to_pct_diff = xto_pct - to_pct,\n    int_pct_diff = xint_pct - int_pct,\n  ) %>%\n  filter(dropbacks >= 250) %>%\n  group_by(passer) %>%\n  mutate(\n    next_to = lead(Turnovers, 1),\n    next_int = lead(Interceptions, 1),\n    next_fmb = lead(Fumbles_Lost, 1),\n    next_db = lead(dropbacks, 1)\n  )\n\nFinally, let’s evaluate how predictive our new expected statistics are compared to their standard counterparts.\n\n\n[1] \"R2 of current TOs to next season's TOs: 0.187481148618913\"\n\n[1] \"R2 of current xTO to next season's TOs: 0.118063100859092\"\n\n[1] \"R2 of current Ints to next season's Ints: 0.178815133484698\"\n\n[1] \"R2 of current xInts to next season's Ints: 0.144854426175095\"\n\n[1] \"R2 of current Fumbles to next season's Fumbles: 0.204064081434686\"\n\n[1] \"R2 of current xFmb to next season's Fumbles: 0.106391377897902\"\n\nAs we can see, the “regular” stats outperform all of the expected turnover statistics. But what if we look at these as rate stats per dropback?\n\n\n[1] \"R2 of current TOs per dropback to next season's TOs per dropback: 0.259161533025725\"\n\n[1] \"R2 of current xTOs per dropback to next season's TOs per dropback: 0.315618037803437\"\n\n[1] \"R2 of current Ints per dropback to next season's Ints per dropback: 0.211910004427118\"\n\n[1] \"R2 of current xInts per dropback to next season's Ints per dropback: 0.297221826811028\"\n\nNow we’ve got some winners! We can predict next season’s turnovers and interceptions per dropback more effectively using xTOs and xInts per dropback than we can using the ordinary rate stats.\nVisuals\nFinally, we can plot the results to help us visualize who is most likely to turn the ball over at a higher or lower rate next season.\n\n\nggplot(subset(xTO, season == 2019), aes(x = Turnovers / dropbacks, y = xTO / dropbacks)) +\n  geom_image(aes(image = teamPlayers.headshot_url), size = 0.05, asp = 16 / 9) +\n  labs(\n    title = \"QB Turnovers 2019\",\n    subtitle = \"Regular Season | Min. 250 Dropbacks\",\n    x = \"Actual Turnovers per Dropback\",\n    y = \"Expected Turnovers per Dropback\",\n    caption = \"@AG_8 | Data: @nflfastR\"\n  ) +\n  theme_bw() +\n  theme(\n    aspect.ratio = 9 / 16,\n    plot.title = element_text(size = 12, hjust = 0.5, face = \"bold\"),\n    plot.subtitle = element_text(size = 10, hjust = 0.5),\n  ) +\n  geom_abline(slope = 1, intercept = 0)\n\n\n\n\nggplot(subset(xTO, season == 2019), aes(x = Interceptions / dropbacks, y = xInt / dropbacks)) +\n  geom_image(aes(image = teamPlayers.headshot_url), size = 0.05, asp = 16 / 9) +\n  labs(\n    title = \"QB Interceptions 2019\",\n    subtitle = \"Regular Season | Min. 250 Dropbacks\",\n    x = \"Actual Interceptions per Dropback\",\n    y = \"Expected Interceptions per Dropback\",\n    caption = \"@AG_8 | Data: @nflfastR\"\n  ) +\n  theme_bw() +\n  theme(\n    aspect.ratio = 9 / 16,\n    plot.title = element_text(size = 12, hjust = 0.5, face = \"bold\"),\n    plot.subtitle = element_text(size = 10, hjust = 0.5),\n  ) +\n  geom_abline(slope = 1, intercept = 0)\n\n\nLastly, we’ll use the gt package to make a really cool looking table of the 2019 data.\n\n\nxTO %>%\n  ungroup() %>%\n  filter(season == 2019) %>%\n  select(c(passer, posteam, dropbacks, to_pct, xto_pct, to_pct_diff, int_pct, xint_pct, int_pct_diff)) %>%\n  mutate(\n    to_pct_diff = to_pct_diff * 100,\n    int_pct_diff = int_pct_diff * 100\n  ) %>%\n  gt() %>%\n  tab_header(\n    title = \"Expected QB Turnovers\",\n    subtitle = \"Regular Season 2019 | Min. 250 Dropbacks\"\n  ) %>%\n  cols_label(\n    passer = \"QB\",\n    posteam = \"Team\",\n    dropbacks = \"Dropbacks\",\n    to_pct = \"TOs per Dropback\",\n    xto_pct = \"xTOs per Dropback\",\n    to_pct_diff = \"xTOs/DB - TOs/DB\",\n    int_pct = \"Ints per Dropback\",\n    xint_pct = \"xInts per Dropback\",\n    int_pct_diff = \"xInts/DB - Ints/DB\"\n  ) %>%\n  fmt_number(\n    columns = c(\"to_pct\", \"xto_pct\", \"to_pct_diff\", \"int_pct\", \"xint_pct\", \"int_pct_diff\"),\n    decimals = 2\n  ) %>%\n  fmt_percent(\n    columns = c(\"to_pct\", \"xto_pct\", \"int_pct\", \"xint_pct\")\n  ) %>%\n  tab_source_note(\"@AG_8 | Data: @nflfastR\") %>%\n  data_color(\n    columns = c(\"to_pct\", \"xto_pct\", \"to_pct_diff\", \"int_pct\", \"xint_pct\", \"int_pct_diff\"),\n    colors = scales::col_numeric(palette = \"Reds\", domain = NULL)\n  ) %>%\n  cols_align(align = \"center\") %>%\n  cols_width(\n    everything() ~ px(90)\n  )\nhtml {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#tzvrvewweq .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#tzvrvewweq .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#tzvrvewweq .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#tzvrvewweq .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 4px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#tzvrvewweq .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#tzvrvewweq .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#tzvrvewweq .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#tzvrvewweq .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#tzvrvewweq .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#tzvrvewweq .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#tzvrvewweq .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#tzvrvewweq .gt_group_heading {\n  padding: 8px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#tzvrvewweq .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#tzvrvewweq .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#tzvrvewweq .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#tzvrvewweq .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#tzvrvewweq .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#tzvrvewweq .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 12px;\n}\n\n#tzvrvewweq .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#tzvrvewweq .gt_first_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n}\n\n#tzvrvewweq .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#tzvrvewweq .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#tzvrvewweq .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#tzvrvewweq .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#tzvrvewweq .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding: 4px;\n}\n\n#tzvrvewweq .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#tzvrvewweq .gt_sourcenote {\n  font-size: 90%;\n  padding: 4px;\n}\n\n#tzvrvewweq .gt_left {\n  text-align: left;\n}\n\n#tzvrvewweq .gt_center {\n  text-align: center;\n}\n\n#tzvrvewweq .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#tzvrvewweq .gt_font_normal {\n  font-weight: normal;\n}\n\n#tzvrvewweq .gt_font_bold {\n  font-weight: bold;\n}\n\n#tzvrvewweq .gt_font_italic {\n  font-style: italic;\n}\n\n#tzvrvewweq .gt_super {\n  font-size: 65%;\n}\n\n#tzvrvewweq .gt_footnote_marks {\n  font-style: italic;\n  font-size: 65%;\n}\nExpected QB Turnovers\n    Regular Season 2019 | Min. 250 Dropbacks\n    QB\n      Team\n      Dropbacks\n      TOs per Dropback\n      xTOs per Dropback\n      xTOs/DB - TOs/DB\n      Ints per Dropback\n      xInts per Dropback\n      xInts/DB - Ints/DB\n    A.Dalton\n      CIN\n      601\n      3.00&percnt;\n      2.64&percnt;\n      −0.35\n      2.33&percnt;\n      2.15&percnt;\n      −0.18\n    A.Rodgers\n      GB\n      677\n      1.18&percnt;\n      1.88&percnt;\n      0.70\n      0.59&percnt;\n      1.61&percnt;\n      1.02\n    B.Mayfield\n      CLE\n      642\n      3.58&percnt;\n      3.01&percnt;\n      −0.57\n      3.27&percnt;\n      2.54&percnt;\n      −0.73\n    C.Keenum\n      WAS\n      285\n      2.81&percnt;\n      3.02&percnt;\n      0.21\n      1.75&percnt;\n      1.96&percnt;\n      0.21\n    C.Wentz\n      PHI\n      714\n      1.96&percnt;\n      2.92&percnt;\n      0.96\n      0.98&percnt;\n      1.94&percnt;\n      0.96\n    D.Brees\n      NO\n      410\n      0.98&percnt;\n      1.60&percnt;\n      0.62\n      0.98&percnt;\n      1.60&percnt;\n      0.62\n    D.Haskins\n      WAS\n      256\n      3.52&percnt;\n      2.81&percnt;\n      −0.70\n      2.73&percnt;\n      1.88&percnt;\n      −0.85\n    D.Jones\n      NYG\n      561\n      4.10&percnt;\n      3.89&percnt;\n      −0.21\n      2.14&percnt;\n      2.29&percnt;\n      0.15\n    D.Prescott\n      DAL\n      675\n      1.93&percnt;\n      2.22&percnt;\n      0.30\n      1.63&percnt;\n      1.81&percnt;\n      0.19\n    D.Watson\n      HOU\n      615\n      2.44&percnt;\n      2.53&percnt;\n      0.09\n      1.95&percnt;\n      1.95&percnt;\n      −0.01\n    J.Allen\n      BUF\n      566\n      2.30&percnt;\n      2.75&percnt;\n      0.46\n      1.59&percnt;\n      1.89&percnt;\n      0.30\n    J.Brissett\n      IND\n      540\n      2.04&percnt;\n      2.34&percnt;\n      0.30\n      1.11&percnt;\n      1.78&percnt;\n      0.67\n    J.Flacco\n      DEN\n      313\n      2.56&percnt;\n      2.74&percnt;\n      0.18\n      1.60&percnt;\n      1.43&percnt;\n      −0.16\n    J.Garoppolo\n      SF\n      561\n      3.21&percnt;\n      2.59&percnt;\n      −0.62\n      2.32&percnt;\n      1.89&percnt;\n      −0.43\n    J.Goff\n      LA\n      704\n      2.98&percnt;\n      2.41&percnt;\n      −0.57\n      2.27&percnt;\n      1.80&percnt;\n      −0.47\n    J.Winston\n      TB\n      743\n      4.71&percnt;\n      3.38&percnt;\n      −1.33\n      4.04&percnt;\n      2.62&percnt;\n      −1.42\n    K.Allen\n      CAR\n      575\n      4.00&percnt;\n      3.72&percnt;\n      −0.28\n      2.78&percnt;\n      2.67&percnt;\n      −0.11\n    K.Cousins\n      MIN\n      514\n      1.75&percnt;\n      1.97&percnt;\n      0.22\n      1.17&percnt;\n      1.12&percnt;\n      −0.04\n    K.Murray\n      ARI\n      654\n      2.14&percnt;\n      1.90&percnt;\n      −0.24\n      1.83&percnt;\n      1.70&percnt;\n      −0.13\n    L.Jackson\n      BAL\n      482\n      1.66&percnt;\n      2.16&percnt;\n      0.50\n      1.24&percnt;\n      1.60&percnt;\n      0.35\n    M.Rudolph\n      PIT\n      331\n      2.72&percnt;\n      2.94&percnt;\n      0.22\n      2.72&percnt;\n      2.30&percnt;\n      −0.42\n    M.Ryan\n      ATL\n      731\n      2.60&percnt;\n      2.59&percnt;\n      −0.01\n      1.92&percnt;\n      1.91&percnt;\n      −0.00\n    M.Stafford\n      DET\n      331\n      2.42&percnt;\n      2.91&percnt;\n      0.49\n      1.51&percnt;\n      2.26&percnt;\n      0.75\n    M.Trubisky\n      CHI\n      606\n      1.98&percnt;\n      2.27&percnt;\n      0.29\n      1.65&percnt;\n      1.96&percnt;\n      0.31\n    P.Mahomes\n      KC\n      581\n      1.20&percnt;\n      1.37&percnt;\n      0.16\n      0.86&percnt;\n      1.18&percnt;\n      0.32\n    P.Rivers\n      LAC\n      676\n      3.40&percnt;\n      2.81&percnt;\n      −0.59\n      2.96&percnt;\n      2.30&percnt;\n      −0.66\n    R.Fitzpatrick\n      MIA\n      613\n      2.45&percnt;\n      2.84&percnt;\n      0.39\n      2.12&percnt;\n      2.14&percnt;\n      0.02\n    R.Tannehill\n      TEN\n      349\n      2.58&percnt;\n      3.29&percnt;\n      0.71\n      1.72&percnt;\n      2.45&percnt;\n      0.73\n    R.Wilson\n      SEA\n      651\n      1.23&percnt;\n      2.01&percnt;\n      0.78\n      0.77&percnt;\n      1.48&percnt;\n      0.72\n    S.Darnold\n      NYJ\n      516\n      3.10&percnt;\n      2.54&percnt;\n      −0.56\n      2.52&percnt;\n      1.86&percnt;\n      −0.65\n    T.Brady\n      NE\n      685\n      1.31&percnt;\n      1.63&percnt;\n      0.32\n      1.17&percnt;\n      1.38&percnt;\n      0.21\n    @AG_8 | Data: @nflfastR\n    \n\nConclusion\nBased on the table and plots above, we can see that Jameis had, by far, the largest difference between expected and actual turnovers. This really isn’t much of a shock since you of course need some bad luck to have as high of a turnover rate as he did.\nAdditionally, I found it interesting that although Rodgers and Wentz were two of “luckiest” with turnovers from last year, if they regress to their xTO and xInt numbers they would still be average or maybe even slightly above average, just in terms of turnovers.\nFinally, although Daniel Jones and Kyle Allen were terrible in terms of hanging onto the rock last season, there isn’t a ton of hope for improvement for either. Both had xTO rates just slightly below their actual TO rates.\nWithout tracking data, it really doesn’t make sense to calculate the likelihood of an interception or fumble on all plays.↩\n",
    "preview": "posts/2020-08-25-expected-turnovers/expected-turnovers_files/figure-html5/unnamed-chunk-15-1.png",
    "last_modified": "2020-10-29T09:08:01+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 3300
  },
  {
    "path": "posts/2020-08-24-getting-into-sports-analytics/",
    "title": "Getting into sports analytics",
    "description": "Collection of short answers to common questions.",
    "author": [
      {
        "name": "Ben Baldwin",
        "url": "https://twitter.com/benbbaldwin"
      }
    ],
    "date": "2020-08-24",
    "categories": [
      "Getting started"
    ],
    "contents": "\nTable of Contents\nIntroduction\nLearning R\nShould I learn R or python?\nBig Data Bowl / Machine Learning\nWhat degree should I get?\nI’m in college, what should I do?\nAdvice from people in the industry\nIntroduction\nI get a lot of emails asking about how to start learning things. Rather than being useful to one person, I’m going to start collecting the questions (anonymized) and answers here.\nLearning R\nQ: How can I get started learning R?\nI wrote a beginner’s guide for working with NFL data in R that people seem to find useful\nThis book “R for Data Science” is the go-to for getting started\nFor later on: Advanced R by Hadley Wickham\nShould I learn R or python?\nShort answer: it doesn’t matter, just pick one and get good at it.\nLonger answer: More people in the football public analytics community use R, so there are more resources for getting up and running faster. In addition, if there is one thing that R is better at python at, it is cleaning and manipulating data, so if all you care about is working with data, R might be a better choice to start. At the same time, python is also great, and in the long run if you end up doing data analysis for a career, you’re probably going to end up learning both at some point anyway. And if you start doing machine learning (ML) stuff (see next question), python generally has more packages and tools available, and most ML courses are taught using python.\nBig Data Bowl / Machine Learning\nQ: I know about the NFL big data bowl and that the papers of many finalists are available on the internet. But I believe that I lack the skills needed to understand those papers and use them to answer my questions. What statistics and machine learning resources do you recommend I use to learn the necessary machine learning that can be applied to football, when appropriate, to answer my questions?\nUnfortunately, economics doesn’t give much training in machine learning. I too couldn’t really understand the Big Data Bowl stuff until I took this course (all the videos and homeworks are free and posted online), which was pretty challenging but gives a great foundation for what ML means and how to think about it.\nLecture videos\nLecture slides\nHomework assignments: bottom of page. Completed in python\nThis course is very similar to the famous Stanford 231n course – the instructor used to teach 231n – but the homeworks are in Colab so there’s much less setup involved with getting python up and running. I did the first four assignments and finally could understand the Big Data Bowl winning solution.\nI also highly recommend An Introduction to Statistical Learning, and I have been recommended this book, Bayesian Data Analysis.\nWhat degree should I get?\nQ: Do I need a PhD to get into football analytics?\nNo, definitely not. Getting a PhD is not required and certainly not even expected for doing this kind of stuff, although there are certainly benefits to having one if you enjoy research (note: maximizing earnings is not one of those benefits). The big question is what you want to do. If it’s work for a team, you’d want to beef up your technical skills, perhaps through a Master’s program, and do stuff like compete in the Big Data Bowls, conduct research and get it out to the public, etc. If you want to go to grad school in econ, you’d probably want to do something like gaining research experience working under a professor and, if needed, taking the math/stats classes needed to be a good candidate for grad school. Ultimately this comes down to what you value so there’s no right answer imo, but earning a PhD is way, way overkill if what you want to do is work for a team. And finally, getting into sports (especially with a team) is hard so thinking about what you’d want to do if you don’t is also useful- i.e. ideally one would pick a field that is employable and inherently interesting to them.\nQ: What field should I choose?\nMy background was in econ and that’s not the best preparation for getting into sports analytics (something like statistics or other fields with more exposure to data science / machine learning tools gives better training), with the caveat that I was in school a long time ago so maybe what is taught has chagned since then. With that said, here’s an example program- this is what Sean Clement did prior to getting hired by the Ravens (see in particular the Data Science track). Derrick Yam (Ravens) has a Master’s in biostatistics, Sarah Bailey (Rams) a Master’s in statistics, Sarah Mallepalle (Ravens) a B.S. in Statistics and Machine Learning, etc. These programs are a lot more technical than what you’d get in an MBA (which don’t make them better or worse, just more aligned with what the people getting these jobs are doing). Finally, I’ve heard good things about Coursera but haven’t personally used it.\nI’m in college, what should I do?\nThere’s no one path, but some good answers to this when I posed this question on twitter:\nEthan: “learn to code (R/Python), learn stats, start doing projects, ideally publicly, focus on communicating your results, get domain knowledge, including coach vocabulary”\nFrom NESSIS talks: “Do analytics and publish it”\nCanzhi: “dont major in one of those sports management programs or whatever. learn math / stats. learn to write code. learn how web technologies work so you can scrape your own data. then build stuff and show people :)”\nAdvice from people in the industry\nSee Namita Nandakumar’s excellent thread here. To highlight two tweets:\n\n\n{\"x\":{\"twid\":\"1232146697192603649\",\"pars\":{\"align\":\"center\",\"dnt\":true,\"conversation\":\"none\"}},\"evals\":[],\"jsHooks\":[]}\n{\"x\":{\"twid\":\"1232147414133460994\",\"pars\":{\"align\":\"center\",\"dnt\":true,\"conversation\":\"none\"}},\"evals\":[],\"jsHooks\":[]}\nMatthew Barlowe:\n\n\n{\"x\":{\"twid\":\"1293325909202698241\",\"pars\":{\"align\":\"center\",\"dnt\":true}},\"evals\":[],\"jsHooks\":[]}\nCaio Brighenti:\n\n\n{\"x\":{\"twid\":\"1281941597920296960\",\"pars\":{\"align\":\"center\",\"dnt\":true}},\"evals\":[],\"jsHooks\":[]}\n\n\n",
    "preview": {},
    "last_modified": "2020-10-29T09:08:00+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-08-24-visualizing-epsns-total-qbr-using-interactive-plots/",
    "title": "Visualizing EPSN's Total QBR Using Interactive Plots",
    "description": "How to get ESPN data and create interactive plots using the plotly ggplot2 library.",
    "author": [
      {
        "name": "Sebastian Carl",
        "url": "https://twitter.com/mrcaseb"
      }
    ],
    "date": "2020-08-24",
    "categories": [
      "Scraping",
      "espnscrapeR",
      "Interactive plots",
      "Total QBR",
      "Figures"
    ],
    "contents": "\nTable of Contents\nPreface\nHow to get the data\nCreate some static weekly QBR plots\nLet’s make it interactive\nPreface\nWhen I first started working with ESPN’s Total QBR data I developed my own code to scrape the website. For this post I dug out the old (bad) code and wanted to make it prettier. But at the last minute I remembered something: there is this undervalued R package espnscrapeR developed and maintained by Thomas Mock. So instead of implementing the scraper by myself in this post I’ll show off, how to get all the data and save it locally for later reuse.\nAnd as the title suggests we want to try out something new here: interactive plots!\nHow to get the data\nWe want to use the espnscrapeR in this section. So let’s install it by running the following code block.\n\n\nif (!requireNamespace(\"remotes\", quietly = TRUE)) {\n  install.packages(\"remotes\")\n}\nremotes::install_github(\"jthomasmock/espnscrapeR\")\n\nNow that espnscrapeR is installed we will load weekly Total QBR.\nWe’ll do this by writing the function get_week_qbr which loads the QBR data for a given week and season.\n\n\nget_week_qbr <- function(s, wk) {\n  if (wk <= 17) {\n    out <- espnscrapeR::get_nfl_qbr(season = s, season_type = \"Regular\", week = wk)\n  } else if (wk > 17 & s != 2017) {\n    out <- espnscrapeR::get_nfl_qbr(season = s, season_type = \"Playoffs\", week = wk - 17)\n  }\n}\n\nThen we call this function to actually do the job and save the output. Note: We call the function for the weeks 1 to 21 although the SB is week 22 (in ESPN’s convention) because of the Pro Bowl. The package automatically converts to week 22 for us.\nThe code is somewhat advanced and you may need to do some more background reading to understand it. The short version: we call the above function for all seasons between 2006 and 2019 and all available weeks in parallel running processes and bind everything together in a dataframe which get’s saved to disc for further usage.\n\n\nlibrary(dplyr)\nfuture::plan(\"multiprocess\")\n\n# Total QBR data as available beginning in 2006\n# Note: Playoffs 2017 are missing\nseasons <- 2006:2019\n\nall_qbr <-\n  furrr::future_pmap_dfr(purrr::transpose(purrr::cross2(seasons, 1:21)), get_week_qbr) %>%\n  dplyr::mutate(\n    game_week = as.numeric(game_week),\n    game_week = dplyr::if_else(season_type == \"Playoffs\", game_week + 17, game_week)\n  ) %>%\n  dplyr::arrange(season, game_week)\n\n# save to disk------------------------------------------------------------------\n# binary\nsaveRDS(all_qbr, file = \"all_qbr.rds\")\n\n# ASCII\nreadr::write_csv(all_qbr, \"all_qbr.csv\")\n\nCreate some static weekly QBR plots\nTo better demonstrate the advantages of an interactive plot I want to start with some static charts by plotting weekly QBR for some selected QBs (the insane numbers). We want to plot multiple QBs so let’s create a function again.\nBefore we do this please note there are some Quarterbacks appearing with special team names (e.g. \"DEN/KC\") in the data because they changed their team within a season. Our new function won’t work for those QBs but I was too lazy to rewrite it because I honestly don’t care about those QBs. So please be careful when trying to run the code with one of the following names and corresponding seasons.\n\n\nlibrary(tidyverse)\nreadRDS(\"all_qbr.rds\") %>%\n  dplyr::filter(stringr::str_length(team) > 3) %>%\n  dplyr::group_by(season, short_name, team) %>%\n  dplyr::summarise() %>%\n  knitr::kable(\"simple\", align = \"c\")\nseason\nshort_name\nteam\n2011\nK. Orton\nDEN/KC\n2013\nJ. Freeman\nMIN/TB\n2013\nM. Flynn\nGB/OAK\n2014\nC. Keenum\nSTL/HOU\n2015\nM. Cassel\nBUF/DAL\n2015\nR. Mallett\nBAL/HOU\n2016\nM. Barkley\nCHI/ARI\n2016\nM. Sanchez\nDAL/DEN\n2017\nT.J. Yates\nBUF/HOU\n2018\nM. Barkley\nBUF/CIN\n2019\nJ. Driskel\nCIN/DET\n\nNow let’s write the plotting function plot_weekly_qbr. To call it we have to pass a year (season), the Quarterback name as listed in the ESPN data and our above saved QBR data.\n\n\nplot_weekly_qbr <- function(year, QB, all_qbr_file) {\n\n  # Filter the QB we want to look at, modify team names and week numbers\n  # to fit nflfastR convention and add the team color of the QB\n  single_qbr <- all_qbr_file %>%\n    dplyr::filter(season == year & name == QB) %>%\n    dplyr::mutate(\n      team = dplyr::case_when(\n        team == \"LAR\" ~ \"LA\",\n        team == \"WSH\" ~ \"WAS\",\n        TRUE ~ team\n      ),\n      game_week = dplyr::if_else(game_week > 21, 21, game_week)\n    ) %>%\n    dplyr::left_join(\n      nflfastR::teams_colors_logos %>% select(team_abbr, team_color),\n      by = c(\"team\" = \"team_abbr\")\n    )\n\n  # Search for the QBs opponents using the schedule function of nflfastR\n  # and add logos of the opponents\n  opponents <- nflfastR::fast_scraper_schedules(year) %>%\n    dplyr::filter(home_team == single_qbr$team | away_team == single_qbr$team) %>%\n    dplyr::mutate(\n      opp = dplyr::if_else(home_team == single_qbr$team, away_team, home_team)\n    ) %>%\n    dplyr::left_join(\n      nflfastR::teams_colors_logos %>% select(team_abbr, team_logo_espn),\n      by = c(\"opp\" = \"team_abbr\")\n    ) %>%\n    dplyr::mutate(\n      grob = purrr::map(seq_along(team_logo_espn), function(x) {\n        grid::rasterGrob(magick::image_read(team_logo_espn[[x]]))\n      })\n    )\n\n  # Combine the QBR data of the chosen QB with the game data\n  chart <- single_qbr %>%\n    dplyr::left_join(opponents, by = c(\"game_week\" = \"week\"))\n\n  # Set title string for later usage\n  if (max(chart$game_week) > 17) {\n    title_string <- glue::glue(\"{QB} Weekly Total QBR {year} including Playoffs\")\n  } else {\n    title_string <- glue::glue(\"{QB} Weekly Total QBR {year} Regular Season\")\n  }\n\n  # going to draw some quantile lines and combine them here\n  quantiles <- c(\n    quantile(all_qbr_file$qbr_total, 0.10),\n    quantile(all_qbr_file$qbr_total, 0.25),\n    quantile(all_qbr_file$qbr_total, 0.75),\n    quantile(all_qbr_file$qbr_total, 0.90),\n    quantile(all_qbr_file$qbr_total, 0.98)\n  )\n\n  chart %>%\n    ggplot(aes(x = game_week, y = qbr_total)) +\n    geom_hline(yintercept = quantiles, color = \"black\", linetype = \"dashed\", alpha = 0.7) +\n    geom_hline(yintercept = quantile(all_qbr_file$qbr_total, 0.50), color = \"black\", linetype = \"solid\", alpha = 0.7) +\n    geom_text(x = 0, y = 2 + quantile(all_qbr_file$qbr_total, 0.10), label = \"10th Percentile\", hjust = 1, size = 2) +\n    geom_text(x = 0, y = 2 + quantile(all_qbr_file$qbr_total, 0.25), label = \"25th Percentile\", hjust = 1, size = 2) +\n    geom_text(x = 0, y = 2 + quantile(all_qbr_file$qbr_total, 0.50), label = \"50th Percentile\", hjust = 1, size = 2) +\n    geom_text(x = 0, y = 2 + quantile(all_qbr_file$qbr_total, 0.75), label = \"75th Percentile\", hjust = 1, size = 2) +\n    geom_text(x = 0, y = 2 + quantile(all_qbr_file$qbr_total, 0.90), label = \"90th Percentile\", hjust = 1, size = 2) +\n    geom_text(x = 0, y = 2 + quantile(all_qbr_file$qbr_total, 0.98), label = \"98th Percentile\", hjust = 1, size = 2) +\n    geom_line(colour = chart$team_color) +\n    scale_x_continuous(\n      limits = c(-1.4, NA),\n      breaks = scales::breaks_pretty()\n    ) +\n    ggpmisc::geom_grob(aes(x = game_week, y = qbr_total, label = grob), vp.width = 0.05) +\n    labs(\n      x = \"Game Week\",\n      y = \"ESPN Total QBR\",\n      caption = \"Figure: @mrcaseb | Data: espnscrapeR by @thomas_mock\",\n      title = title_string,\n      subtitle = \"Percentiles of the whole NFL 2006 - 2019 are drawn for orientation\\nTo qualify, a player must play a minimum of 20 action plays per team game\"\n    ) +\n    ggthemes::theme_stata(scheme = \"sj\", base_size = 8) +\n    theme(\n      plot.title = element_text(face = \"bold\"),\n      plot.caption = element_text(hjust = 1),\n      axis.text.y = element_text(angle = 0, vjust = 0.5),\n      legend.title = element_text(size = 8, hjust = 0, vjust = 0.5, face = \"bold\"),\n      legend.position = \"top\",\n      aspect.ratio = 1 / 1.618\n    ) +\n    NULL\n}\n\nWell, that’s a lot of code. Let’s see if it is working properly by choosing the insane 2007 season of Tom Brady (in terms of Total QBR the all-time best season).\n\n\nall_qbr <- readRDS(\"all_qbr.rds\")\nplot_weekly_qbr(2007, \"Tom Brady\", all_qbr)\n\n\nThe other two in the all-time best seasons in terms of Total QBR are Peyton Manning in 2006 and Aaron Rodgers in 2011.\n\n\nplot_weekly_qbr(2006, \"Peyton Manning\", all_qbr)\n\n\n\n\nplot_weekly_qbr(2011, \"Aaron Rodgers\", all_qbr)\n\n\nAnd what does it look like if the season wasn’t that good?\n\n\nplot_weekly_qbr(2019, \"Mitchell Trubisky\", all_qbr)\n\n\nLet’s make it interactive\nThe above plots are nice and all but actually we may miss a lot of information when looking at them. For example, we might want to know\nif the game was at home or on the road,\nwhat the result was or\nhow many QB plays were counted for Total QBR.\nAdditionally we are only looking at a single season because plotting more will be a little overwhelming. Instead, it would be cool if you could watch a range of games from several seasons.\nThis is what we are going to do now. We will use the Plotly ggplot2 Library1 so you’ll have to install it if it’s not installed yet.\nI will use a function again because I want to compare two Quarterbacks. Please note that the code looks similar to the above example but it differs in some points.\nThe additional information will be prompted if you hover with the mouse over an opponents logo. We have to create the shown text in this code block as well.\n\n\ninteractive_weekly_qbr <- function(first_year, last_year, QB, all_qbr_file) {\n\n  # Filter the QB we want to look at, modify team names and week numbers\n  # to fit nflfastR convention and add the team color of the QB\n  single_qbr <- all_qbr_file %>%\n    dplyr::filter(dplyr::between(season, first_year, last_year) & name == QB) %>%\n    dplyr::mutate(\n      team = dplyr::case_when(\n        team == \"LAR\" ~ \"LA\",\n        team == \"WSH\" ~ \"WAS\",\n        team == \"OAK\" ~ \"LV\",\n        TRUE ~ team\n      ),\n      game_week = dplyr::if_else(game_week > 21, 21, game_week),\n      gm = 1:dplyr::n() # this is a running game number within the chosen era. We'll use it to plot\n    ) %>%\n    dplyr::left_join(\n      nflfastR::teams_colors_logos %>% select(team_abbr, team_color),\n      by = c(\"team\" = \"team_abbr\")\n    )\n\n  # Search for the QBs opponents using the schedule function of nflfastR\n  # and add logos of the opponents\n  opponents <- nflfastR::fast_scraper_schedules(first_year:last_year) %>%\n    dplyr::filter(home_team == single_qbr$team | away_team == single_qbr$team) %>%\n    dplyr::mutate(\n      opp = dplyr::if_else(home_team == single_qbr$team, away_team, home_team),\n      opp = dplyr::case_when(\n        opp == \"OAK\" ~ \"LV\",\n        TRUE ~ opp\n      ),\n\n      # create string for game location\n      loc_desc = dplyr::if_else(\n        home_team == single_qbr$team,\n        glue::glue(\"vs. {away_team}\"),\n        glue::glue(\"@ {home_team}\")\n      ),\n\n      # create string for game description\n      game_desc = dplyr::case_when(\n        week == 18 ~ \"Wild Card\",\n        week == 19 ~ \"Divisional Round\",\n        week == 20 ~ \"Conference Championship\",\n        week == 21 ~ \"Super Bowl\",\n        TRUE ~ \"Regular Season\"\n      )\n    ) %>%\n    dplyr::left_join(\n      nflfastR::teams_colors_logos %>% select(team_abbr, opp_color = team_color, team_logo_espn),\n      by = c(\"opp\" = \"team_abbr\")\n    )\n\n  # Combine the QBR data of the chosen QB with the game data\n  chart <- single_qbr %>%\n    dplyr::left_join(opponents, by = c(\"season\", \"game_week\" = \"week\")) %>%\n    dplyr::mutate(\n\n      # create string for game outcome\n      outcome = dplyr::case_when(\n        team == home_team & home_result > 0 ~ \"Won\",\n        team == home_team & home_result < 0 ~ \"Lost\",\n        team == away_team & home_result > 0 ~ \"Lost\",\n        team == away_team & home_result < 0 ~ \"Won\",\n        home_result == 0 ~ \"Tie\",\n        TRUE ~ NA_character_\n      )\n    )\n\n  # Set title string for later usage\n  if (max(chart$game_week) > 17) {\n    title_string <- glue::glue(\"{QB} Weekly Total QBR from {first_year} to {last_year} including Playoffs\")\n  } else {\n    title_string <- glue::glue(\"{QB} Weekly Total QBR from {first_year} to {last_year} Regular Season\")\n  }\n\n  # going to draw some quantile lines and combine them here\n  quantiles <- c(\n    quantile(all_qbr_file$qbr_total, 0.10),\n    quantile(all_qbr_file$qbr_total, 0.25),\n    quantile(all_qbr_file$qbr_total, 0.75),\n    quantile(all_qbr_file$qbr_total, 0.90),\n    quantile(all_qbr_file$qbr_total, 0.98)\n  )\n\n  # Adding the logos to the interactive plots needs to be done in a different way\n  # Here we compute the list needed to add the later\n  image_list <- chart %>%\n    dplyr::transmute(\n      source = team_logo_espn,\n      xref = \"x\",\n      yref = \"y\",\n      x = gm,\n      y = qbr_total,\n      sizex = 7,\n      sizey = 7,\n      opacity = 0.9,\n      xanchor = \"center\",\n      yanchor = \"middle\"\n    ) %>%\n    purrr::transpose()\n\n  plot <-\n    chart %>%\n    ggplot(aes(x = gm, y = qbr_total)) +\n    geom_hline(yintercept = quantiles, color = \"black\", linetype = \"dashed\", alpha = 0.7) +\n    geom_hline(yintercept = quantile(all_qbr_file$qbr_total, 0.50), color = \"black\", linetype = \"solid\", alpha = 0.7) +\n    geom_vline(xintercept = chart$gm[chart$game_week == 1] - 0.5, color = \"black\", linetype = \"dotted\", alpha = 0.7) +\n    geom_text(x = -0.5, y = 2 + quantile(all_qbr_file$qbr_total, 0.10), label = \"10th Percentile\", size = 3.5) +\n    geom_text(x = -0.5, y = 2 + quantile(all_qbr_file$qbr_total, 0.25), label = \"25th Percentile\", size = 3.5) +\n    geom_text(x = -0.5, y = 2 + quantile(all_qbr_file$qbr_total, 0.50), label = \"50th Percentile\", size = 3.5) +\n    geom_text(x = -0.5, y = 2 + quantile(all_qbr_file$qbr_total, 0.75), label = \"75th Percentile\", size = 3.5) +\n    geom_text(x = -0.5, y = 2 + quantile(all_qbr_file$qbr_total, 0.90), label = \"90th Percentile\", size = 3.5) +\n    geom_text(x = -0.5, y = 2 + quantile(all_qbr_file$qbr_total, 0.98), label = \"98th Percentile\", size = 3.5) +\n    geom_line(color = chart$team_color) +\n    geom_point(\n      # The text aesthetics in here are used for the tooltip text in the plotly object\n      aes(text = glue::glue(\n        \"Season: {season}\\nWeek: {game_week} ({game_desc})\\n{outcome} {away_score}:{home_score} {loc_desc}\\nTotal QBR: {qbr_total}\\nNumber of QB plays: {qb_plays}\"\n      )),\n      color = chart$opp_color,\n      size = 0.05\n    ) +\n    scale_x_continuous(\n      limits = c(-1.4, NA),\n      breaks = scales::breaks_pretty()\n    ) +\n    # Subtitle and caption labs don't work in the plotly object so I have removed them\n    # and added them manually to the object using plotly::layout()\n    labs(\n      x = glue::glue(\"Game Number in Given Era ({first_year}-{last_year})\"),\n      y = \"ESPN Total QBR\"\n    ) +\n    theme_bw() +\n    NULL\n\n  # The interactive part is done with plotly\n  plotly::ggplotly(plot, tooltip = \"text\") %>%\n    plotly::layout(\n      title = list(text = glue::glue(\"{title_string}<br><sup>Percentiles of the whole NFL 2006 - 2019 are drawn for orientation.<\/sup>\")),\n      margin = list(t = 50),\n      images = image_list\n    ) %>%\n    plotly::rangeslider(start = -3, end = min(30.5, max(chart$gm) + 0.5), bgcolor = \"#D3D3D3\", thickness = 0.08)\n}\n\nNow we are ready to call the function for specific Quarterbacks and seasons. I don’t want to do overkill so I am just doing three seasons for two Quarterbacks from the 2017 Draft.\n\n\nlibrary(plotly)\ninteractive_weekly_qbr(2017, 2019, \"Patrick Mahomes\", all_qbr)\n\n{\"x\":{\"data\":[{\"x\":[-3.12,34.72,null,-3.12,34.72,null,-3.12,34.72,null,-3.12,34.72,null,-3.12,34.72],\"y\":[16.4,16.4,null,32.3,32.3,null,75.725,75.725,null,88.33,88.33,null,96.1,96.1],\"text\":\"\",\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,0,0,0.7)\",\"dash\":\"dash\"},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[-3.12,34.72],\"y\":[54.9,54.9],\"text\":\"\",\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,0,0,0.7)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1.5,1.5,null,17.5,17.5],\"y\":[12.345,101.555,null,12.345,101.555],\"text\":\"\",\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,0,0,0.7)\",\"dash\":\"dot\"},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5],\"y\":[18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4],\"text\":[\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\"],\"hovertext\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"],\"textfont\":{\"size\":13.2283464566929,\"color\":\"rgba(0,0,0,1)\"},\"type\":\"scatter\",\"mode\":\"text\",\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5],\"y\":[34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3],\"text\":[\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\"],\"hovertext\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"],\"textfont\":{\"size\":13.2283464566929,\"color\":\"rgba(0,0,0,1)\"},\"type\":\"scatter\",\"mode\":\"text\",\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5],\"y\":[56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9],\"text\":[\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\"],\"hovertext\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"],\"textfont\":{\"size\":13.2283464566929,\"color\":\"rgba(0,0,0,1)\"},\"type\":\"scatter\",\"mode\":\"text\",\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5],\"y\":[77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725],\"text\":[\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\"],\"hovertext\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"],\"textfont\":{\"size\":13.2283464566929,\"color\":\"rgba(0,0,0,1)\"},\"type\":\"scatter\",\"mode\":\"text\",\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5],\"y\":[90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33],\"text\":[\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\"],\"hovertext\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"],\"textfont\":{\"size\":13.2283464566929,\"color\":\"rgba(0,0,0,1)\"},\"type\":\"scatter\",\"mode\":\"text\",\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5],\"y\":[98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1],\"text\":[\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\"],\"hovertext\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"],\"textfont\":{\"size\":13.2283464566929,\"color\":\"rgba(0,0,0,1)\"},\"type\":\"scatter\",\"mode\":\"text\",\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33],\"y\":[68.2,85,97,87.7,81.4,69.5,74.5,86.6,74.5,92.2,66.5,76.5,92.8,59.7,64.3,76.4,71.8,90.7,94.6,85.5,66.7,50.5,53.9,83.2,60.6,69.5,57.1,90.7,88.5,59.7,91.5,97.5,61.6],\"text\":\"\",\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(227,24,55,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33],\"y\":[68.2,85,97,87.7,81.4,69.5,74.5,86.6,74.5,92.2,66.5,76.5,92.8,59.7,64.3,76.4,71.8,90.7,94.6,85.5,66.7,50.5,53.9,83.2,60.6,69.5,57.1,90.7,88.5,59.7,91.5,97.5,61.6],\"text\":[\"Season: 2017<br />Week: 17 (Regular Season)<br />Won 27:24 @ DEN<br />Total QBR: 68.2<br />Number of QB plays: 44\",\"Season: 2018<br />Week: 1 (Regular Season)<br />Won 38:28 @ LAC<br />Total QBR: 85<br />Number of QB plays: 36\",\"Season: 2018<br />Week: 2 (Regular Season)<br />Won 42:37 @ PIT<br />Total QBR: 97<br />Number of QB plays: 35\",\"Season: 2018<br />Week: 3 (Regular Season)<br />Won 27:38 vs. SF<br />Total QBR: 87.7<br />Number of QB plays: 51\",\"Season: 2018<br />Week: 4 (Regular Season)<br />Won 27:23 @ DEN<br />Total QBR: 81.4<br />Number of QB plays: 52\",\"Season: 2018<br />Week: 5 (Regular Season)<br />Won 14:30 vs. JAX<br />Total QBR: 69.5<br />Number of QB plays: 46\",\"Season: 2018<br />Week: 6 (Regular Season)<br />Lost 40:43 @ NE<br />Total QBR: 74.5<br />Number of QB plays: 39\",\"Season: 2018<br />Week: 7 (Regular Season)<br />Won 10:45 vs. CIN<br />Total QBR: 86.6<br />Number of QB plays: 47\",\"Season: 2018<br />Week: 8 (Regular Season)<br />Won 23:30 vs. DEN<br />Total QBR: 74.5<br />Number of QB plays: 45\",\"Season: 2018<br />Week: 9 (Regular Season)<br />Won 37:21 @ CLE<br />Total QBR: 92.2<br />Number of QB plays: 42\",\"Season: 2018<br />Week: 10 (Regular Season)<br />Won 14:26 vs. ARI<br />Total QBR: 66.5<br />Number of QB plays: 39\",\"Season: 2018<br />Week: 11 (Regular Season)<br />Lost 51:54 @ LA<br />Total QBR: 76.5<br />Number of QB plays: 59\",\"Season: 2018<br />Week: 13 (Regular Season)<br />Won 40:33 @ OAK<br />Total QBR: 92.8<br />Number of QB plays: 48\",\"Season: 2018<br />Week: 14 (Regular Season)<br />Won 24:27 vs. BAL<br />Total QBR: 59.7<br />Number of QB plays: 63\",\"Season: 2018<br />Week: 15 (Regular Season)<br />Lost 29:28 vs. LAC<br />Total QBR: 64.3<br />Number of QB plays: 46\",\"Season: 2018<br />Week: 16 (Regular Season)<br />Lost 31:38 @ SEA<br />Total QBR: 76.4<br />Number of QB plays: 48\",\"Season: 2018<br />Week: 17 (Regular Season)<br />Won 3:35 vs. OAK<br />Total QBR: 71.8<br />Number of QB plays: 26\",\"Season: 2019<br />Week: 1 (Regular Season)<br />Won 40:26 @ JAX<br />Total QBR: 90.7<br />Number of QB plays: 38\",\"Season: 2019<br />Week: 2 (Regular Season)<br />Won 28:10 @ OAK<br />Total QBR: 94.6<br />Number of QB plays: 51\",\"Season: 2019<br />Week: 3 (Regular Season)<br />Won 28:33 vs. BAL<br />Total QBR: 85.5<br />Number of QB plays: 41\",\"Season: 2019<br />Week: 4 (Regular Season)<br />Won 34:30 @ DET<br />Total QBR: 66.7<br />Number of QB plays: 50\",\"Season: 2019<br />Week: 5 (Regular Season)<br />Lost 19:13 vs. IND<br />Total QBR: 50.5<br />Number of QB plays: 49\",\"Season: 2019<br />Week: 6 (Regular Season)<br />Lost 31:24 vs. HOU<br />Total QBR: 53.9<br />Number of QB plays: 45\",\"Season: 2019<br />Week: 10 (Regular Season)<br />Lost 32:35 @ TEN<br />Total QBR: 83.2<br />Number of QB plays: 56\",\"Season: 2019<br />Week: 11 (Regular Season)<br />Won 24:17 @ LAC<br />Total QBR: 60.6<br />Number of QB plays: 42\",\"Season: 2019<br />Week: 13 (Regular Season)<br />Won 9:40 vs. OAK<br />Total QBR: 69.5<br />Number of QB plays: 38\",\"Season: 2019<br />Week: 14 (Regular Season)<br />Won 23:16 @ NE<br />Total QBR: 57.1<br />Number of QB plays: 49\",\"Season: 2019<br />Week: 15 (Regular Season)<br />Won 3:23 vs. DEN<br />Total QBR: 90.7<br />Number of QB plays: 42\",\"Season: 2019<br />Week: 16 (Regular Season)<br />Won 26:3 @ CHI<br />Total QBR: 88.5<br />Number of QB plays: 38\",\"Season: 2019<br />Week: 17 (Regular Season)<br />Won 21:31 vs. LAC<br />Total QBR: 59.7<br />Number of QB plays: 34\",\"Season: 2019<br />Week: 19 (Divisional Round)<br />Won 31:51 vs. HOU<br />Total QBR: 91.5<br />Number of QB plays: 46\",\"Season: 2019<br />Week: 20 (Conference Championship)<br />Won 24:35 vs. TEN<br />Total QBR: 97.5<br />Number of QB plays: 46\",\"Season: 2019<br />Week: 21 (Super Bowl)<br />Won 20:31 vs. SF<br />Total QBR: 61.6<br />Number of QB plays: 56\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":[\"rgba(0,34,68,1)\",\"rgba(0,34,68,1)\",\"rgba(0,0,0,1)\",\"rgba(170,0,0,1)\",\"rgba(0,34,68,1)\",\"rgba(0,0,0,1)\",\"rgba(0,34,68,1)\",\"rgba(0,0,0,1)\",\"rgba(0,34,68,1)\",\"rgba(251,79,20,1)\",\"rgba(151,35,63,1)\",\"rgba(0,34,68,1)\",\"rgba(165,172,175,1)\",\"rgba(36,23,115,1)\",\"rgba(0,34,68,1)\",\"rgba(0,34,68,1)\",\"rgba(165,172,175,1)\",\"rgba(0,0,0,1)\",\"rgba(165,172,175,1)\",\"rgba(36,23,115,1)\",\"rgba(0,90,139,1)\",\"rgba(0,44,95,1)\",\"rgba(3,32,47,1)\",\"rgba(0,34,68,1)\",\"rgba(0,34,68,1)\",\"rgba(165,172,175,1)\",\"rgba(0,34,68,1)\",\"rgba(0,34,68,1)\",\"rgba(11,22,42,1)\",\"rgba(0,34,68,1)\",\"rgba(3,32,47,1)\",\"rgba(0,34,68,1)\",\"rgba(170,0,0,1)\"],\"opacity\":1,\"size\":0.188976377952756,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":[\"rgba(0,34,68,1)\",\"rgba(0,34,68,1)\",\"rgba(0,0,0,1)\",\"rgba(170,0,0,1)\",\"rgba(0,34,68,1)\",\"rgba(0,0,0,1)\",\"rgba(0,34,68,1)\",\"rgba(0,0,0,1)\",\"rgba(0,34,68,1)\",\"rgba(251,79,20,1)\",\"rgba(151,35,63,1)\",\"rgba(0,34,68,1)\",\"rgba(165,172,175,1)\",\"rgba(36,23,115,1)\",\"rgba(0,34,68,1)\",\"rgba(0,34,68,1)\",\"rgba(165,172,175,1)\",\"rgba(0,0,0,1)\",\"rgba(165,172,175,1)\",\"rgba(36,23,115,1)\",\"rgba(0,90,139,1)\",\"rgba(0,44,95,1)\",\"rgba(3,32,47,1)\",\"rgba(0,34,68,1)\",\"rgba(0,34,68,1)\",\"rgba(165,172,175,1)\",\"rgba(0,34,68,1)\",\"rgba(0,34,68,1)\",\"rgba(11,22,42,1)\",\"rgba(0,34,68,1)\",\"rgba(3,32,47,1)\",\"rgba(0,34,68,1)\",\"rgba(170,0,0,1)\"]}},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":50,\"r\":7.30593607305936,\"b\":48.9497716894977,\"l\":43.1050228310502},\"plot_bgcolor\":\"rgba(255,255,255,1)\",\"paper_bgcolor\":\"rgba(255,255,255,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-3,30.5],\"tickmode\":\"array\",\"ticktext\":[\"0\",\"10\",\"20\",\"30\"],\"tickvals\":[4.44089209850063e-16,10,20,30],\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"10\",\"20\",\"30\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"Game Number in Given Era (2017-2019)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\",\"rangeslider\":{\"visible\":true,\"bgcolor\":\"#D3D3D3\",\"thickness\":0.08}},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[12.345,101.555],\"tickmode\":\"array\",\"ticktext\":[\"25\",\"50\",\"75\",\"100\"],\"tickvals\":[25,50,75,100],\"categoryorder\":\"array\",\"categoryarray\":[\"25\",\"50\",\"75\",\"100\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"ESPN Total QBR\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":\"transparent\",\"line\":{\"color\":\"rgba(51,51,51,1)\",\"width\":0.66417600664176,\"linetype\":\"solid\"},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":1.88976377952756,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895}},\"hovermode\":\"closest\",\"barmode\":\"relative\",\"title\":{\"text\":\"Patrick Mahomes Weekly Total QBR from 2017 to 2019 including Playoffs<br><sup>Percentiles of the whole NFL 2006 - 2019 are drawn for orientation.<\\/sup>\"},\"images\":[{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/den.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":1,\"y\":68.2,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/lac.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":2,\"y\":85,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/pit.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":3,\"y\":97,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/sf.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":4,\"y\":87.7,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/den.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":5,\"y\":81.4,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/jax.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":6,\"y\":69.5,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/ne.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":7,\"y\":74.5,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/cin.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":8,\"y\":86.6,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/den.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":9,\"y\":74.5,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/cle.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":10,\"y\":92.2,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/ari.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":11,\"y\":66.5,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/lar.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":12,\"y\":76.5,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/lv.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":13,\"y\":92.8,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/bal.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":14,\"y\":59.7,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/lac.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":15,\"y\":64.3,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/sea.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":16,\"y\":76.4,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/lv.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":17,\"y\":71.8,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/jax.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":18,\"y\":90.7,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/lv.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":19,\"y\":94.6,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/bal.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":20,\"y\":85.5,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/det.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":21,\"y\":66.7,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/ind.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":22,\"y\":50.5,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/hou.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":23,\"y\":53.9,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/ten.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":24,\"y\":83.2,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/lac.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":25,\"y\":60.6,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/lv.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":26,\"y\":69.5,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/ne.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":27,\"y\":57.1,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/den.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":28,\"y\":90.7,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/chi.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":29,\"y\":88.5,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/lac.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":30,\"y\":59.7,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/hou.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":31,\"y\":91.5,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/ten.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":32,\"y\":97.5,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/sf.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":33,\"y\":61.6,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"}]},\"config\":{\"doubleClick\":\"reset\",\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"a3c71c42589\":{\"yintercept\":{},\"type\":\"scatter\"},\"a3c259b7311\":{\"yintercept\":{}},\"a3c10067157\":{\"xintercept\":{}},\"a3c42d16a86\":{\"x\":{},\"y\":{}},\"a3c3c01203d\":{\"x\":{},\"y\":{}},\"a3c559f57cb\":{\"x\":{},\"y\":{}},\"a3c24fa4d3f\":{\"x\":{},\"y\":{}},\"a3c4d0d6c22\":{\"x\":{},\"y\":{}},\"a3c24df62d3\":{\"x\":{},\"y\":{}},\"a3c5a19dd3\":{\"x\":{},\"y\":{}},\"a3c392c6294\":{\"text\":{},\"x\":{},\"y\":{}}},\"cur_data\":\"a3c71c42589\",\"visdat\":{\"a3c71c42589\":[\"function (y) \",\"x\"],\"a3c259b7311\":[\"function (y) \",\"x\"],\"a3c10067157\":[\"function (y) \",\"x\"],\"a3c42d16a86\":[\"function (y) \",\"x\"],\"a3c3c01203d\":[\"function (y) \",\"x\"],\"a3c559f57cb\":[\"function (y) \",\"x\"],\"a3c24fa4d3f\":[\"function (y) \",\"x\"],\"a3c4d0d6c22\":[\"function (y) \",\"x\"],\"a3c24df62d3\":[\"function (y) \",\"x\"],\"a3c5a19dd3\":[\"function (y) \",\"x\"],\"a3c392c6294\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\n\n\ninteractive_weekly_qbr(2017, 2019, \"Mitchell Trubisky\", all_qbr)\n\n{\"x\":{\"data\":[{\"x\":[-3.42,41.02,null,-3.42,41.02,null,-3.42,41.02,null,-3.42,41.02,null,-3.42,41.02],\"y\":[16.4,16.4,null,32.3,32.3,null,75.725,75.725,null,88.33,88.33,null,96.1,96.1],\"text\":\"\",\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,0,0,0.7)\",\"dash\":\"dash\"},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[-3.42,41.02],\"y\":[54.9,54.9],\"text\":\"\",\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,0,0,0.7)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[11.5,11.5,null,25.5,25.5],\"y\":[-4.275,102.975,null,-4.275,102.975],\"text\":\"\",\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,0,0,0.7)\",\"dash\":\"dot\"},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5],\"y\":[18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4,18.4],\"text\":[\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\",\"10th Percentile\"],\"hovertext\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"],\"textfont\":{\"size\":13.2283464566929,\"color\":\"rgba(0,0,0,1)\"},\"type\":\"scatter\",\"mode\":\"text\",\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5],\"y\":[34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3,34.3],\"text\":[\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\",\"25th Percentile\"],\"hovertext\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"],\"textfont\":{\"size\":13.2283464566929,\"color\":\"rgba(0,0,0,1)\"},\"type\":\"scatter\",\"mode\":\"text\",\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5],\"y\":[56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9,56.9],\"text\":[\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\",\"50th Percentile\"],\"hovertext\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"],\"textfont\":{\"size\":13.2283464566929,\"color\":\"rgba(0,0,0,1)\"},\"type\":\"scatter\",\"mode\":\"text\",\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5],\"y\":[77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725,77.725],\"text\":[\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\",\"75th Percentile\"],\"hovertext\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"],\"textfont\":{\"size\":13.2283464566929,\"color\":\"rgba(0,0,0,1)\"},\"type\":\"scatter\",\"mode\":\"text\",\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5],\"y\":[90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33,90.33],\"text\":[\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\",\"90th Percentile\"],\"hovertext\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"],\"textfont\":{\"size\":13.2283464566929,\"color\":\"rgba(0,0,0,1)\"},\"type\":\"scatter\",\"mode\":\"text\",\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5],\"y\":[98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1,98.1],\"text\":[\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\",\"98th Percentile\"],\"hovertext\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"],\"textfont\":{\"size\":13.2283464566929,\"color\":\"rgba(0,0,0,1)\"},\"type\":\"scatter\",\"mode\":\"text\",\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39],\"y\":[17.5,50.4,45.6,28.3,55.3,0.6,40.3,65.5,21,55.8,32.5,27.5,52.1,31.4,98.1,71.6,83,90.1,59.1,94.3,84.2,25.5,68,71,89.7,26.4,35.6,55.4,27.3,41,19.6,43.7,48.4,31.3,64.3,82.1,43.4,5.6,32.9],\"text\":\"\",\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(11,22,42,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39],\"y\":[17.5,50.4,45.6,28.3,55.3,0.6,40.3,65.5,21,55.8,32.5,27.5,52.1,31.4,98.1,71.6,83,90.1,59.1,94.3,84.2,25.5,68,71,89.7,26.4,35.6,55.4,27.3,41,19.6,43.7,48.4,31.3,64.3,82.1,43.4,5.6,32.9],\"text\":[\"Season: 2017<br />Week: 5 (Regular Season)<br />Lost 20:17 vs. MIN<br />Total QBR: 17.5<br />Number of QB plays: 34\",\"Season: 2017<br />Week: 6 (Regular Season)<br />Won 27:24 @ BAL<br />Total QBR: 50.4<br />Number of QB plays: 24\",\"Season: 2017<br />Week: 8 (Regular Season)<br />Lost 12:20 @ NO<br />Total QBR: 45.6<br />Number of QB plays: 40\",\"Season: 2017<br />Week: 10 (Regular Season)<br />Lost 23:16 vs. GB<br />Total QBR: 28.3<br />Number of QB plays: 45\",\"Season: 2017<br />Week: 11 (Regular Season)<br />Lost 27:24 vs. DET<br />Total QBR: 55.3<br />Number of QB plays: 38\",\"Season: 2017<br />Week: 12 (Regular Season)<br />Lost 3:31 @ PHI<br />Total QBR: 0.6<br />Number of QB plays: 45\",\"Season: 2017<br />Week: 13 (Regular Season)<br />Lost 15:14 vs. SF<br />Total QBR: 40.3<br />Number of QB plays: 21\",\"Season: 2017<br />Week: 14 (Regular Season)<br />Won 33:7 @ CIN<br />Total QBR: 65.5<br />Number of QB plays: 39\",\"Season: 2017<br />Week: 15 (Regular Season)<br />Lost 10:20 @ DET<br />Total QBR: 21<br />Number of QB plays: 57\",\"Season: 2017<br />Week: 16 (Regular Season)<br />Won 3:20 vs. CLE<br />Total QBR: 55.8<br />Number of QB plays: 38\",\"Season: 2017<br />Week: 17 (Regular Season)<br />Lost 10:23 @ MIN<br />Total QBR: 32.5<br />Number of QB plays: 42\",\"Season: 2018<br />Week: 1 (Regular Season)<br />Lost 23:24 @ GB<br />Total QBR: 27.5<br />Number of QB plays: 46\",\"Season: 2018<br />Week: 2 (Regular Season)<br />Won 17:24 vs. SEA<br />Total QBR: 52.1<br />Number of QB plays: 43\",\"Season: 2018<br />Week: 3 (Regular Season)<br />Won 16:14 @ ARI<br />Total QBR: 31.4<br />Number of QB plays: 43\",\"Season: 2018<br />Week: 4 (Regular Season)<br />Won 10:48 vs. TB<br />Total QBR: 98.1<br />Number of QB plays: 36\",\"Season: 2018<br />Week: 6 (Regular Season)<br />Lost 28:31 @ MIA<br />Total QBR: 71.6<br />Number of QB plays: 45\",\"Season: 2018<br />Week: 7 (Regular Season)<br />Lost 38:31 vs. NE<br />Total QBR: 83<br />Number of QB plays: 61\",\"Season: 2018<br />Week: 8 (Regular Season)<br />Won 10:24 vs. NYJ<br />Total QBR: 90.1<br />Number of QB plays: 39\",\"Season: 2018<br />Week: 9 (Regular Season)<br />Won 41:9 @ BUF<br />Total QBR: 59.1<br />Number of QB plays: 28\",\"Season: 2018<br />Week: 10 (Regular Season)<br />Won 22:34 vs. DET<br />Total QBR: 94.3<br />Number of QB plays: 38\",\"Season: 2018<br />Week: 11 (Regular Season)<br />Won 20:25 vs. MIN<br />Total QBR: 84.2<br />Number of QB plays: 41\",\"Season: 2018<br />Week: 14 (Regular Season)<br />Won 6:15 vs. LA<br />Total QBR: 25.5<br />Number of QB plays: 39\",\"Season: 2018<br />Week: 15 (Regular Season)<br />Won 17:24 vs. GB<br />Total QBR: 68<br />Number of QB plays: 38\",\"Season: 2018<br />Week: 16 (Regular Season)<br />Won 14:9 @ SF<br />Total QBR: 71<br />Number of QB plays: 37\",\"Season: 2018<br />Week: 17 (Regular Season)<br />Won 24:10 @ MIN<br />Total QBR: 89.7<br />Number of QB plays: 31\",\"Season: 2019<br />Week: 1 (Regular Season)<br />Lost 10:3 vs. GB<br />Total QBR: 26.4<br />Number of QB plays: 58\",\"Season: 2019<br />Week: 2 (Regular Season)<br />Won 16:14 @ DEN<br />Total QBR: 35.6<br />Number of QB plays: 32\",\"Season: 2019<br />Week: 3 (Regular Season)<br />Won 31:15 @ WAS<br />Total QBR: 55.4<br />Number of QB plays: 41\",\"Season: 2019<br />Week: 7 (Regular Season)<br />Lost 36:25 vs. NO<br />Total QBR: 27.3<br />Number of QB plays: 58\",\"Season: 2019<br />Week: 8 (Regular Season)<br />Lost 17:16 vs. LAC<br />Total QBR: 41<br />Number of QB plays: 46\",\"Season: 2019<br />Week: 9 (Regular Season)<br />Lost 14:22 @ PHI<br />Total QBR: 19.6<br />Number of QB plays: 28\",\"Season: 2019<br />Week: 10 (Regular Season)<br />Won 13:20 vs. DET<br />Total QBR: 43.7<br />Number of QB plays: 34\",\"Season: 2019<br />Week: 11 (Regular Season)<br />Lost 7:17 @ LA<br />Total QBR: 48.4<br />Number of QB plays: 47\",\"Season: 2019<br />Week: 12 (Regular Season)<br />Won 14:19 vs. NYG<br />Total QBR: 31.3<br />Number of QB plays: 51\",\"Season: 2019<br />Week: 13 (Regular Season)<br />Won 24:20 @ DET<br />Total QBR: 64.3<br />Number of QB plays: 43\",\"Season: 2019<br />Week: 14 (Regular Season)<br />Won 24:31 vs. DAL<br />Total QBR: 82.1<br />Number of QB plays: 48\",\"Season: 2019<br />Week: 15 (Regular Season)<br />Lost 13:21 @ GB<br />Total QBR: 43.4<br />Number of QB plays: 60\",\"Season: 2019<br />Week: 16 (Regular Season)<br />Lost 26:3 vs. KC<br />Total QBR: 5.6<br />Number of QB plays: 50\",\"Season: 2019<br />Week: 17 (Regular Season)<br />Won 21:19 @ MIN<br />Total QBR: 32.9<br />Number of QB plays: 44\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":[\"rgba(79,38,131,1)\",\"rgba(36,23,115,1)\",\"rgba(159,137,88,1)\",\"rgba(32,55,49,1)\",\"rgba(0,90,139,1)\",\"rgba(0,73,83,1)\",\"rgba(170,0,0,1)\",\"rgba(0,0,0,1)\",\"rgba(0,90,139,1)\",\"rgba(251,79,20,1)\",\"rgba(79,38,131,1)\",\"rgba(32,55,49,1)\",\"rgba(0,34,68,1)\",\"rgba(151,35,63,1)\",\"rgba(213,10,10,1)\",\"rgba(0,142,151,1)\",\"rgba(0,34,68,1)\",\"rgba(32,55,49,1)\",\"rgba(0,51,141,1)\",\"rgba(0,90,139,1)\",\"rgba(79,38,131,1)\",\"rgba(0,34,68,1)\",\"rgba(32,55,49,1)\",\"rgba(170,0,0,1)\",\"rgba(79,38,131,1)\",\"rgba(32,55,49,1)\",\"rgba(0,34,68,1)\",\"rgba(119,49,65,1)\",\"rgba(159,137,88,1)\",\"rgba(0,34,68,1)\",\"rgba(0,73,83,1)\",\"rgba(0,90,139,1)\",\"rgba(0,34,68,1)\",\"rgba(11,34,101,1)\",\"rgba(0,90,139,1)\",\"rgba(0,34,68,1)\",\"rgba(32,55,49,1)\",\"rgba(227,24,55,1)\",\"rgba(79,38,131,1)\"],\"opacity\":1,\"size\":0.188976377952756,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":[\"rgba(79,38,131,1)\",\"rgba(36,23,115,1)\",\"rgba(159,137,88,1)\",\"rgba(32,55,49,1)\",\"rgba(0,90,139,1)\",\"rgba(0,73,83,1)\",\"rgba(170,0,0,1)\",\"rgba(0,0,0,1)\",\"rgba(0,90,139,1)\",\"rgba(251,79,20,1)\",\"rgba(79,38,131,1)\",\"rgba(32,55,49,1)\",\"rgba(0,34,68,1)\",\"rgba(151,35,63,1)\",\"rgba(213,10,10,1)\",\"rgba(0,142,151,1)\",\"rgba(0,34,68,1)\",\"rgba(32,55,49,1)\",\"rgba(0,51,141,1)\",\"rgba(0,90,139,1)\",\"rgba(79,38,131,1)\",\"rgba(0,34,68,1)\",\"rgba(32,55,49,1)\",\"rgba(170,0,0,1)\",\"rgba(79,38,131,1)\",\"rgba(32,55,49,1)\",\"rgba(0,34,68,1)\",\"rgba(119,49,65,1)\",\"rgba(159,137,88,1)\",\"rgba(0,34,68,1)\",\"rgba(0,73,83,1)\",\"rgba(0,90,139,1)\",\"rgba(0,34,68,1)\",\"rgba(11,34,101,1)\",\"rgba(0,90,139,1)\",\"rgba(0,34,68,1)\",\"rgba(32,55,49,1)\",\"rgba(227,24,55,1)\",\"rgba(79,38,131,1)\"]}},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":50,\"r\":7.30593607305936,\"b\":48.9497716894977,\"l\":43.1050228310502},\"plot_bgcolor\":\"rgba(255,255,255,1)\",\"paper_bgcolor\":\"rgba(255,255,255,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-3,30.5],\"tickmode\":\"array\",\"ticktext\":[\"0\",\"10\",\"20\",\"30\",\"40\"],\"tickvals\":[0,10,20,30,40],\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"10\",\"20\",\"30\",\"40\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"Game Number in Given Era (2017-2019)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\",\"rangeslider\":{\"visible\":true,\"bgcolor\":\"#D3D3D3\",\"thickness\":0.08}},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-4.275,102.975],\"tickmode\":\"array\",\"ticktext\":[\"0\",\"25\",\"50\",\"75\",\"100\"],\"tickvals\":[0,25,50,75,100],\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"25\",\"50\",\"75\",\"100\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"ESPN Total QBR\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":\"transparent\",\"line\":{\"color\":\"rgba(51,51,51,1)\",\"width\":0.66417600664176,\"linetype\":\"solid\"},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":1.88976377952756,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895}},\"hovermode\":\"closest\",\"barmode\":\"relative\",\"title\":{\"text\":\"Mitchell Trubisky Weekly Total QBR from 2017 to 2019 Regular Season<br><sup>Percentiles of the whole NFL 2006 - 2019 are drawn for orientation.<\\/sup>\"},\"images\":[{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/min.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":1,\"y\":17.5,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/bal.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":2,\"y\":50.4,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/no.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":3,\"y\":45.6,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/gb.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":4,\"y\":28.3,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/det.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":5,\"y\":55.3,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/phi.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":6,\"y\":0.6,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/sf.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":7,\"y\":40.3,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/cin.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":8,\"y\":65.5,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/det.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":9,\"y\":21,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/cle.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":10,\"y\":55.8,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/min.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":11,\"y\":32.5,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/gb.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":12,\"y\":27.5,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/sea.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":13,\"y\":52.1,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/ari.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":14,\"y\":31.4,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/tb.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":15,\"y\":98.1,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/mia.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":16,\"y\":71.6,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/ne.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":17,\"y\":83,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/nyj.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":18,\"y\":90.1,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/buf.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":19,\"y\":59.1,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/det.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":20,\"y\":94.3,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/min.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":21,\"y\":84.2,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/lar.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":22,\"y\":25.5,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/gb.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":23,\"y\":68,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/sf.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":24,\"y\":71,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/min.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":25,\"y\":89.7,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/gb.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":26,\"y\":26.4,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/den.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":27,\"y\":35.6,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/wsh.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":28,\"y\":55.4,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/no.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":29,\"y\":27.3,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/lac.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":30,\"y\":41,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/phi.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":31,\"y\":19.6,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/det.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":32,\"y\":43.7,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/lar.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":33,\"y\":48.4,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/nyg.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":34,\"y\":31.3,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/det.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":35,\"y\":64.3,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/dal.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":36,\"y\":82.1,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/gb.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":37,\"y\":43.4,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/kc.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":38,\"y\":5.6,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"},{\"source\":\"https://a.espncdn.com/i/teamlogos/nfl/500/min.png\",\"xref\":\"x\",\"yref\":\"y\",\"x\":39,\"y\":32.9,\"sizex\":7,\"sizey\":7,\"opacity\":0.9,\"xanchor\":\"center\",\"yanchor\":\"middle\"}]},\"config\":{\"doubleClick\":\"reset\",\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"a3c4f36506c\":{\"yintercept\":{},\"type\":\"scatter\"},\"a3c758f5f69\":{\"yintercept\":{}},\"a3ca245990\":{\"xintercept\":{}},\"a3c58eb32d4\":{\"x\":{},\"y\":{}},\"a3c525457dd\":{\"x\":{},\"y\":{}},\"a3c752e5c36\":{\"x\":{},\"y\":{}},\"a3c38c25f1c\":{\"x\":{},\"y\":{}},\"a3ccf3ac\":{\"x\":{},\"y\":{}},\"a3c32437f75\":{\"x\":{},\"y\":{}},\"a3c76e91ac6\":{\"x\":{},\"y\":{}},\"a3c6f681828\":{\"text\":{},\"x\":{},\"y\":{}}},\"cur_data\":\"a3c4f36506c\",\"visdat\":{\"a3c4f36506c\":[\"function (y) \",\"x\"],\"a3c758f5f69\":[\"function (y) \",\"x\"],\"a3ca245990\":[\"function (y) \",\"x\"],\"a3c58eb32d4\":[\"function (y) \",\"x\"],\"a3c525457dd\":[\"function (y) \",\"x\"],\"a3c752e5c36\":[\"function (y) \",\"x\"],\"a3c38c25f1c\":[\"function (y) \",\"x\"],\"a3ccf3ac\":[\"function (y) \",\"x\"],\"a3c32437f75\":[\"function (y) \",\"x\"],\"a3c76e91ac6\":[\"function (y) \",\"x\"],\"a3c6f681828\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\nThere are multiple possible packages usable for this website. I suggest starting here.↩︎\n",
    "preview": "posts/2020-08-24-visualizing-epsns-total-qbr-using-interactive-plots/visualizing-epsns-total-qbr-using-interactive-plots_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2020-10-29T09:08:00+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 3000
  },
  {
    "path": "posts/2020-08-23-exploring-wins-with-nflfastr/",
    "title": "Exploring Wins with nflfastR",
    "description": "Looking at what metrics are important for predicting wins. Creating expected season win totals and comparing to reality.",
    "author": [
      {
        "name": "Austin Ryan",
        "url": "https://twitter.com/packeRanalytics"
      }
    ],
    "date": "2020-08-23",
    "categories": [
      "Tidymodels",
      "Figures",
      "nflfastR"
    ],
    "contents": "\nTable of Contents\nSimple Linear Regression\nRandom Forest Variable Importance\nMultiple Linear Regression\nHow did expected and actual wins look in 2019?\nWhat does this mean for the 2020 season?\nOther Findings\nWhat can two decades worth of play-by-play data and some math tell us about what wins games in the NFL? Let’s look at some simple linear regressions using metrics we can easily compute with nflfastR data.\nPlease note code chunks have been intentionally hidden in this post for readability. See the rmd file at https://github.com/mrcaseb/open-source-football/ if you would like to see the underlying code.\nSimple Linear Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see passing efficiency metrics have the strongest relationships with wins. Furthermore, offensive passing efficiency metrics have stronger relationships than defensive passing metrics do.\nA team’s expected points added per dropback explains nearly half of the variation in their season win total. Whereas defensive expected points added per dropback explains about 32% of the variation in wins. Offensive and defensive rushing efficiency metrics only explain about 18 and 9% of the variation in wins respectively.\n\n\n\n\n\n\nRandom Forest Variable Importance\nWe can also build a random forest model and let the model tell us what features yield the most information gain. Again, passing efficiency is the largest driver of wins and it is not particularly close.\n\n\n\n\n\n\n\n\n\nMultiple Linear Regression\nWe know offensive and defensive EPA per dropback metrics are useful for explaining season win totals. Just for fun make a linear regression model that uses EPA per dropback and per rush for both sides of the ball. This regression explains 78% of the variation in season wins.\nWe can use the regression formula to develop expected wins based on EPA per play metrics. The distribution of actual wins minus expected wins is normally distributed with a mean of 0 and a standard deviation of 1.4 wins.\nThis means 68% of the season win totals from 2009-2019 are plus or minus 1.4 wins from what our expected wins formula predicts. Furthermore, 95% of the season win totals are within 3 games of what we would predict. Put another way, it is rare for a team to out or underperform their expected wins by more than 3 games.\n\n\n\n\n\n\n\n\n\nHow did expected and actual wins look in 2019?\n\n\n\n\n\n\n\n\n\nBased on our expected wins formula the NFC North champs were predicted to have 10 wins while they actual won 13. Additionally, the team they beat to get to the NFC Championship looked more like an 8 win team rather than an 11 win team. On the other end of the spectrum the Cowboys produced EPA per play metrics that predicted an 11 win team, however, they ended up 3 wins short.\n\n\n\n\n\n\n\n\n\nWhat does this mean for the 2020 season?\nLooking at the 25 teams in the right tail (those who over performed by more than 2.5 wins) from 1999 to 2018 we find that on average their wins dropped by 2.3 games in the next season. Not great news for Packers or Seahawks fans in 2020.\nThe 29 teams n the left tail we see that teams who under performed by more than 2.5 wins increased their wins by 2.7 games the next season. The 2019 Cowboys, Chargers, and Buccaneers also fall into this tail.\n\n\n\n\n\n\nIf we look at teams who over performed by more than 2 games (56 from 1999 to 2018) we see their wins drop on average by 2.6 games the next season. Conversely, teams who under perform by more than 2 games (50 from 1999 to 2018) increase their wins the next season by 2.6 games on average.\nOther Findings\nThe difference between actual and expected wins is largely a function of how a team performs in one score games and on special teams performance. Record in one score games isn’t very stable year over year for the most part, however, a few teams did consistently out or over perform their expected wins.\nOf the 669 season long performances in the data only 38 teams under performed by more than 2.35 wins. The Chargers account for over a fifth of those seasons.\n\n\n\n\n\n\nThe Browns have not over performed since 2009 when they won 5 games but this model saw them as more of a 2 win team.\n\n\n\nOn the other end of the spectrum the Patriots have only under performed by more than half a game two times.\n\n\n\n\n\n",
    "preview": "posts/2020-08-23-exploring-wins-with-nflfastr/exploring-wins-with-nflfastr_files/figure-html5/unnamed-chunk-10-1.png",
    "last_modified": "2020-10-29T09:08:00+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 2400
  },
  {
    "path": "posts/2020-08-22-ranking-qbs-using-era-adjusted-elo/",
    "title": "Ranking QBs Using Era Adjusted Elo",
    "description": "Use 538's QB Elo value, a highly predictive measurement of QB impact, to compare QB careers across era",
    "author": [
      {
        "name": "Robby Greer",
        "url": "https://twitter.com/greerreNFL"
      }
    ],
    "date": "2020-08-22",
    "categories": [
      "Elo",
      "python"
    ],
    "contents": "\nTable of Contents\nPart 0: Background and summary\nPart 1: Importing and cleaning data\nPart 2: Adjusting for era\nPart 3: Adding stats and compiling careers\nPart 4: Graphing careers\nPart 0: Background and summary\nElo is a ranking and prediction framework that 538 has successfully applied to the NFL. Because QB performance plays such a strong role in overall team performance, 538’s Elo framework models QB contributions separately before adding them back to the overall team grade.\nThese QB rankings significantly improve the overall predictive power of the framework, making them a fairly accurate measure of a QB’s value. Every 25 points of Elo are equivalent to roughly 1 point of expected game margin. For instance, a QB with an Elo of 100 would be worth roughly 4 points more per game than a replacement level QB.\nMeasuring the cumulative Elo added by a QB over the course of their career is akin to measuring the total points added above a replacement level player. In this post, QB Elo values are pulled from 538 and normalized by era, allowing for an interesting comparison of QB careers throughout the history of the NFL. As QB rankings can be a touchy subject, it is worth noting that these rankings are just one quantitative view of a QBs overall performance.\nPart 1: Importing and cleaning data\nFirst, import packages:\n\n\nimport pandas as pd\nimport numpy\nimport requests\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nNext, pull Elo data from 538 and load it into a pandas data frame:\n\n\ndata_link = 'https://projects.fivethirtyeight.com/nfl-api/nfl_elo.csv'\ndata_df = pd.read_csv(data_link)\n\nWe only want data with QB grades, and we’ll exclude playoffs:\n\n\ndata_df = data_df[(~numpy.isnan(data_df['qbelo1_pre'])) & (~numpy.isnan(data_df['qbelo2_pre']))]\ndata_df = data_df[data_df['playoff'].isna()]\n\nElo game data comes with game dates, not weeks:\n\n\ndata_df['date'].sample(5)\n\n12007    2003-09-07\n12393    2004-11-07\n9108     1991-10-20\n4180     1968-09-15\n3363     1963-09-15\nName: date, dtype: object\n\nWhich we’ll want to convert to weeks to make them easier to group:\n\n\n## create a datetime series from the date ##\ndata_df['date_time'] = pd.to_datetime(data_df['date'])\n\n## mondays are new weeks, so subtract a day and then trunc to get an NFL week ##\ndata_df['date_time'] = data_df['date_time'] - pd.Timedelta(days=1)\ndata_df['week_of'] = data_df['date_time'].dt.week\n\nNext, separate Home and Away data and merge it into a single flat file, filtering out unnecessary fields and renaming columns in the process:\n\n\n## create a flat file ##\nhome_data_df = data_df.copy()[[\n    'season',\n    'week_of',\n    'qb1',\n    'qb1_value_post',\n    'score1',\n    'score2'\n]].rename(columns={\n    'qb1' : 'qb_name',\n    'qb1_value_post' : 'qb_elo_value',\n    'score1' : 'points_for',\n    'score2' : 'points_against',\n})\n\naway_data_df = data_df.copy()[[\n    'season',\n    'week_of',\n    'qb2',\n    'qb2_value_post',\n    'score2',\n    'score1'\n]].rename(columns={\n    'qb2' : 'qb_name',\n    'qb2_value_post' : 'qb_elo_value',\n    'score2' : 'points_for',\n    'score1' : 'points_against',\n})\n\nflat_df = pd.concat([home_data_df,away_data_df])\nflat_df = flat_df.sort_values(by=['season','week_of'])\n\nThis yields a data frame with individual QB games:\n\n\nflat_df.sample(5)\n\n       season  week_of  ... points_for  points_against\n11821    2002       41  ...         10              17\n2500     1954       44  ...         30               6\n5913     1977       38  ...         30              27\n12046    2003       38  ...         12              13\n5502     1975       38  ...         14              42\n\n[5 rows x 6 columns]\n\nNote that the above data are Elo values. To convert Elo values to Elo, you’d need to multiply by 3.3 per 538’s methodology. Add some addition stats:\n\n\nflat_df['point_margin'] = flat_df['points_for'] - flat_df['points_against']\nflat_df['win'] = numpy.where(flat_df['point_margin'] > 0,1,0)\n\nPart 2: Adjusting for era\n538’s QB ratings are based on stats that have increased overtime alongside improved QB play. This can be seen by looking at the median QB Elo value overtime:\n\n\n## calculate median QB values by season week ##\nmedian_df = flat_df.groupby(['season','week_of']).agg(\n    qb_elo_value_median = ('qb_elo_value', 'median'),\n    qb_elo_value_min = ('qb_elo_value', 'min'),\n    qb_elo_value_max = ('qb_elo_value', 'max')\n).reset_index()\n\n## plot ##\nmedian_line = median_df['qb_elo_value_median'].plot.line()\nplt.show()\n\n\nTo make 538’s Elo values comparable across era, this stat inflation needs to be removed:\n\n\n## add weekly median to flat file ##\nflat_df = pd.merge(\n    flat_df,\n    median_df,\n    on=['season','week_of'],\n    how='left'\n)\n\n## calculate an adjusted stat that removes the median\nflat_df['qb_elo_value_era_adjusted'] = flat_df['qb_elo_value'] - flat_df['qb_elo_value_median']\n\nPart 3: Adding stats and compiling careers\nTo compare quarterbacks, we’ll need to aggregate all of their QB values, but first, some additional stats can be added to make the ultimate comparisons more interesting. Namely, Elo ranking relative to other starters at a point in time, total starts, and win percentages:\n\n\n## add weekly ranking ##\nflat_df['qb_rank'] = flat_df.groupby(['season','week_of'])['qb_elo_value'].rank(method='max', ascending=False)\nflat_df['top_1_qb'] = numpy.where(flat_df['qb_rank']<=1, 1,0)\nflat_df['top_3_qb'] = numpy.where(flat_df['qb_rank']<=3, 1,0)\nflat_df['top_5_qb'] = numpy.where(flat_df['qb_rank']<=5, 1,0)\n\n## add cumulative count ##\nflat_df['game_number'] = flat_df.groupby(['qb_name']).cumcount() + 1\nflat_df['cumulative_era_adjusted_value'] = flat_df.groupby('qb_name')['qb_elo_value_era_adjusted'].transform(pd.Series.cumsum)\nflat_df['cumulative_wins'] = flat_df.groupby('qb_name')['win'].transform(pd.Series.cumsum)\nflat_df['cumulative_best_starter'] = flat_df.groupby('qb_name')['top_1_qb'].transform(pd.Series.cumsum)\nflat_df['cumulative_top_3_starts'] = flat_df.groupby('qb_name')['top_3_qb'].transform(pd.Series.cumsum)\nflat_df['cumulative_top_5_starts'] = flat_df.groupby('qb_name')['top_5_qb'].transform(pd.Series.cumsum)\n\nAfter adding stats, compile at the QB level to get a look at their career:\n\n\n## aggregate ##\nagg_df = flat_df.groupby('qb_name').agg(\n    total_starts = ('game_number', 'max'),\n    cumulative_era_adjusted_elo_value = ('qb_elo_value_era_adjusted', 'sum'),\n    winning_percentage = ('win', 'mean'),\n    pct_of_starts_as_best_qb = ('top_1_qb', 'mean'),\n    pct_of_starts_as_top3_qb = ('top_3_qb', 'mean'),\n    pct_of_starts_as_top5_qb = ('top_5_qb', 'mean')\n).reset_index()\n\nagg_df['average_era_adjusted_elo_value'] = agg_df['cumulative_era_adjusted_elo_value'] / agg_df['total_starts']\n\nSort by total Elo value to see the era adjusted rankings:\n\n\n## sort ##\nagg_df = agg_df.sort_values(by=['cumulative_era_adjusted_elo_value'],ascending=[False])[[\n    'qb_name',\n    'total_starts',\n    'cumulative_era_adjusted_elo_value',\n    'average_era_adjusted_elo_value',\n    'winning_percentage',\n    'pct_of_starts_as_best_qb',\n    'pct_of_starts_as_top3_qb',\n    'pct_of_starts_as_top5_qb'\n]]\n\nagg_df[['qb_name','total_starts','cumulative_era_adjusted_elo_value']].head(15)\n\n            qb_name  total_starts  cumulative_era_adjusted_elo_value\n493  Peyton Manning           265                       26065.054253\n204      Drew Brees           274                       23763.826759\n616       Tom Brady           283                       19225.832143\n146      Dan Marino           240                       17160.155283\n226  Fran Tarkenton           239                       14498.753253\n338     Joe Montana           164                       14043.230167\n73      Brett Favre           298                       12764.784141\n588     Steve Young           143                       12689.178178\n4     Aaron Rodgers           174                       12132.510866\n364   Johnny Unitas           185                        9796.247620\n144       Dan Fouts           171                        9767.284528\n441       Matt Ryan           189                        8398.344958\n380    Ken Anderson           172                        7866.688036\n495   Philip Rivers           224                        7385.531378\n347      John Elway           231                        7250.838144\n\nPart 4: Graphing careers\nThough simple, era adjusted QB Elo appears to provide a fairly good ranking of QBs across era. One interesting way to leverage this measure further is by comparing cumulative QB Elo gained over the course of a QB’s career.\nCreate a function for graphing career Elo based on a list of QBs:\n\n\ndef create_qb_chart(qbs_to_plot):\n    ## plot career cumulative Elo value based on a list of QBs ##\n    ## make a copy of the flat file ##\n    chart_df = flat_df.copy()\n    ## filter to just relevant fields ##\n    chart_df = chart_df[[\n        'qb_name',\n        'game_number',\n        'cumulative_era_adjusted_value'\n    ]]\n    ## create sub selection of QBs\n    chart_df = chart_df[numpy.isin(\n        chart_df['qb_name'],\n        qbs_to_plot\n    )]\n    ## set up plot ##\n    sns.lineplot(\n        'game_number',\n        'cumulative_era_adjusted_value',\n        hue='qb_name',\n        ci=None,\n        palette='RdPu',\n        data=chart_df\n    )\n    sns.despine()\n    ## set axis titles and sizes ##\n    plt.xlabel('Games Played', labelpad=10, fontsize='small', weight='bold')\n    plt.ylabel('Cumulative Elo Value Added', labelpad=10, fontsize='small', weight='bold')\n    plt.rc('xtick',labelsize='x-small')\n    plt.rc('ytick',labelsize='x-small')\n    ## define plot ranges, leaving a little room for padding ##\n    xmin = 0\n    xmax = chart_df['game_number'].max() * 1.2\n    ymin = chart_df['cumulative_era_adjusted_value'].min() * 1.15\n    ymax = chart_df['cumulative_era_adjusted_value'].max() * 1.15\n    plt.xlim(xmin,xmax)\n    plt.ylim(ymin,ymax)\n    ## add darker axis ##\n    plt.axhline(y = ymin, color = 'black', linewidth = 1.75)\n    plt.axvline(x = xmin, color = 'black', linewidth = 1.75)\n    ## and a line at zero\n    plt.axhline(y = 0, color = 'black', linewidth = 0.75)\n    ## add labels at the end of each line ##\n    for i in qbs_to_plot:\n        plt.text(\n            x = chart_df[chart_df['qb_name'] == i]['game_number'].iloc[-1] + 1,\n            y = chart_df[chart_df['qb_name'] == i]['cumulative_era_adjusted_value'].iloc[-1] + 5,\n            s = i,\n            weight = 'bold',\n            fontsize = 'small',\n            backgroundcolor = '#ffffff'\n        )\n    ## remove legend ##\n    plt.legend([],[], frameon=False)\n    plt.tight_layout()\n\nMake your comparisons…\nManning, Brady, and Brees:\n\n\nqb_list = ['Tom Brady', 'Peyton Manning','Drew Brees']\ncreate_qb_chart(qb_list)\nplt.show()\n\n\nRomo > Dak > Aikman?:\n\n\nqb_list = ['Tony Romo', 'Troy Aikman', 'Dak Prescott']\ncreate_qb_chart(qb_list)\nplt.show()\n\n\nJaMarcus Russell, it could have been worse:\n\n\nqb_list = ['JaMarcus Russell', 'Johnny Manziel', 'Ryan Leaf']\ncreate_qb_chart(qb_list)\nplt.show()\n\n\nMaybe Leaf just needed more time and a better defense:\n\n\nqb_list = ['Ryan Leaf', 'Trent Dilfer']\ncreate_qb_chart(qb_list)\nplt.show()\n\n\nMahomes, off to one of the best starts ever:\n\n\nqb_list = ['Patrick Mahomes', 'Aaron Rodgers', 'Dan Marino']\ncreate_qb_chart(qb_list)\nplt.show()\n\n\nJosh Allen, not so much …\n\n\nqb_list = ['Josh Allen', 'Sam Darnold', 'Mitchell Trubisky', 'Baker Mayfield']\ncreate_qb_chart(qb_list)\nplt.show()\n\n\n\n\n",
    "preview": "posts/2020-08-22-ranking-qbs-using-era-adjusted-elo/ranking-qbs-using-era-adjusted-elo_files/figure-html5/all_time_greats-1.png",
    "last_modified": "2020-10-29T09:08:00+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 2700
  },
  {
    "path": "posts/2020-08-21-game-excitement-and-win-probability-in-the-nfl/",
    "title": "Game Excitement and Win Probability in the NFL",
    "description": "Game excitement calculation and a win probability figure.",
    "author": [
      {
        "name": "Max Bolger",
        "url": "https://twitter.com/mnpykings"
      }
    ],
    "date": "2020-08-21",
    "categories": [
      "nflfastR",
      "python"
    ],
    "contents": "\nTable of Contents\nPart 1: Importing and Preprocessing\nPart 2: Game Excitement Index\nPart 3: Win Probability Chart\nPart 1: Importing and Preprocessing\nFirst we need to import our dependencies. These pacakges are what make this analysis possible.\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nNext we will read in our data from the nflfastR data repo.\n\n\n# Read in data\nYEAR = 2019\n\ndata = pd.read_csv('https://github.com/guga31bb/nflfastR-data/blob/master/data/' \\\n                         'play_by_play_' + str(YEAR) + '.csv.gz?raw=True',\n                         compression='gzip', low_memory=False)\n\nPerfect! Our data and notebook are set up and ready to go. The next step is to filter our df to include only the game we would like to work with. We will subset by game_id (which we will need later). The new nflfastR game ids are very convenient and use the following format:\nYEAR_WEEK_AWAY_HOME\nNote, the year needs to be in YYYY format and single digit weeks must lead with a 0.\n\n\n#Subset the game of interest\ngame_df = data[\n             (data.game_id== '2019_09_MIN_KC')\n             ]\n\n#View a random sample of our df to ensure everything is correct          \ngame_df.sample(3)\n\n       play_id         game_id  ...  xyac_success   xyac_fd\n23013     1294  2019_09_MIN_KC  ...      1.000000  1.000000\n23080     3077  2019_09_MIN_KC  ...      0.140994  0.107368\n23077     2992  2019_09_MIN_KC  ...           NaN       NaN\n\n[3 rows x 340 columns]\n\nThe last step in preprocessing for this particular analysis is dropping null values to avoid jumps in our WP chart. To clean things up, we can filter the columns to show only those that are of importance to us.\n\n\ncols = ['home_wp','away_wp','game_seconds_remaining']\ngame_df = game_df[cols].dropna()\n\n#View new df to again ensure everything is correct\ngame_df\n\n        home_wp   away_wp  game_seconds_remaining\n22960  0.560850  0.439150                  3600.0\n22961  0.560850  0.439150                  3600.0\n22962  0.599848  0.400152                  3596.0\n22963  0.612526  0.387474                  3590.0\n22964  0.629503  0.370497                  3584.0\n...         ...       ...                     ...\n23132  0.697633  0.302367                    59.0\n23134  0.806030  0.193970                    24.0\n23135  0.910061  0.089939                     4.0\n23136  0.927525  0.072475                     3.0\n23137  1.000000  0.000000                     0.0\n\n[166 rows x 3 columns]\n\nEverything looks good to go! Before we use this data to create the WP chart, we are going to calculate the game’s excitement index.\nPart 2: Game Excitement Index\nWe are using Luke Benz’ formula for GEI which can be found here. It’s simple yet effective which is why I like it so much. As Luke notes, “the formula sums the absolute value of the win probability change from each play”. Here, we are creating a function (inspired by ChiefsAnalytics) that follows his formula. This function requires a single parameter game_id. The new version of nflfastR’s game id must be used here.\n\n\n#Calculate average length of 2019 games for use in our function\navg_length = data.groupby(by=['game_id'])['epa'].count().mean()\n\ndef calc_gei(game_id):\n  game = data[(data['game_id']==game_id)]\n  #Length of game\n  length = len(game)\n  #Adjusting for game length\n  normalize = avg_length / length\n  #Get win probability differences for each play\n  win_prob_change = game['home_wp'].diff().abs()\n  #Normalization\n  gei = normalize * win_prob_change.sum()\n  return gei\n\nLet’s run the function by passing in our game id from earlier.\n\n\nprint(f\"Vikings @ Chiefs GEI: {calc_gei('2019_09_MIN_KC')}\")\n\nVikings @ Chiefs GEI: 4.652632439280925\n\nThis seemed to be a pretty exciting game. Let’s compare it to other notable games from last season.\n\n\n# Week 1 blowout between the Ravens and Dolphins\nprint(f\"Ravens @ Dolphins GEI: {calc_gei('2019_01_BAL_MIA')}\")\n\n# Week 14 thriller between the 49ers and Saints\n\nRavens @ Dolphins GEI: 0.9723172478637379\n\nprint(f\"49ers @ Saints GEI: {calc_gei('2019_14_SF_NO')}\")\n\n49ers @ Saints GEI: 5.190375267367869\n\nYep, the Vikings vs Chiefs game was definitely one of the more exciting regular season games of last season. Let’s see how it looks visually with a WP chart!\nPart 3: Win Probability Chart\nMatplotlib and Seaborn can be used together to create some beautiful plots. Before we start, below is a useful line of code that prints out all usable matplotlib styles. You can also see how each of them look by checking out the documentation.\n\n\n#Print all matplotlib styles\nprint(plt.style.available)\n\n['Solarize_Light2', '_classic_test_patch', 'bmh', 'classic', 'dark_background', 'fast', 'fivethirtyeight', 'ggplot', 'grayscale', 'seaborn', 'seaborn-bright', 'seaborn-colorblind', 'seaborn-dark', 'seaborn-dark-palette', 'seaborn-darkgrid', 'seaborn-deep', 'seaborn-muted', 'seaborn-notebook', 'seaborn-paper', 'seaborn-pastel', 'seaborn-poster', 'seaborn-talk', 'seaborn-ticks', 'seaborn-white', 'seaborn-whitegrid', 'tableau-colorblind10']\n\nSince we already have all of our data set up from Step 1, we can jump straight to the plot!\n\n\n#Set style\nplt.style.use('dark_background')\n\n#Create a figure\nfig, ax = plt.subplots(figsize=(16,8))\n\n#Generate lineplots\nsns.lineplot('game_seconds_remaining', 'away_wp', \n             data=game_df, color='#4F2683',linewidth=2)\n\nsns.lineplot('game_seconds_remaining', 'home_wp', \n             data=game_df, color='#E31837',linewidth=2)\n\n#Generate fills for the favored team at any given time\n\n<AxesSubplot:xlabel='game_seconds_remaining', ylabel='home_wp'>\n\nax.fill_between(game_df['game_seconds_remaining'], 0.5, game_df['away_wp'], \n                where=game_df['away_wp']>.5, color = '#4F2683',alpha=0.3)\n\nax.fill_between(game_df['game_seconds_remaining'], 0.5, game_df['home_wp'], \n                where=game_df['home_wp']>.5, color = '#E31837',alpha=0.3)\n\n#Labels\nplt.ylabel('Win Probability %', fontsize=16)\nplt.xlabel('', fontsize=16)\n\n#Divider lines for aesthetics\nplt.axvline(x=900, color='white', alpha=0.7)\nplt.axvline(x=1800, color='white', alpha=0.7)\nplt.axvline(x=2700, color='white', alpha=0.7)\nplt.axhline(y=.50, color='white', alpha=0.7)\n\n#Format and rename xticks\nax.set_xticks(np.arange(0, 3601,900))\n\n[<matplotlib.axis.XTick object at 0x000000002F30CF60>, <matplotlib.axis.XTick object at 0x000000002F30CB00>, <matplotlib.axis.XTick object at 0x000000002F33FD30>, <matplotlib.axis.XTick object at 0x000000002F3D0438>, <matplotlib.axis.XTick object at 0x000000002F3D08D0>]\n\nplt.gca().invert_xaxis()\nx_ticks_labels = ['End','End Q3','Half','End Q1','Kickoff']\nax.set_xticklabels(x_ticks_labels, fontsize=12)\n\n#Titles\n\n[Text(0, 0, 'End'), Text(900, 0, 'End Q3'), Text(1800, 0, 'Half'), Text(2700, 0, 'End Q1'), Text(3600, 0, 'Kickoff')]\n\nplt.suptitle('Minnesota Vikings @ Kansas City Chiefs', \n             fontsize=20, style='italic',weight='bold')\n\nplt.title('KC 26, MIN 23 - Week 9 ', fontsize=16, \n          style='italic', weight='semibold')\n\n#Creating a textbox with GEI score\nprops = dict(boxstyle='round', facecolor='black', alpha=0.6)\nplt.figtext(.133,.85,'Game Excitement Index (GEI): 4.65',style='italic',bbox=props)\n\n#Citations\nplt.figtext(0.131,0.137,'Graph: @mnpykings | Data: @nflfastR')\n\n#Save figure if you wish\n#plt.savefig('winprobchart.png', dpi=300)\n\n\nWow, this game had a ton of WP changes. No wonder it had a high GEI!\nThings to be aware of:\nSometimes the plot generates small gaps in the fill. This only occurs when the previous data point is on the opposite side of the 50% threshold compared to the current data point or vice versa (this happens twice to the Chiefs’ WP line towards the end of the game). The .fill_between() function only checks to fill at each new data point and not inbetween. This is very minor and the dark background makes it hardly noticeable, but I wanted to address it to make sure nobody gets confused if this happens to them.\nThe nflfastR win probability model is a little wonky in OT due to it not accounting for ties as Sebastian mentions here. Be mindful of this when calculating GEI or creating WP charts with OT games.\nThat concludes this tutorial. Thanks for reading, I hope you learned some python in the process! Big thanks to Sebastian Carl and Ben Baldwin for everything they do; I’m looking forward to watching this platform grow! The future of sports analytics has never looked brighter.\n\n\n",
    "preview": "posts/2020-08-21-game-excitement-and-win-probability-in-the-nfl/game-excitement-and-win-probability-in-the-nfl_files/figure-html5/plot-1.png",
    "last_modified": "2020-10-29T09:08:00+00:00",
    "input_file": {},
    "preview_width": 9600,
    "preview_height": 4800
  },
  {
    "path": "posts/2020-08-22-nfl-pass-location-visualization/",
    "title": "NFL Pass Location Visualization",
    "description": "Methods for visualizing NFL passing location data.",
    "author": [
      {
        "name": "Ethan Douglas",
        "url": "https://twitter.com/ChiefsAnalytics"
      }
    ],
    "date": "2020-08-21",
    "categories": [
      "nflfastR",
      "python"
    ],
    "contents": "\nTable of Contents\nVisualizing NFL Pass Location\nSet-up\nData Exploration\n\nVisualizing NFL Pass Location\nSo this is a post all about how my life got flipped-turned upside down…\nKidding, but kind of. About a year ago I became very interested in NFL analytics, and managed to stumble across the incredible work of Sarah Mallepalle, Ron Yurko, Konstantinos Pelechrinis, and Sam Ventura. Their paper (https://arxiv.org/abs/1906.03339) unveiled the creation of a scraper which could take static Next Gen Stats passing charts and translate them into x and y coordinate data. This tool was as impressive as it was interesting, and so it should be no surprise that Sarah (the lead author on the paper) was scooped up to work as an analyst for the Baltimore Ravens, terminating the public code for this work. Thankfully, Ron Yurko was happy to revive her github when I reached out. I have had a ton of fun exploring this dataset, and even adapted the code a bit to scrape routes and carries as well.\nThat said, sometimes location data can be a bit overwhelming so I thought I’d do a quick walkthrough of a few things you can do with it. Consider this post Part One, where I’ll walk through some simple two-dimensional visualizations. In later posts I’ll do my best to go over the expected completion surfaces Sarah et al. debuted in the paper, and other ways to visualize the data.\nSet-up\nFirst we need to import the libraries we’ll be using\n\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nNext we’ll read in our data from the repo I maintain\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/ArrowheadAnalytics/next-gen-scrapy-2.0/master/pass_and_game_data.csv', low_memory=False)\n#There's an additional index row we don't need, so I am getting rid of it here\ndf = df.iloc[0:,1:]\n\nData Exploration\n\n\n#Drop the games that weren't able to be scraped\ndf.dropna(inplace=True)\n#Explore the data a bit\ndf.head(3)\n\n      game_id           name  ... home_score away_score\n0  2017091004  Carson Palmer  ...         35         23\n1  2017091004  Carson Palmer  ...         35         23\n2  2017091004  Carson Palmer  ...         35         23\n\n[3 rows x 14 columns]\n\n\n\n#Get a summary of the numerical data in our dataframe\ndf.describe()\n\n            game_id       x_coord  ...    home_score    away_score\ncount  4.383900e+04  43839.000000  ...  43839.000000  43839.000000\nmean   2.018206e+09      0.401437  ...     23.654600     22.306827\nstd    8.202842e+05     15.229154  ...     10.318057     10.243724\nmin    2.017091e+09    -28.300000  ...      0.000000      0.000000\n25%    2.017122e+09    -12.300000  ...     17.000000     16.000000\n50%    2.018112e+09      0.900000  ...     24.000000     23.000000\n75%    2.019101e+09     12.900000  ...     31.000000     28.000000\nmax    2.019123e+09     28.300000  ...     57.000000     59.000000\n\n[8 rows x 6 columns]\n\nLooks like we’ve got almost 43k passes, from 2017-2019, ranging from -28.3 to 28.3 in the x coordinate and -10.4 to 62.1 in the y coordinate. We know a field is only 53 and 1/3 yards wide, so tthere are some passes being identified that are technically out of bounds.\nOne common thing to do with new data is to explore the distributions of the numerical variables we are interested in. This is typically done with histograms.\n\n\n#Let's visualize the distribution of the coordinates\nfig = sns.distplot(df.x_coord, kde=False)\nplt.show()\n\n\nNothing fancy here but we can already see some trends in where players pass the ball. Let’s look at the y direction\n\n\n#Now the x coordinate\nfig = sns.distplot(df.y_coord, kde=False)\nplt.show()\n\n\nSo we’ve plotted both the horizontal (x) and vertical (y) field components, but nothing too interesting here still. What if we wanted to visualize these distributions as 2-dimensional?\nWe have a few different ways to go about this. One, a simple jointplot.\n\n\nfig = sns.jointplot(x='x_coord', y='y_coord', data=df)\nplt.show()\n\n\nThis is neat! We’ve now visualized the data by both the x and y coordinate, and we can see the distributions for both on the side of the graph\nBut, with just one small tweak we can make this even more clear.\n\n\nfig = sns.jointplot(x='x_coord', y='y_coord', data=df, kind='hex', color='red')\nplt.show()\n\n\nWhat we’ve done here is bin each of the passes into a hexagons by where they were thrown on the field. The darker hexagons here represent areas of the field where a higher density of passes were targeted.\nIf this kind of discrete binning isn’t your thing, how about a smoother 2-dimensional kernel density estimate? Again, with just a very small tweak to the above code we can plot a different way of visualizing the location of these passes\n\n\nfig = sns.jointplot(x='x_coord', y='y_coord', data=df, kind='kde', cmap='Reds', color='red')\nplt.show()\n\n\nNow, what if we want to directly compare two distributions? Looking at the entire dataset has been interesting, but how does someone like Derek Carr differ in his target locations compared to someone like Patrick Mahomes?\n\n\n#prepare our two plots - no reason to separate out our axes here as we want to compare these two players on an equal scale so we set sharex and sharey to true\nfig, ax =plt.subplots(1,2, sharex=True, sharey=True)\n\n#First, we'll partition the data to the player we are looking for\nqb_name = 'Mahomes' #I'm being intentional vague here and not listing the full player name, because many player names listed on Next Gen Stats are different than you're likely accustomed to. So, I've found this method easire to ensure I get the player data\nqb = df.loc[(df['name'].str.contains(qb_name))]\n#Assign this plot to the first (0-index) axis. Not going to use the univariate distributions on the axes for these plots as they don't work well as subplots\nsns.kdeplot(qb.x_coord, qb.y_coord, ax=ax[0], cmap='Reds')\n\n#Let's get player 2\nqb_name = 'Carr'\nqb = df.loc[(df['name'].str.contains(qb_name))]\n#Second plot\nsns.kdeplot(qb.x_coord, qb.y_coord, ax=ax[1], cmap='Reds')\nplt.show()\n\n\nThis isn’ too bad! With just a few lines of code we can see that Carr’s passes are more dense around the line of scrimmage, while Mahomes spreads the ball around more.\nLet’s spice these plots up a bit\n\n\n#Layering two plotting styles here to get a big, clean, but dark background look\nplt.style.use('seaborn-poster')\nplt.style.use('dark_background')\n\n#Set up our subplots\nfig, (ax1, ax2) =plt.subplots(1,2)\n\n\nqb_name = 'Mahomes'\nqb = df.loc[(df['name'].str.contains(qb_name))]\n\n#What we've added here is shading for the densities, but leaving the lowest density area unshaded.\n#I've also added the *n_level* parameter, which allows us to choose how many levels we want to have in our contour. The higher the number here, the smoother the plot will look.\nsns.kdeplot(qb.x_coord, qb.y_coord, ax=ax1, cmap='gist_heat', shade=True, shade_lowest=False, n_levels=10)\n\n#Set title, remove ticks and labels\nax1.set_title(qb_name)\nax1.set_xlabel('')\nax1.set_xticks([])\n\nax1.set_yticks([])\n\nax1.set_ylabel('')\n\n#Remove any part of the plot that is out of bounds\nax1.set_xlim(-53.3333/2, 53.3333/2)\n\nax1.set_ylim(-15,60)\n#This makes our scales (x and y) equal (1 pixel in the x direction is the same 'distance' in coordinates as 1 pixel in the y direction)\n\n\n\n\n#Plot all of the field markings (line of scrimmage, hash marks, etc.)\n\nfor j in range(-15,60-1,1):\n    ax1.annotate('--', (-3.1,j-0.5),\n                 ha='center',fontsize =10)\n    ax1.annotate('--', (3.1,j-0.5),\n                 ha='center',fontsize =10)\n    \nfor i in range(-10,60,5):\n    ax1.axhline(i,c='w',ls='-',alpha=0.7, lw=1.5)\n    \nfor i in range(-10,60,10):\n    ax1.axhline(i,c='w',ls='-',alpha=1, lw=1.5)\n    \nfor i in range(10,60-1,10):\n    ax1.annotate(str(i), (-12.88,i-1.15),\n            ha='center',fontsize =15,\n                rotation=270)\n    \n    ax1.annotate(str(i), (12.88,i-0.65),\n            ha='center',fontsize =15,\n                rotation=90)\n\n\n#Now we just repeat for player 2.\nqb_name = 'Carr'\nqb = df.loc[(df['name'].str.contains(qb_name))]\n#Second plot\nsns.kdeplot(qb.x_coord, qb.y_coord, ax=ax2, cmap='gist_heat', shade=True, shade_lowest=False, n_levels=10)\n\nax2.set_title(qb_name)\nax2.set_xlabel('')\nax2.set_ylabel('')\nax2.set_xticks([])\n\nax2.set_yticks([])\n\nax2.set_xlim(-53.3333/2, 53.3333/2)\n\nax2.set_ylim(-15,60)\n\n\nfor j in range(-15,60,1):\n    ax2.annotate('--', (-3.1,j-0.5),\n                 ha='center',fontsize =10)\n    ax2.annotate('--', (3.1,j-0.5),\n                 ha='center',fontsize =10)\n    \nfor i in range(-10,60,5):\n    ax2.axhline(i,c='w',ls='-',alpha=0.7, lw=1.5)\n    \nfor i in range(-10,60,10):\n    ax2.axhline(i,c='w',ls='-',alpha=1, lw=1.5)\n    \nfor i in range(10,60-1,10):\n    ax2.annotate(str(i), (-12.88,i-1.15),\n            ha='center',fontsize =15,\n                rotation=270)\n    \n    ax2.annotate(str(i), (12.88,i-0.65),\n            ha='center',fontsize =15,\n                rotation=90)\n\nplt.show()\n\n\nNow the nice thing about these seaborn plots is they do all the heavy lifting for us - but, they don’t let us see the underlying numbers. We can’t see at the coordinate (-20, 10) exactly what density of passes each player has.\nThat concludes this post. In my next post I’ll walk through performing a kernel density estimate ourselves, which lets us overlay QB densities and do some other fun things. Huge thanks to Sarah Mallepalle and her coauthors for publishing this awesome data and scraper, thanks to Ronald Yurko, Samuel Ventura, and Maksim Horowitz for the creation of the nflscrapR tool which spurred my and so many other’s interest in NFL analytics, and to Ben Baldwin and Sebastian Carl for continuing to advance the public work in this space.\n\n\n",
    "preview": "posts/2020-08-22-nfl-pass-location-visualization/nfl-pass-location-visualization_files/figure-html5/subplots2-1.png",
    "last_modified": "2020-10-29T09:08:00+00:00",
    "input_file": {},
    "preview_width": 7680,
    "preview_height": 5280
  },
  {
    "path": "posts/2020-08-22-rodgers-efficiency-decline/",
    "title": "Rodgers Efficiency Decline",
    "description": "A look into Rodgers Efficiency Decline. Also some functions for plotting EPA/CPOE moving averages.",
    "author": [
      {
        "name": "Austin Ryan",
        "url": "https://twitter.com/packeRanalytics"
      }
    ],
    "date": "2020-08-21",
    "categories": [
      "Figures",
      "nflfastR",
      "CPOE / EPA functions",
      "Packers"
    ],
    "contents": "\nTable of Contents\nEfficiency Metrics\nRodgers career as seen by EPA, CPOE, and QBR\nDigging Deeper\nSupporting Cast\n2008 - 2014\n2015-2016\n2017\n2018\n2019\n\nWhat’s next?\nFor the first time in the seven-year span of Mike Sando’s quarterback tiers survey Aaron Rodgers was not ranked as the NFL’s top quarterback. However, the fifty NFL coaches and evaluators who participated in the survey have not observed much of a drop off. In the 2020 survey Rodgers received a tier 1 vote from 46 respondents and finished 3rd overall in the ranking.\nFellow NFL players recently gave Rodgers his lowest ever ranking in the ten-year history of the annual NFL Top 100 poll following the 2019 season. Similar to the respondents of Sando’s survey, players still view Rodgers as a near top tier player. He earned the 16th overall ranking and was the 6th highest ranked quarterback in the 2020 poll.\nPFF grades tell a similar but not quite as rosy of a story. From 2008, Rodgers first season as a starter, to 2014 his PFF grade was never worse than 6th best in the league. In 2015 his PFF grade fell out of the top 10 but quickly bounced back to top 3 in 2016. After an injury shortened 2017 season PFF graded Rodgers as their 5th and 7th best in 2018 and 2019 respectively.\nIf people in the NFL still consider Rodgers a near top talent and PFF grade has him slightly outside the top 5 why are the efficiency stats so much more sour on him recently? Most of the advanced efficiency metrics rank the future Hall of Famer closer to league average than top 5. Expected Points Added (EPA), ESPN’s Quarterback Rating (QBR), Football Outsiders Defense-adjusted Value Over Average (DVOA), and Completion Percentage Over Expectation (CPOE) suggest Rodgers hasn’t been a tier 1 quarterback since 2016.\nThe Computer Cowboy’s EPA + CPOE composite metric on rbsdm.com ranked Rodgers as the 16th best since 2016, one spot below Ryan Fitzpatrick and three spots below Kirk Cousins.\n“Efficiency”Football Outsiders DVOA placed Rodgers outside the top 10 in each of the last three seasons. QBR lists Rodgers one spot below Andy Dalton in 2018 (16th) and one spot ahead of Jacoby Brissett in 2019 (20th).\nEfficiency Metrics\nIf you are not familiar with EPA, QBR, DVOA, or CPOE please check out the below.\nEPA\nEPA\nQBR\nDVOA\nnflfastR’s CPOE (Completion Percentage Over Expectation) forms an expectation of how likely a pass is to be completed based on factors we can observe like how far down the field the pass was thrown, whether the pass was thrown to the left, middle, or right, or whether or not it was thrown in a dome, and compares it to what the passer’s actual completion percentage.\nRodgers career as seen by EPA, CPOE, and QBR\n\n\n\n\n\n\nThe below graph is plotting the average expected points added per dropback from Rodgers most recent 300 dropbacks over the course of his career. When the white line and yellow fill are near .20 on the Y axis it means Rodgers last 300 dropbacks produced an EPA average similar to one that the 5th best in the game would have earned. The further above or below the .20 mark it goes it means he was that much better or worse than the 5th best.\n\n\n\nThis tells us Rodgers EPA was tier one level for over 71% of his dropbacks through 2014. We can see the highs were are really high as Rodgers doubled the EPA average of the 5th best at times. Remarkably, Rodgers was never outside top 5 play in his 2011 and 2014 MVP seasons. The lows were never very low either. Rodgers EPA average only fell below the .10 mark for less than 4% of his 4,600 plus dropbacks prior to 2015. The only similarly sustained levels of dominance I can see in the nflfastR library are a couple different stretches from Peyton Manning and the Patrick Mahomes Experience.\n\n\n\nAfter 2014 we see Rodgers rolling average collapse to an alarming low in 2015, skyrocket in 2016 during the “run-the-table” stretch, and then remain below top 5 play outside of a couple week stretch in 2019.\nCPOE mostly matches what we saw in the EPA per dropback plot. Rodgers CPOE was generally among the game’s elites through 2014 but has been rather mediocre other than the 2016 run since.\n\n\n\n\n\n\nESPN’s QBR is scaled to equate a 50 rating with average play and Pro Bowl-level play at 75. Again we see Rodgers hasn’t been viewed as one of the games best by QBR in recent seasons. His 2011 season ranks as the 3rd best QBR season of all time above 2019 Lamar Jackson and 2018 Patrick Mahomes. However, unlike respondents of Sando’s survey and the Top 100 poll, QBR downgraded Rodgers to marks well below that the past two seasons.\n\n\n\n\n\n\nWe are assigning EPA and CPOE to Rodgers in the above plots. However, a more apt description would be EPA or CPOE of the Packers passing offense in plays where Aaron Rodgers was the quarterback. Coaching, playcalling, and supporting cast are responsible for some portion of the output but we do know the quarterback plays a very large role.\nDigging Deeper\nWhat else can the publicly available data tell us about what part of Rodgers game is declining in the eyes of advanced efficiency metrics? One feature in the data we can use to look at EPA and CPOE at a more granular level is air yards. The air yards a pass gets assigned is the distance the ball traveled in the air past the line of scrimmage (not the distance traveled from the QBs release to the receiver).\n\n\n\nIf we bin air yards into into deep, intermediate, short, and behind the line of scrimmage passes we find CPOE has regressed at every level of the field in the past five years.\n\n\n\n\n\n\nThe 2008 to 2014 figures were absurdly good so some regression would be expected. However, over the last five years passes in the 10-19 and 20+ yard ranges are being completed at rates lower than we would expect while the 0-9 yard range is barely positive. In fact, below we see that Rodgers worst four seasons in the 0-9 yard range and 10-19 yard range (the two most frequently targeted regions) have come in the last five years.\n\n\n\nCompleting less passes than one would expect at every level obviously translates to lower EPA at every level of the field. Since 2015 deep passing EPA per dropback has decreased by 27% (.74 to .54), intermediate passing has decreased 25% (.61 to .45), and short passing efficiency has decreased a whopping 40% (.30 to .18).\n\n\n\nSupporting Cast\nSando’s article and his recent appearance on the Bill Barnwell Show suggest the NFL community hasn’t seen a significant deterioration of Rodgers skills and are assigning most of the statistical drop off to the supporting cast. It’s very hard to disentangle the strength of the scheme and the supporting cast from EPA and CPOE numbers. Nevertheless, let’s see what we can glean from the available data.\nThe below plot looks at EPA per target and CPOE by targeted receiver for passes that went past the line of scrimmage. Targets from 2008 to 2014 show as white dots and targets from 2015 to 2019 are in yellow. Players with fewer than 50 targets have been removed and the size of the dot corresponds to the number of targets the player received.\n\n\n\n\n\n\n\n\n\nWe see targeted receivers from ’08 to ’14 are largely clustered in the upper right meaning those targets were completed more frequently than we would expect and they were very efficient in terms of EPA. The only targeted receivers in that quadrant from ’15 to ’19 are Jordy Nelson, Davante Adams, and Allen Lazards 50 targets. The dispersion of yellow and white dots shouldn’t be a surprise given what we know about Rodgers numbers as a whole over those time frames. To give this some more context let’s think about the situation Rodgers came into in 2008.\n2008 - 2014\nRodgers spent the 2005 through 2007 seasons on the bench behind Favre learning Mike McCarthy’s offense. Donald Driver was entering his age 33 season coming of back-to-back Pro Bowl selections in 2006 and 2007. Late 2006 2nd round selection Greg Jennings was entering his 3rd season after emerging as a blossoming star while posting an impressive 11 yard per target 12 touchdown season in ’07. Late 2007 3rd round pick James Jones was entering his second season. Green Bay general manager Ted Thompson also invested an early 2nd round pick and late 3rd on pass catchers in the 2008 draft by selecting Jordy Nelson and Jermichael Finley.\nThese five receivers accounted for nearly 80% of Rodgers targets in his first three seasons en route to the Super Bowl XLV victory in February 2011. A few months later in 2011 Thompson & Co spent another 2nd round pick on a wide receiver (Randall Cobb). The continuity from the aforementioned five pass catchers and the addition of Cobb helped spur Rodgers to one of the best statistical seasons ever in 2011. In a 2011 ESPN The Magazine article Rodgers spoke to what helped contribute to his phenomal accuracy. He was quoted saying the following:\n“Learning to time up my drop with each route has been a big thing for me. It allows me to throw the ball in rhythm and hit the same release point with every throw, meaning that no matter what else is happening, the ball comes out on a similar plane. That’s when accuracy comes.”\nHe went on to say the fundamentals come first, “…then you have to become an expert in your own offense. Then you can get to a point where you’re attacking instead of reacting. Rich Gannon told me this back in 2006: You’ll know you’re at a good level by the things you’re thinking about when you break the huddle. If you’re thinking about your own guys – what routes they have, who has what – you’re not thinking about the right things. When I break the huddle now, I know what my guys are doing. I know the areas they’re going to be in.”\nRodgers efficiency was largely maintained over the next three seasons despite losing Driver and Jennings after the 2012 season along with Jones and Finley after the 2013 season. Rodgers added his 2nd MVP award in 2014 when Nelson earned a career high number of targets and another 2nd rount talent, Davante Adams, was added to the mix.\n2015-2016\nGreen Bay’s passing efficiency dropped to abysmal levels in 2015 after Nelson tore his ACL in the preseason. The efficiency didn’t immediately return to elite when Nelson came back in 2016 as the Packers sputtered out to a 4-6 start. Doug Farrar wrote about Green Bay’s passing game regression in October 2016.\n“Over time, Rodgers has overcompensated for the things that offense doesn’t provide to the point where it’s broken him as a mechanically consistent player.”\n“Rodgers appears to be operating under the belief that he must transcend a faulty offense with his own impressive physical attributes.”\nGreg Cosell also commented “What continues to stand out is that his accuracy is not what it once was. Precise ball placement is what made him special, and that attribute has been very erratic going back to last season. And now, it’s getting worse. Now, he’s missing wide-open receivers.”\nIf you refer back to Rodgers moving average EPA and CPOE plots the passing offense was not just below top 5 but considerably lower. It is fair to say Rodgers probably not at a “good level” when Nelson exited the line up and he didn’t trust what his guys were doing or where they were going to be. However, after Rodgers “run-the-table” remarks something changed and the offense turned in one of the better stretches of Rodgers career in terms of efficiency over the next nine games.\n2017\nIn 2017 Rodgers EPA and CPOE metrics were okay before the collarbone injury in week 6 (.16 EPA/dropback and CPOE +2.2%). He did return several weeks later in an effort to make a run at the playoffs but had a very poor performance which. Ultimately the 301 dropbacks placed Rodgers 14 overall in the rbsdm.com EPA and CPOE composite metric.\n2018\nPassing game continuity took a hit in 2018 as Jordy Nelson moved on to the Raiders. Adams and Cobb returned but the front office hadn’t been using draft capital on pass catchers like they did earlier in Rodgers career. Since taking Adams in the 2nd round of the 2014 draft Green Bay used two 5th and two 7th round selections on pass catchers from ’15 to ’17. Jimmy Graham was signed to a big deal entering his age 32 season two years removed from a torn patellar tendon to fill some of the void. Three receivers were also added in the 2018 draft although the earliest selection was in round 4 at pick 144. To make matter worse Rodgers suffered a tibial plateau fracture and a sprained MCL in week 1. The shakeup in personnel and coaching compounded by injury surely impacted the timing and rhythm and subsequent efficiency numbers.\nAnother thing that doesn’t help efficiency stats are throwaways. The most glaring numbers from 2018 the eye popping number of throwaways. Per PFF Rodgers threw the ball away once every 26 dropbacks from 2008 to 2017 but jumped to an astonishing once every 11 dropbacks in 2018.\nPFF Steve and Sam posited on the PFF NFL Show that this is likely the reason PFF grades have been more favorable to Rodgers than offensive efficiency metrics because of how much weight is placed on avoiding turnover worthy plays. PFF grade penalizes Rodgers much less for throwaways than EPA or CPOE would.\nIf we refer back to the initial plot we see Rodgers ended 2018 with a rolling 300 play average of .13 per play. In early 2019 under LaFleur Rodgers rolling average crept back into elite territory by the end of the week 8 matchup against Kansas City. Coincidentally, Rodgers posted an astonishing 95.5 QBR, .96 EPA per dropback, and perfect 158.3 passer rating in week 7 immediatly after Ben Baldwin’s No longer elite article on Rodgers. But then the next 300 dropbacks that cover week 9 at the Chargers to mid-game week 17 at Detroit we see Rodgers rolling average collapse down to -.05 per play.\n2019\nPrior to the 2019 Season Josh Hermsmeyer posed the question Are We Sure Aaron Rodgers Is Still An Elite Quarterback? at fivethirtyeight. Hermsmeyer noted CPOE decline starting in 2015 and speculated that the Packer’s underperformance could be due to a subpar play-action passing game. From 2015 to 2018 Hermsmeyer found Rodgers QBR was 32nd out of 41 in Raw QBR and could possibly be bolstered by a better play-action game with LaFleur in 2019.\nI charted play-action and personnel info in 2019 to pair this with the EPA and CPOE play by play data from nflscrapR to see how this played out. Rodgers CPOE on play-action was +5.6% as opposed to 0.2% for non play-action passes, however, EPA per play was the same at .17. This would suggest Rodgers wasn’t hitting the explosive downfield shots any more frequently on play-action. One glimmer of hope for Green Bay fans entering 2020 is that EPA per play on play-action plays with 2 or fewer wide receivers on the field was .24 per play and .08 per play in 11 personnel.\nWhat’s next?\nThe numbers tell us Green Bay has not completed passes and moved the chains at the rate an elite offense would lately. In 2019 we saw Rodgers flirt with elite level efficiency numbers through the first 8 weeks. But that all came crashing down and we rapidly approached the abysmal 2015 levels in later weeks.\nMaybe Rodgers will master the offense and get back to attacking instead of reaction and 2020 brings a big year two leap. Maybe without several elite weapons and an intimate comfortability with the scheme Rodgers just doesn’t see things well and we shouldn’t expect any bounce in year two. Maybe the confluence of factors that contributed to the previous elite play will never come back and we will see flashes of brilliance littered with throwaways.\nI don’t have the data or the knowledge to say whether or not Rodgers isn’t pulling the trigger when he should be. Alex Rollins of SB Nation took a film based approach to answering this question and noted Rodgers is not consistently operating within the structure of the offense and is sacrificing efficiency to extend plays but isn’t hitting them. Maybe the drop in receiver talent is driving Rodgers inconsistent play.\nWe surely can’t say the supporting cast is trash when it includes Davante Adams. Hermsmeyer used tracking data to measure how good NFL receivers are at creating separation and ranked Adams’ 2018 and 2019 seasons as the first and third best at generating separation on intermediate routes. Maybe we’ve reached the point where the lack of aggression and propensity to avoid the turnover worthy play is holding back the offense. NFL Next Gen Stats has created an aggressiveness metric that tracks the amount of passing attempts a quarterback makes that are into tight coverage, where there is a defender within 1 yard or less of the receiver at the time of completion or incompletion. The aggressiveness metric only goes back to 2016 but Rodgers has consistently been among the lowest in the league as he is not afraid to throw the ball away if he doesn’t see anything he likes.\nDesirability bias badly wants me to handwave the poor numbers over the past five years and blame McCarthy, injury, or lack of receiving weapons. I don’t know how much to weigh those factors but they are at least partially contributing to Rodgers decline in efficiency stats. I have a few more weeks to hope the offense takes a significant step foward like the 2016 Falcons offense did under Shanahan and LaFleur. But you wont’t see me making any bullish bets on this offense anytime soon.\n\n\n",
    "preview": "posts/2020-08-22-rodgers-efficiency-decline/rodgers-efficiency-decline_files/figure-html5/unnamed-chunk-10-1.png",
    "last_modified": "2020-10-29T09:08:00+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 2400
  },
  {
    "path": "posts/2020-08-19-visualizing-the-runpass-efficiency-gap/",
    "title": "Visualizing the Run/Pass Efficiency Gap",
    "description": "Using nflfastR data to show how much more efficient passing is than rushing at the team level",
    "author": [
      {
        "name": "Anthony Reinhard",
        "url": "https://twitter.com/reinhurdler"
      }
    ],
    "date": "2020-08-20",
    "categories": [
      "Figures",
      "nflfastR"
    ],
    "contents": "\nTable of Contents\nIntro\nThe Data\nPlot Extras\nBasic Plot\nFancier Plot\nIntro\nThe football philosophers of Twitter know that the passing game reigns supreme over the rushing game. As easy as it is to just say this, we need to be able to prove it. We also need to acknowledge that there could come a time where a team is so good at running the ball or so bad at passing that they should be running the ball more often. I thought an easy way to communicate this would be show the difference between a team’s dropback EPA/play and their designed rush EPA/play while also showing how often they attempt to pass.\nThe Data\nAfter we’ve pulled in the 2019 play-by-play data from the nflfastR data repository, we need to start thinking about what cuts of data we should exclude from our sample. I’ve chosen to include only 1st and 2nd down here, as teams are not burdened on these plays with having to advance the ball to the first down marker to keep their drive alive. I’m also going to only include plays where the game was reasonably competitive, which I’m defining here as win probability between 20% and 80%. Lastly, I’m going to remove plays that begin inside the final two minutes of the half, where teams are more likely to be passing (unless they are salting away a lead, of course). Keeping only the plays described above ensures that the offense is not in an obvious passing or running situation.\n\n\nlibrary(tidyverse)\n\npbp_df <- readRDS(url(\"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_2019.rds\"))\n\nprem_epa_df <- pbp_df %>%\n  filter(down <= 2 & wp <= .8 & wp >= .2 & half_seconds_remaining >= 120) %>%\n  group_by(posteam) %>%\n  summarize(\n    pass_freq = mean(pass),\n    pass_epa = mean(ifelse(pass == 1, epa, NA), na.rm = T),\n    run_epa = mean(ifelse(rush == 1, epa, NA), na.rm = T),\n    pass_epa_prem = pass_epa - run_epa\n  )\n\nPlot Extras\nIt doesn’t hurt to label the four corners of the plot so it can be understood more easily. We also want to include some gridlines that show the league averages.\n\n\nlabel_df <- data.frame(\n  pass_freq = c(.675, .425, .675, .425),\n  pass_epa_prem = c(.42, .42, -.02, -.02),\n  label = c(\n    \"Better at Passing\\nPass A Lot\",\n    \"Better at Passing\\nRun A Lot\",\n    \"Better at Running\\nPass A Lot\",\n    \"Better at Running\\nRun A Lot\"\n  )\n)\n\npbp_df %>%\n  filter(down <= 2 & wp <= .8 & wp >= .2 & half_seconds_remaining >= 120) %>%\n  group_by(season) %>%\n  summarize(\n    pass_freq = mean(pass),\n    pass_epa = mean(ifelse(pass == 1, epa, NA), na.rm = T),\n    run_epa = mean(ifelse(rush == 1, epa, NA), na.rm = T),\n    pass_epa_prem = pass_epa - run_epa\n  )\n\n# A tibble: 1 x 5\n  season pass_freq pass_epa run_epa pass_epa_prem\n   <int>     <dbl>    <dbl>   <dbl>         <dbl>\n1   2019     0.524   0.0894  -0.117         0.206\n\nWe can use this function to turn team abbreviations into the URL needed get high quality logos from ESPN.\n\n\nESPN_logo_url <- function(x) paste0(\"https://a.espncdn.com/i/teamlogos/nfl/500/\", x, \".png\")\n\nBasic Plot\nI typically make some cosmetic changes to my graphs to make them more visually appealing. If that isn’t your thing, this is the bare bones version.\n\n\nlibrary(ggimage) # I'm going to use ggimage's geom_image to add team logos here\nlibrary(scales) # percent_format will convert the y-axis to percentages\n\nggplot(data = prem_epa_df, aes(x = pass_epa_prem, y = pass_freq)) +\n  geom_hline(yintercept = 0.525, color = \"red\", linetype = \"52\", size = 0.2) +\n  geom_vline(xintercept = 0.198, color = \"red\", linetype = \"52\", size = 0.2) +\n  geom_text(data = label_df, aes(label = label), color = \"red\", size = 1.5) +\n  geom_image(aes(image = ESPN_logo_url(posteam)), size = 0.04) +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  labs(\n    title = \"Dropback Premium vs Dropback Frequency\",\n    subtitle = \"Early Downs with Win Probability Between 20% and 80% Outside of\\nLast Two Minutes of The Half\",\n    y = \"Dropback Freq\",\n    x = \"(EPA/Play on Dropbacks) - (EPA/Play on Designed Runs)\"\n  ) +\n  theme_minimal()\n\n\nFancier Plot\nAdding a bunch of junk to this plot will require loading some additional packages…\n\n\nlibrary(scales) # fixes labels\nlibrary(shadowtext) # geom_shadowtext adds a thin border around your text, making it more readable\nlibrary(ggpmisc) # geom_grob is my preferred function for placing images on plots\nlibrary(grid) # convert an image to a raster object\nlibrary(magick) # read our image in so we can make manipulations, if needed\n\nI like to use +/- signs for positive & negative sometimes. This function usually gets the job done and can be applied directly to label argument of the axis layer.\n\n\nplus_lab <- function(x, accuracy = NULL, suffix = \"\") paste0(ifelse(x > 0, \"+\", \"\"), number(x, accuracy = accuracy, suffix = suffix, scale = ifelse(suffix == \"%\", 100, 1)))\nplus_lab_format <- function(accuracy = NULL, suffix = \"\") function(x) plus_lab(x, accuracy = accuracy, suffix = suffix)\n\nThis function will take a URL and return a grob with a modified color or transparency. It will produce an image when set as the label aesthetic for geom_grob. Processing graphs takes longer with geom_grob than it does with geom_image, but aspect ratios are always perfect for each image.\n\n\ngrob_img_adj <- function(img_url, alpha = 1, whitewash = 0) {\n  return(lapply(img_url, function(x) {\n    if (is.na(x)) {\n      return(NULL)\n    } else {\n      img <- magick::image_read(x)[[1]]\n      img[1, , ] <- as.raw(255 - (255 - as.integer(img[1, , ])) * (1 - whitewash))\n      img[2, , ] <- as.raw(255 - (255 - as.integer(img[2, , ])) * (1 - whitewash))\n      img[3, , ] <- as.raw(255 - (255 - as.integer(img[3, , ])) * (1 - whitewash))\n      img[4, , ] <- as.raw(as.integer(img[4, , ]) * alpha)\n      return(grid::rasterGrob(image = magick::image_read(img)))\n    }\n  }))\n}\n\nThis is my own personal theme that I use for all my graphs. I typically use a different font, but I’ll use the default here. I’m also going to leave out my personal branding for this one.\n\n\ntheme_SB <- theme(\n  line = element_line(lineend = \"round\", color = \"darkblue\"),\n  text = element_text(color = \"darkblue\"),\n  plot.background = element_rect(fill = \"grey95\", color = \"transparent\"),\n  panel.border = element_rect(color = \"darkblue\", fill = NA),\n  panel.background = element_rect(fill = \"white\", color = \"transparent\"),\n  axis.ticks = element_line(color = \"darkblue\", size = 0.5),\n  axis.ticks.length = unit(2.75, \"pt\"),\n  axis.title = element_text(size = 8),\n  axis.text = element_text(size = 7, color = \"darkblue\"),\n  plot.title = element_text(size = 14),\n  plot.subtitle = element_text(size = 8),\n  plot.caption = element_text(size = 5),\n  legend.background = element_rect(fill = \"grey90\", color = \"darkblue\"),\n  legend.key = element_blank(),\n  panel.grid.minor = element_blank(),\n  panel.grid.major = element_line(color = \"grey85\", size = 0.3),\n  axis.title.y = element_text(angle = 0, vjust = 0.5),\n  strip.background = element_blank(),\n  strip.text = element_text(size = 6, color = \"darkblue\"),\n  legend.position = \"bottom\"\n)\n\nAnd here is the final graph!\n\n\nggplot(data = prem_epa_df, aes(x = pass_epa_prem, y = pass_freq)) +\n  geom_hline(yintercept = 0.525, color = \"red\", linetype = \"52\", size = 0.2) +\n  geom_vline(xintercept = 0.198, color = \"red\", linetype = \"52\", size = 0.2) +\n  geom_shadowtext(data = label_df, aes(label = label), color = \"red\", size = 1.5, bg.color = \"white\", bg.r = 0.2) +\n  geom_grob(aes(x = pass_epa_prem, y = pass_freq, label = grob_img_adj(ESPN_logo_url(posteam), alpha = 0.7), vp.height = 0.06)) +\n  scale_x_continuous(labels = plus_lab_format(accuracy = .01), breaks = seq(-1, 1, .1), limits = c(-0.1, 0.5), expand = expansion(mult = c(0.02, 0.02))) +\n  scale_y_continuous(labels = percent_format(accuracy = 1), breaks = seq(0, 1, .05), limits = c(.4, .7), expand = expansion(mult = c(0.03, 0.03))) +\n  labs(\n    title = \"Dropback Premium vs Dropback Frequency\",\n    subtitle = \"Early Downs with Win Probability Between 20% and 80% Outside of Last Two\\nMinutes of The Half\",\n    y = \"Dropback\\nFreq\",\n    x = \"(EPA/Play on Dropbacks) - (EPA/Play on Designed Runs)\"\n  ) +\n  theme_SB\n\n\n\n\n",
    "preview": "posts/2020-08-19-visualizing-the-runpass-efficiency-gap/visualizing-the-runpass-efficiency-gap_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2020-10-29T09:08:00+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 3300
  },
  {
    "path": "posts/2020-08-20-adjusting-epa-for-strenght-of-opponent/",
    "title": "Adjusting EPA for Strength of Opponent",
    "description": "This article shows how to adjust a team's EPA per play for the strength of their opponent. The benefits of adjusted EPA will be demonstrated as well!",
    "author": [
      {
        "name": "Jonathan Goldberg",
        "url": "https://twitter.com/gberg1303"
      }
    ],
    "date": "2020-08-20",
    "categories": [
      "Opponent adjusted EPA",
      "Figures",
      "nflfastR"
    ],
    "contents": "\nHere we are going to take a look at how to adjust a team’s epa per play to the strength of their opponent. This technique will use weekly epa/play metrics, which can ultimately summarize a team’s season-long performance. It is also possible to adjust the epa of individual plays with this process if you are so inclined to do so.\nQuick note: the adjustments were inspired by the work done in this paper. It’s a bit technical but a good additional read!\nAlright, let’s get into it by first loading up our data!\n\n\nNFL_PBP <- purrr::map_df(2009:2019, function(x) {\n  readr::read_csv(\n    glue::glue(\"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_{x}.csv.gz\")\n  )\n})\n\nWith the data loaded, we can finally get down to business by summarizing each team’s weekly epa/play.\n\n\nlibrary(tidyverse)\nepa_data <- NFL_PBP %>%\n  dplyr::filter(!is.na(epa), !is.na(ep), !is.na(posteam), play_type == \"pass\" | play_type == \"run\") %>%\n  dplyr::group_by(game_id, season, week, posteam, home_team) %>%\n  dplyr::summarise(\n    off_epa = mean(epa),\n  ) %>%\n  dplyr::left_join(NFL_PBP %>%\n    filter(!is.na(epa), !is.na(ep), !is.na(posteam), play_type == \"pass\" | play_type == \"run\") %>%\n    dplyr::group_by(game_id, season, week, defteam, away_team) %>%\n    dplyr::summarise(def_epa = mean(epa)),\n  by = c(\"game_id\", \"posteam\" = \"defteam\", \"season\", \"week\"),\n  all.x = TRUE\n  ) %>%\n  dplyr::mutate(opponent = ifelse(posteam == home_team, away_team, home_team)) %>%\n  dplyr::select(game_id, season, week, home_team, away_team, posteam, opponent, off_epa, def_epa)\n\nNow we can get into the fun part: adjusting a team’s epa/play based on the strength of the opponent they are up against.\nWe are going to reframe each team’s epa/play as a team’s weekly opponent.\nWe are going to convert each statistic into a moving average of the last ten games — this decision was based on this research and this model — and lag that statistic by one week. The lag is important because we need to be comparing a team’s weekly performance against their opponent’s average performance up to that point in the season.\nWe are going to join the data back to the epa_dataset.\n\n\n# Construct opponent dataset and lag the moving average of their last ten games.\nopponent_data <- epa_data %>%\n  dplyr::select(-opponent) %>%\n  dplyr::rename(\n    opp_off_epa = off_epa,\n    opp_def_epa = def_epa\n  ) %>%\n  dplyr::group_by(posteam) %>%\n  dplyr::arrange(season, week) %>%\n  dplyr::mutate(\n    opp_def_epa = pracma::movavg(opp_def_epa, n = 10, type = \"s\"),\n    opp_def_epa = dplyr::lag(opp_def_epa),\n    opp_off_epa = pracma::movavg(opp_off_epa, n = 10, type = \"s\"),\n    opp_off_epa = dplyr::lag(opp_off_epa)\n  )\n\n# Merge opponent data back in with the weekly epa data\nepa_data <- epa_data %>%\n  left_join(\n    opponent_data,\n    by = c(\"game_id\", \"season\", \"week\", \"home_team\", \"away_team\", \"opponent\" = \"posteam\"),\n    all.x = TRUE\n  )\n\nDon’t fret that the opponent’s epa columns will have NAs in the first week. You simply can’t lag from the first observation.\nThe final piece of the equation needed to make the adjustments is the league mean for epa/play on offense and defense. We need to know how strong the opponent is relative to the average team in the league.\n\n\nepa_data <- epa_data %>%\n  dplyr::left_join(epa_data %>%\n    dplyr::filter(posteam == home_team) %>%\n    dplyr::group_by(season, week) %>%\n    dplyr::summarise(\n      league_mean = mean(off_epa + def_epa)\n    ) %>%\n    dplyr::ungroup() %>%\n    dplyr::group_by(season) %>%\n    dplyr::mutate(\n      league_mean = lag(pracma::movavg(league_mean, n = 10, type = \"s\"), ) # We lag because we need to know the league mean up to that point in the season\n    ),\n  by = c(\"season\", \"week\"),\n  all.x = TRUE\n  )\n\nFinally, we can get to adjusting a team’s epa/play. We’ll create an adjustment measure by subtracting the opponent’s epa/play metrics from the league mean. Then we add the adjustment measure to each team’s weekly performance.\n\n\n# Adjust EPA\nepa_data <- epa_data %>%\n  dplyr::mutate(\n    off_adjustment_factor = ifelse(!is.na(league_mean), league_mean - opp_def_epa, 0),\n    def_adjustment_factor = ifelse(!is.na(league_mean), league_mean - opp_off_epa, 0),\n    adjusted_off_epa = off_epa + off_adjustment_factor,\n    adjusted_def_epa = def_epa + def_adjustment_factor,\n  )\n\nWe’re done! You can now view each team’s epa/play adjusted for their strength of schedule. Let’s check out how different the league looks by comparing unadjusted epa to adjusted epa stats.\n\n\n\nAbove, you can see that some teams are revealed to be stronger after adjusting their epa/play while other teams appear to be weaker. We can use these adjustments to make more accurate predictions of individual NFL games.\nHere, each metrics are used in separate glm models to predict the outcome of games from the past two seasons. Their accuracy is below.\n\n\n[1] \"Adjusted EPA Accuracy\"\n\n[1] 0.6404494\n\n\n\n[1] \"Normal EPA Accuracy\"\n\n[1] 0.6348315\n\nThere is a slight edge to the adjusted EPA model. Its a solid start but there is more work to be done in finding the best version on epa/play.\nThere is good work being done on properly weighting epa on a given type of play. For instance, DVOA is a does well in predicting future team performance because the downweight the impact of interceptions in their metric. More work can be done to properly weight epa based on its play type!\nIt is possible to make these adjustments at the individual play level and with more specificity. For instance, you could adjust run plays based on the team’s run defense rather than adjusting the entire offense to the team’s entire defense. I think more work should be done to determine if these more detailed techniques can improve the predictiveness of the stat.\nThere may be other ways to construct epa/play that improve its strength as a predictor The paper that inspired this article uses the solution of an optimization problem to construct a team’s true offensive epa/play and defensive epa/play. Perhaps a moving averaged should be eschewed in favor of a technique that more properly accounts for common regression to the mean over the offseason.\nThanks to Sebastian Carl and Ben Baldwin for setting this forum up! I can’t wait to see others’ works and improvements to my own make its way on here.\n\n\n",
    "preview": "posts/2020-08-20-adjusting-epa-for-strenght-of-opponent/adjusting-epa-for-strength-of-opponent_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2020-10-29T09:08:00+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 3300
  },
  {
    "path": "posts/2020-08-20-python-contributing-example/",
    "title": "Python contributing example",
    "description": "Showing how to contribute using Python code",
    "author": [
      {
        "name": "Ben Baldwin",
        "url": "https://twitter.com/benbbaldwin"
      }
    ],
    "date": "2020-08-20",
    "categories": [
      "nflfastR",
      "python"
    ],
    "contents": "\nTable of Contents\nContributing in pythonThere is some stuff to install and setup:\nOnce setup is done:\n\nContributing in python\nThere is some stuff to install and setup:\nR and RStudio\nGet github in RStudio setup (see here)\nIn R, run install.packages(c(\"distill\", \"rmarkdown\")) to get the needed packages for the website installed, and install.packages(\"reticulate\") to install reticulate, which allows python to be run from within R\nYou’ll need to somehow point RStudio to your python installation. I ran py_install(\"pandas\") and the below worked\nOnce setup is done:\nStart working on your document. You can put in python code chunks as in the examples below (see here for the source code).\nOnce you have everything ready in your document, press the “Knit” button on the top and it will create the document.\nThen you can committ all the files using the Git tab\nPlease see the step-by-step instructions here\nOnce everything is set up, you can just use python code as normal. Here is a quick example borrowing some code from from Deryck’s nflfastR python guide.\n\n\nimport pandas\ndata = pandas.read_csv(\"https://github.com/guga31bb/nflfastR-data/blob/master/data/play_by_play_2019.csv.gz?raw=True\", compression='gzip', low_memory=False)\ndata = data.loc[(data.play_type.isin(['no_play','pass','run'])) & (data.epa.isna()==False)]\ndata.groupby('posteam')[['epa']].mean()\n\n              epa\nposteam          \nARI      0.002421\nATL      0.009436\nBAL      0.165036\nBUF     -0.030460\nCAR     -0.091361\nCHI     -0.082597\nCIN     -0.095607\nCLE     -0.039001\nDAL      0.104154\nDEN     -0.057336\nDET     -0.046365\nGB       0.054262\nHOU      0.026844\nIND      0.002937\nJAX     -0.060641\nKC       0.167632\nLA      -0.007031\nLAC      0.027416\nLV       0.024514\nMIA     -0.082475\nMIN      0.024179\nNE      -0.004101\nNO       0.060163\nNYG     -0.062409\nNYJ     -0.147206\nPHI     -0.002494\nPIT     -0.146697\nSEA      0.024912\nSF       0.066445\nTB      -0.017379\nTEN      0.058257\nWAS     -0.133023\n\nGrouping by QBs:\n\n\nqbs = data.groupby(['passer','posteam'], as_index=False).agg({'epa':'mean',\n                                                              'cpoe':'mean',\n                                                              'play_id':'count'})\n# at least 200 plays\nqbs = qbs.loc[qbs.play_id>199]\n# sort by EPA\nqbs.sort_values('epa', ascending=False, inplace=True)\n\n#Round to two decimal places where appropriate\nqbs = qbs.round(2)\n\n#Rename columns\nqbs.columns = ['Player','Team','EPA per Dropback','CPOE','Dropbacks']\n\nqbs\n\n            Player Team  EPA per Dropback   CPOE  Dropbacks\n86       P.Mahomes   KC              0.32   2.26        721\n71       L.Jackson  BAL              0.29   2.76        554\n81      M.Stafford  DET              0.22   2.09        330\n39      D.Prescott  DAL              0.19   0.97        675\n95     R.Tannehill  TEN              0.19   6.36        421\n28         D.Brees   NO              0.18   6.20        447\n52     J.Garoppolo   SF              0.17   1.81        629\n96        R.Wilson  SEA              0.16   7.10        732\n64       K.Cousins  MIN              0.16   3.77        585\n40        D.Watson  HOU              0.15   2.12        715\n6        A.Rodgers   GB              0.15   1.72        755\n29          D.Carr   LV              0.14   5.69        574\n87        P.Rivers  LAC              0.11   3.72        676\n105  T.Bridgewater   NO              0.09   0.41        235\n53          J.Goff   LA              0.09  -1.79        703\n61       J.Winston   TB              0.08   0.67        743\n92   R.Fitzpatrick  MIA              0.07  -0.48        612\n25         C.Wentz  PHI              0.07  -0.77        719\n79          M.Ryan  ATL              0.06   1.80        731\n104        T.Brady   NE              0.06  -2.53        724\n46      J.Brissett  IND              0.06  -3.26        540\n45         J.Allen  BUF              0.03  -2.29        618\n14      B.Mayfield  CLE              0.02  -2.96        641\n67        K.Murray  ARI              0.02  -1.52        654\n18        C.Keenum  WAS              0.01  -1.09        285\n44    G.Minshew II  JAX              0.01  -4.28        592\n75       M.Mariota  TEN              0.01  -3.21        209\n1         A.Dalton  CIN             -0.00  -3.27        601\n97       S.Darnold  NYJ             -0.01   1.04        516\n62         K.Allen  CAR             -0.03  -0.95        574\n36         D.Jones  NYG             -0.03  -1.94        560\n82      M.Trubisky  CHI             -0.03  -2.33        605\n78       M.Rudolph  PIT             -0.03  -1.23        331\n51        J.Flacco  DEN             -0.06  -0.21        313\n32       D.Haskins  WAS             -0.18  -4.65        255\n27        D.Blough  DET             -0.20 -13.59        209\n\nHopefully the process is painless once all the setup is done.\n\n\n",
    "preview": {},
    "last_modified": "2020-10-29T09:08:00+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-08-19-matching-players-without-id-keys/",
    "title": "Matching players without ID keys",
    "description": "Rebuilding player graphs when ID keys go missing or are corrupted.",
    "author": [
      {
        "name": "Analytics Darkweb",
        "url": "https://twitter.com/footballdaRkweb"
      }
    ],
    "date": "2020-08-19",
    "categories": [
      "Figures",
      "Roster",
      "nflfastR"
    ],
    "contents": "\nSometimes when we go to remake graphs from other people resources that used to work no longer do. This example shows how to work through that problem.\nFirst let’s load the data and define which season we care about.\n\n\nlibrary(tidyverse)\nlibrary(nflfastR)\nseasons <- 2019\npbp <- purrr::map_df(seasons, function(x) {\n  readr::read_csv(\n    glue::glue(\"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_{x}.csv.gz\")\n  )\n})\n\nroster <-\n  readRDS(url(\"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/roster-data/roster.rds\")) %>%\n  filter(teamPlayers.position == \"QB\", team.season == 2019)\n\ncpoe <-\n  pbp %>%\n  filter(!is.na(cpoe)) %>%\n  group_by(passer_player_id, air_yards) %>%\n  summarise(\n    count = n(),\n    cpoe = mean(cpoe)\n  )\n\nseason <- 2019\n\nNext, we have a couple of players that are in our Top 30 from last year that have changed teams from where our roster has a different nfl ID structure than what we get from nflfastR.\nSo instead we can match on first initial, last name, and jersey number. I’ve renamed posteam to team.abbr below to change as little of Seb’s code as possible. Also, we have to update the roster file to indicate that the Raiders have moved to Las Vegas.\n\n\nsummary <-\n  pbp %>%\n  separate(passer_player_name, c(\"firstName\", \"lastName\"), sep = \"\\\\.\") %>%\n  filter(!is.na(cpoe)) %>%\n  group_by(lastName, posteam, firstName, passer_player_id, jersey_number) %>%\n  summarise(plays = n()) %>%\n  ungroup() %>%\n  arrange(desc(plays)) %>%\n  head(30) %>%\n  mutate(\n     lastName = ifelse(lastName == \"Minshew II\", \"Minshew\", lastName)\n   ) %>%\n  left_join(\n    roster %>% \n      filter(team.season == seasons) %>% \n      mutate(firstInit = str_extract(teamPlayers.firstName, \"\\\\w\"), team.abbr = ifelse(team.abbr == \"OAK\", \"LV\", team.abbr)) %>% \n      select(name = teamPlayers.displayName, firstInit, teamPlayers.lastName, team.abbr, teamPlayers.headshot_url, teamPlayers.jerseyNumber),\n      by = c(\"lastName\" = \"teamPlayers.lastName\", \"firstName\" = \"firstInit\", \"jersey_number\" = \"teamPlayers.jerseyNumber\", \"posteam\" = \"team.abbr\")\n  ) %>%\n  mutate(# some headshot urls are broken. They are checked here and set to a default \n    teamPlayers.headshot_url = dplyr::if_else(\n      RCurl::url.exists(as.character(teamPlayers.headshot_url)),\n      as.character(teamPlayers.headshot_url),\n      \"http://static.nfl.com/static/content/public/image/fantasy/transparent/200x200/default.png\",\n    )\n  ) %>%\n  left_join(cpoe, by = \"passer_player_id\") %>%\n  left_join(\n    teams_colors_logos %>% select(team_abbr, team_color, team_logo_espn),\n    by = c(\"posteam\" = \"team_abbr\")\n  ) %>%\n  rename(team.abbr = posteam)\n\ncolors_raw <-\n  summary %>%\n  group_by(passer_player_id) %>%\n  summarise(team = first(team.abbr), name = first(name)) %>%\n  left_join(\n    teams_colors_logos %>% select(team_abbr, team_color),\n    by = c(\"team\" = \"team_abbr\")\n  ) %>%\n  arrange(name)\n\nn_eval <- 80\ncolors <-\n  as.data.frame(lapply(colors_raw, rep, n_eval)) %>%\n  arrange(name)\n\n\nmean <-\n  summary %>%\n  group_by(air_yards) %>%\n  summarise(league = mean(cpoe), league_count = n())\n\nNext, we need to change how Sebastian calls for colors in the geom_smooth call due to a package update. We can make a named vector to match the color hex numbers to the data frame.\n\n\nasp <- 1.2\ncols <- c()\nfor(i in 1:length(unique(summary$team_color))) {\n  cols <- append(cols, unique(summary$team_color)[i])\n}\ncolor_names <- as.vector(unique(summary$team_color))\ncols <- set_names(cols, color_names)\n\nplot <-\n  summary %>%\n  ggplot(aes(x = air_yards, y = cpoe)) +\n  geom_smooth(\n    data = mean, aes(x = air_yards, y = league, weight = league_count), n = n_eval,\n    color = \"red\", alpha = 0.7, se = FALSE, size = 0.5, linetype = \"dashed\"\n  ) +\n  geom_smooth(\n    se = FALSE, alpha = 0.7, aes(weight = count, color = team_color), size = 0.65,\n    n = n_eval\n  ) +\n  scale_color_manual(values = cols) + \n  geom_point(color = summary$team_color, size = summary$count / 15, alpha = 0.4) +\n  ggimage::geom_image(aes(x = 27.5, y = -20, image = team_logo_espn),\n                      size = .15, by = \"width\", asp = asp\n  ) +\n  ggimage::geom_image(aes(x = -2.5, y = -20, image = teamPlayers.headshot_url),\n                      size = .15, by = \"width\", asp = asp\n  ) +\n  xlim(-10, 40) + # makes sure the smoothing algorithm is evaluated between -10 and 40\n  coord_cartesian(xlim = c(-5, 30), ylim = c(-25, 25)) + # 'zoom in'\n  labs(\n    x = \"Target Depth In Yards Thrown Beyond The Line Of Scrimmage (DOT)\",\n    y = \"Completion Percentage Over Expectation (CPOE in percentage points)\",\n    caption = \"Figure: @mrcaseb | Data: @nflfastR | Update: AnalyticsDarkweb\",\n    title = glue::glue(\"Passing Efficiency {season}\"),\n    subtitle = \"CPOE function of depth. Dotsize equivalent to num targets. Red Line = League Average.\"\n  ) +\n  theme_bw() +\n  theme(\n    axis.title = element_text(size = 10),\n    axis.text = element_text(size = 6),\n    plot.title = element_text(size = 12, hjust = 0.5, face = \"bold\"),\n    plot.subtitle = element_text(size = 10, hjust = 0.5),\n    plot.caption = element_text(size = 8),\n    legend.title = element_text(size = 8),\n    legend.text = element_text(size = 6),\n    strip.text = element_text(size = 6, hjust = 0.5, face = \"bold\"),\n    aspect.ratio = 1 / asp,\n    legend.position = \"none\"\n  ) +\n  facet_wrap(vars(name), ncol = 6, scales = \"free\")\n\nLastly, we can kick out a save of our image file as before.\n\n\nplot\n\n\nggsave(glue::glue(\"cpoe_vs_dot_{season}.png\"), dpi = 600, width = 24, height = 21, units = \"cm\")\n\n\n\n",
    "preview": "posts/2020-08-19-matching-players-without-id-keys/matching-players-without-id-keys_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2020-10-29T09:08:00+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 4800
  },
  {
    "path": "posts/2020-08-19-neural-nets-using-r/",
    "title": "Neural Nets using R",
    "description": "Using Keras in R to build neural networks.",
    "author": [
      {
        "name": "Analytics Darkweb",
        "url": "https://twitter.com/footballdaRkweb"
      }
    ],
    "date": "2020-08-19",
    "categories": [
      "Keras",
      "Tensorflow",
      "nflfastR"
    ],
    "contents": "\n\n\nlibrary(reticulate)\nlibrary(tidyverse)\n\nuse_condaenv(\"r-tf-gpu\", required = TRUE)\n\n\nlibrary(keras)\n\nBefore you proceed you will need to install Keras for R. In order to do that, I followed this guide. https://github.com/antoniosehk/keras-tensorflow-windows-installation\nThe following guide is heavily borrowed from the following Rstudio guide! https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_regression/\nUse the following lines to download the data if you need to.\n\n\nseasons <- 2010:2019\npbp <- purrr::map_df(seasons, function(x) {\n  readr::read_csv(\n    glue::glue(\"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_{x}.csv.gz\")\n  )\n})\n\nInstead of throwing the kitchen sink at a problem, let’s choose some variables we think would influence yards after catch.\n\n\ndf <-\n  pbp %>%\n  filter(pass == 1) %>%\n  mutate(\n    pass_location = as.numeric(ifelse(pass_location == \"middle\", 1, 0)),\n    roof = as.numeric(as.factor(roof))\n    ) %>%\n  select(yardline_100, down, ydstogo, shotgun, air_yards, yards_after_catch, qb_hit, pass_location, roof) %>%\n  na.omit()\n\nIn this step you are converting your data frame into something Keras can injest.\n\n\nset.seed(7)\nsample <- sample.int(n = nrow(df), size = floor(.9*nrow(df)), replace = F)\ntrain_df <- df[sample, ]\ntest_df  <- df[-sample, ]\n\ntrain_labels <- train_df$yards_after_catch\ntest_labels <- test_df$yards_after_catch\n\ntrain_df <- train_df %>% select(-yards_after_catch)\ntest_df <- test_df %>% select(-yards_after_catch)\n\ncolumn_names <- colnames(train_df)\n\ntrain_df <- train_df %>% \n  as_tibble(.name_repair = \"minimal\") %>% \n  setNames(column_names) %>% \n  mutate(label = train_labels)\n\ntest_df <- test_df %>% \n  as_tibble(.name_repair = \"minimal\") %>% \n  setNames(column_names) %>% \n  mutate(label = test_labels)\n\nNext we’ll use a little helper function to create the model, here we’re just doing a little toy model. No convolutions or anything too fancy. Just a little good ole fashioned brute force! Mostly because you should go read about different network types before you use them. :)\n\n\nlibrary(tfdatasets)\n\nspec <- feature_spec(train_df, label ~ . ) %>% \n  step_numeric_column(all_numeric(), normalizer_fn = scaler_standard()) %>% \n  fit()\n\nspec\n\n-- Feature Spec --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- \nA feature_spec with 8 steps.\nFitted: TRUE \n-- Steps ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- \nThe feature_spec has 1 dense features.\nStepNumericColumn: yardline_100, down, ydstogo, shotgun, air_yards, qb_hit, pass_location, roof \n-- Dense features ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- \n\nlayer <- layer_dense_features(\n  feature_columns = dense_features(spec), \n  dtype = tf$float32\n)\n\nbuild_model <- function() {\n  input <- layer_input_from_dataset(train_df %>% select(-label))\n  \n  output <- input %>% \n    layer_dense_features(dense_features(spec)) %>% \n    layer_dense(units = 64, activation = \"relu\") %>%\n    layer_dropout(.25) %>%\n    layer_dense(units = 64, activation = \"relu\") %>%\n    layer_dropout(.25) %>%\n    layer_dense(units = 64, activation = \"relu\") %>%\n    layer_dropout(.25) %>%\n    layer_dense(units = 1) \n  \n  model <- keras_model(input, output)\n  \n  model %>% \n    compile(\n      loss = \"mse\",\n      optimizer = \"adam\",\n      metrics = list(\"mean_absolute_error\")\n    )\n  \n  model\n}\n\nearly_stop <- callback_early_stopping(monitor = \"val_loss\", patience = 20)\n\nprint_dot_callback <- callback_lambda(\n  on_epoch_end = function(epoch, logs) {\n    if (epoch %% 20 == 0) cat(\"\\n\")\n    cat(\".\")\n  }\n)  \n\nmodel <- build_model()\n\nsummary(model)\n\nModel: \"model\"\n______________________________________________________________________\nLayer (type)           Output Shape   Param # Connected to            \n======================================================================\nair_yards (InputLayer) [(None,)]      0                               \n______________________________________________________________________\ndown (InputLayer)      [(None,)]      0                               \n______________________________________________________________________\npass_location (InputLa [(None,)]      0                               \n______________________________________________________________________\nqb_hit (InputLayer)    [(None,)]      0                               \n______________________________________________________________________\nroof (InputLayer)      [(None,)]      0                               \n______________________________________________________________________\nshotgun (InputLayer)   [(None,)]      0                               \n______________________________________________________________________\nyardline_100 (InputLay [(None,)]      0                               \n______________________________________________________________________\nydstogo (InputLayer)   [(None,)]      0                               \n______________________________________________________________________\ndense_features_1 (Dens (None, 8)      0       air_yards[0][0]         \n                                              down[0][0]              \n                                              pass_location[0][0]     \n                                              qb_hit[0][0]            \n                                              roof[0][0]              \n                                              shotgun[0][0]           \n                                              yardline_100[0][0]      \n                                              ydstogo[0][0]           \n______________________________________________________________________\ndense (Dense)          (None, 64)     576     dense_features_1[0][0]  \n______________________________________________________________________\ndropout (Dropout)      (None, 64)     0       dense[0][0]             \n______________________________________________________________________\ndense_1 (Dense)        (None, 64)     4160    dropout[0][0]           \n______________________________________________________________________\ndropout_1 (Dropout)    (None, 64)     0       dense_1[0][0]           \n______________________________________________________________________\ndense_2 (Dense)        (None, 64)     4160    dropout_1[0][0]         \n______________________________________________________________________\ndropout_2 (Dropout)    (None, 64)     0       dense_2[0][0]           \n______________________________________________________________________\ndense_3 (Dense)        (None, 1)      65      dropout_2[0][0]         \n======================================================================\nTotal params: 8,961\nTrainable params: 8,961\nNon-trainable params: 0\n______________________________________________________________________\n\nNext let’s run the model and see how it does!\n\n\nhistory <- model %>% fit(\n  x = train_df %>% select(-label),\n  y = train_df$label,\n  epochs = 500,\n  batchsize = 64,\n  validation_split = 0.2,\n  verbose = 0,\n  callbacks = list(print_dot_callback, early_stop)\n)\n\n....................\n.................\n\nNow to check the results!\nHere we visualize how our nnet trained over our epochs. We define epochs here since we had some early stopping.\n\n\nlibrary(ggplot2)\nhistory$params$epochs <- length(history$metrics$loss)\n\nplot(history)\n\n\ntest_predictions <- model %>% predict(test_df %>% select(-label))\n\nNext we can take a look at the mean absolute error and loss from our model on the test set.\n\n\nc(loss, mae) %<-% (model %>% evaluate(test_df %>% select(-label), test_df$label, verbose = 0))\n\nloss\n\n[1] 46.37026\n\nmae\n\n[1] 4.239087\n\ntest_predictions <- test_predictions %>% as.data.frame()\n\ntest_predictions %>% ggplot(aes(V1)) + geom_density()\n\n\nLastly, let’s visualize our trained model versus both actual YAC yardage and nflfastR’s XYAC mean yards model.\n\n\ndf <-\n  pbp %>%\n  filter(pass == 1) %>%\n  mutate(\n    pass_location = as.numeric(ifelse(pass_location == \"middle\", 1, 0)),\n    roof = as.numeric(as.factor(roof))\n  ) %>%\n  select(yardline_100, down, ydstogo, shotgun, air_yards, yards_after_catch, qb_hit, pass_location, roof, xyac_mean_yardage) %>%\n  na.omit()\n\ntrain_df <- df %>% \n  as_tibble(.name_repair = \"minimal\") %>% \n  setNames(colnames(df)) %>% \n  mutate(label = yards_after_catch)\n\ntest_predictions <- model %>% predict(train_df %>% select(-label))\n\ntest_predictions <- test_predictions %>% as.data.frame()\n\ndf <- \n  cbind(df, test_predictions)\n\ndf %>%\n  ggplot() + \n  geom_density(aes(V1), color = \"blue\") + \n  geom_density(aes(yards_after_catch)) + \n  geom_density(aes(xyac_mean_yardage), color = \"red\") +\n  xlim(c(-10, 20)) + \n  theme_minimal() + \n  labs(\n    title = \"Expected YAC yardage\",\n    x = \"YAC Yardage\",\n    y = \"Density\",\n    subtitle = \"Actual: Black, NNet: Blue, XYAC_Mean: Red\"\n    \n  )\n\n\nThere you have it, your own little nnet done completely in R using the Keras/tensorflow backend.\n\n\n",
    "preview": "posts/2020-08-19-neural-nets-using-r/neural-nets-using-r_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2020-10-29T09:08:00+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 2400
  },
  {
    "path": "posts/2020-08-19-the-accumulation-of-qb-hits-vs-passing-efficiency/",
    "title": "The accumulation of QB hits vs passing efficiency",
    "description": "Do quarterbacks who get hit see their performance decline throughout the game?",
    "author": [
      {
        "name": "Ben Baldwin",
        "url": "https://twitter.com/benbbaldwin"
      }
    ],
    "date": "2020-08-19",
    "categories": [
      "Figures",
      "nflfastR"
    ],
    "contents": "\nTable of Contents\nGet the data\nCalculate total hits and cumulative hits\nMake sure the data are sound\nSome final cleaning up\nMake the graph\nWrapping up\nIn a follow-up to his excellent piece on the value of the run game in The Athletic (great website, highly recommended), Ted Nguyen shared the following:\n“In-house NFL analytics crews track QB hits and the results of the accumulation of hits and how it affects offensive performance over the course of a game.”\nDoes the accumulation of hits affect offensive performance over the game? Is this finally a feather in the cap for the run game defenders?\nBecause QB hits are tracked by the NFL, we can investigate this ourselves. Let’s dive in.\nGet the data\n\n\nlibrary(tidyverse)\n\npbp <- map_df(2015 : 2019, ~{\n  readRDS(\n    url(\n      glue::glue(\"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_{.}.rds\")\n    )\n  ) %>%\n    filter(pass == 1, !is.na(epa))\n})\n\nAs a starting point, I’m using the saved dataset of pass plays from nflfastR.\nLet’s make sure the qb_hit variable includes penalty plays, because presumably a quarterback feels the effects of a hit even if the play didn’t count.\n\n\npbp %>% \n  filter(qb_hit==1, play_type == \"no_play\") %>%\n  select(desc, qb_hit)\n\n# A tibble: 0 x 2\n# ... with 2 variables: desc <chr>, qb_hit <dbl>\n\nWomp womp. Let’s see if we can just create hits by searching for the bracket [ since that’s what NFL uses to denote hits.\n\n\npbp %>% \n  filter(play_type != \"no_play\") %>%\n  mutate(\n    hit = if_else(stringr::str_detect(desc, \"\\\\[\") | sack == 1, 1, 0)\n  ) %>%\n  group_by(hit, qb_hit) %>%\n  summarize(n = n())\n\n# A tibble: 4 x 3\n# Groups:   hit [2]\n    hit qb_hit     n\n  <dbl>  <dbl> <int>\n1     0      0 88971\n2     0      1     3\n3     1      0   373\n4     1      1 14515\n\nJust counting sacks and hits works pretty well for the non-penalty plays; there’s high agreement between the official NFL stats (qb_hit) and the variable we created (hit). Let’s see which plays drive the difference:\n\n\npbp %>% \n  filter(play_type != \"no_play\") %>%\n  mutate(\n    hit = if_else(stringr::str_detect(desc, \"\\\\[\") | sack == 1, 1, 0)\n  ) %>%\n  filter(hit == 0 & qb_hit == 1) %>%\n  select(desc)\n\n# A tibble: 3 x 1\n  desc                                                                \n  <chr>                                                               \n1 (6:33) (Shotgun) 5-J.Flacco to DEN 15 for -5 yards. FUMBLES, and re~\n2 (8:37) (Shotgun) 11-C.Wentz Aborted. 62-J.Kelce FUMBLES at NYJ 28, ~\n3 (:44) (Shotgun) 6-D.Hodges FUMBLES (Aborted) at NYJ 49, and recover~\n\nI guess these are data errors or something? I don’t know.\n\n\npbp %>% \n  filter(play_type != \"no_play\") %>%\n  mutate(\n    hit = if_else(stringr::str_detect(desc, \"\\\\[\") | sack == 1, 1, 0)\n  ) %>%\n  filter(hit == 1 & qb_hit == 0) %>%\n  select(desc)\n\n# A tibble: 373 x 1\n   desc                                                               \n   <chr>                                                              \n 1 (7:04) (Shotgun) 5-T.Taylor sacked ob at BUF 23 for -6 yards (50-R~\n 2 (4:41) (Shotgun) 12-T.Brady sacked at BUF 41 for -6 yards (55-J.Hu~\n 3 (14:21) 3-R.Wilson sacked ob at SEA 43 for -1 yards (56-J.Peppers).\n 4 (13:08) (Shotgun) 5-N.Foles sacked at STL 31 for -6 yards. FUMBLES~\n 5 (4:58) (Shotgun) 9-D.Brees sacked at NO 15 for -8 yards (56-J.Smit~\n 6 (1:37) (No Huddle, Shotgun) 8-M.Mariota sacked at CLE 43 for -8 ya~\n 7 (1:17) (No Huddle, Shotgun) 17-R.Tannehill sacked at MIA 39 for -7~\n 8 (14:21) (Shotgun) 12-A.Luck sacked at IND 18 for -5 yards (97-K.Kl~\n 9 (11:33) (Shotgun) 5-B.Bortles sacked at NE 31 for -6 yards (91-J.C~\n10 (14:13) (Shotgun) 2-M.Vick sacked ob at PIT 27 for 0 yards (52-A.O~\n# ... with 363 more rows\n\nSeems like these are sacks out of bounds or fumbles without getting hit. Okay whatever, close enough. Let’s go with the official qb_hit on normal plays and the created version for no_play.\n\n\nhits_data <- pbp %>% \n  mutate(\n    hit = case_when(\n      play_type != \"no_play\" & qb_hit == 1 ~ 1,\n      play_type == \"no_play\" & (stringr::str_detect(desc, \"\\\\[\") | sack == 1) ~ 1,\n      TRUE ~ 0\n    )\n  )\n\nCalculate total hits and cumulative hits\nNow we need to create two variables: (1) qb hits taken up to the current point in the game and (2) total qb hits taken in the game. I’ll also filter out run plays.\n\n\nhits_data <- hits_data %>% \n  group_by(posteam, game_id) %>%\n  mutate(\n    cum_hits=cumsum(qb_hit),\n    total_hits=sum(qb_hit)\n  ) %>%\n  ungroup()\n\nI’m grouping by team (posteam), which isn’t quite perfect. If a team has to switch quarterbacks mid-game, then the count of hits won’t be accurate for the second quarterback. But because these situations are so rare, it shouldn’t matter in the aggregate.\nThe variable cum_hits is created using cumsum, which totals up how many QB hits a team has suffered to that point in the game. And total_hits just sums up the total number of hits over the whole game. I’m kind of amazed at how easy this is to do in R.\nNow let’s see how total_hits affects EPA per dropback at the game level:\n\n\nhits_data %>% \n  group_by(total_hits) %>%\n  summarize(\n    mean_epa = mean(epa),\n    games=n_distinct(game_id, posteam)\n    )\n\n# A tibble: 17 x 3\n   total_hits mean_epa games\n        <dbl>    <dbl> <int>\n 1          0   0.256     35\n 2          1   0.232    119\n 3          2   0.195    238\n 4          3   0.167    307\n 5          4   0.123    374\n 6          5   0.0557   407\n 7          6   0.0339   331\n 8          7   0.0171   285\n 9          8  -0.0162   200\n10          9  -0.0511   146\n11         10  -0.0343    90\n12         11  -0.112     59\n13         12  -0.0274    36\n14         13  -0.0337    24\n15         14   0.0182     7\n16         15  -0.249      7\n17         16  -0.245      5\n\nWow, the most efficient games are most decidedly the ones in which a QB isn’t hit often!\nMake sure the data are sound\nI was surprised that there have been so many games where a QB was never hit (35, the first row above). Initially I thought I did something wrong, but it checks out. Let’s make sure we can replicate the official NFL data. I’m going to look at the later stage of Cleveland’s 2018 season because I know that’s where some of the 0-hit games come from.\n\n\nhits_data %>% \n  filter(posteam == \"CLE\" & season == 2018 & week >= 10) %>%\n  group_by(week) %>%\n  summarize(hits = mean(total_hits), mean_epa = mean(epa))\n\n# A tibble: 7 x 3\n   week  hits mean_epa\n  <int> <dbl>    <dbl>\n1    10     0   0.614 \n2    12     1   0.837 \n3    13     1  -0.145 \n4    14     1  -0.112 \n5    15     3  -0.0154\n6    16     0   0.458 \n7    17     2   0.211 \n\nNow compare to the official stats (with thanks to SportRadar):\nHitsBoom! A perfect match!\nSome final cleaning up\nReturning to the relationship between hits and EPA per dropback, case closed, right? Games with fewer hits have higher EPA per dropback. Well, not so fast. This is picking up, in part, a game script effect, where overmatched teams fall behind early and are forced to pass a lot, resulting in their QB being hit more often.\nSo we want to create a level playing field. To do this, let’s take teams with a given number of hits and see how the number of accumulated hits affects passing efficiency, holding the total number of hits received in the game constant. There are a number of other ways we could have approached this – looking at plays within some range of win probability or score differential, for example – but I think this is a nice illustration.\n\n\nhits_data <- hits_data %>%\n  mutate(\n    hit_in_game=\n      case_when(total_hits==0 | total_hits==1~\"0-1\",\n                 total_hits==2 | total_hits==3~\"2-3\", \n                 total_hits==4 | total_hits==5~\"4-5\", \n                 total_hits==6 | total_hits==7~\"6-7\", \n                 total_hits==8|total_hits==9~\"8-9\", \n                 total_hits>9~\"10+\") %>% \n                  factor(levels = c(\"0\", \"2-3\", \"4-5\", \"6-7\", \"8-9\", \"10+\"))\n    )\n\nAbove, we’ve created some BINS based on how often a quarterback is hit in a game (the factor(levels… part at the end isn’t strictly necessary, but allows the legend to display in the right order later on).\nNow we can group by our bins, along with how many hits a QB has taken up to that point in a given game.\n\n\nchart <- hits_data %>% \n  group_by(hit_in_game,cum_hits) %>%\n  summarize(avg_epa = mean(epa), plays = n())\n\nMake the graph\nNow all that’s left to do is plot the data (with a huge thanks to R genius Josh Hornsby for helping make this look better)\n\n\nchart %>% \n  filter(cum_hits > 0 & cum_hits <=12 & !is.na(hit_in_game)) %>%\n  ggplot(aes(x = cum_hits, y = avg_epa, color = hit_in_game, shape = hit_in_game)) +\n    geom_jitter(aes(x = cum_hits, y = avg_epa, fill = hit_in_game), shape = 21, stroke = 1.25, size = 4, width = 0.1, show.legend=FALSE)+\n   geom_smooth(method = lm, se = FALSE) +\n   theme_minimal() +\n   theme(\n    legend.position = c(0.99, 0.99), \n    legend.justification = c(1, 1) ,\n    plot.title = element_text(size = 16, hjust = 0.5),\n    panel.grid.minor = element_blank())+ \n  ggsci::scale_color_locuszoom(name = \"Total Hits\\nIn-Game\") +\n  scale_y_continuous(name = \"EPA Per Dropback\", breaks = scales::pretty_breaks(n = 5))+\n  scale_x_continuous(breaks = 0:50, name = \"Cumulative QB Hits Suffered In Game So Far\")+\n  labs(title=\"QB hits versus QB efficiency\", caption = \"Data from nflfastR\")\n\n\nWell then. The negative relationship between QB hits and efficiency is because the group of teams that get hit often are the only ones to make it to the high numbers of hits. Stated this way, it sounds obvious, but it’s important. These teams aren’t necessarily inefficient because their QBs are getting hit a lot; but rather, their QBs are getting hit a lot because they’re bad teams to begin with.\nSide note: I’m not showing 0 hits because there’s a mechanical relationship between QB hits and efficiency. It is the one x-axis point that contains 0 hits, by definition, so of course EPA per play is higher: it’s a comparison of a set of plays with no QB hits to other sets of plays with QB hits. I also truncated the x-axis at 12 hits because anything higher is extremely rare.\nWrapping up\nLetting your QB get hit is bad. Obviously. Teams that allow more hits are less likely to have efficient offenses. But for a given level of hits, there is no evidence that the accumulation of hits makes any difference throughout the course of a game. The evidence suggests that we’ve found a variation of Brian Burke’s “passing paradox”:\nBurkeAs with the Rule of 53, the NFL has appeared to draw the wrong conclusions from a correlation driven by game state.\n\n\n",
    "preview": "posts/2020-08-19-the-accumulation-of-qb-hits-vs-passing-efficiency/the-accumulation-of-qb-hits-vs-passing-efficiency_files/figure-html5/unnamed-chunk-12-1.png",
    "last_modified": "2020-10-29T09:08:00+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 3300
  },
  {
    "path": "posts/2020-08-19-wins-above-expectation/",
    "title": "Wins Above Expectation",
    "description": "This article looks at the percentage of snaps with win probability over an \narbitralily chosen critical value and compares it with the true win percentage.",
    "author": [
      {
        "name": "Sebastian Carl",
        "url": "https://twitter.com/mrcaseb"
      }
    ],
    "date": "2020-08-19",
    "categories": [
      "Figures",
      "nflfastR"
    ],
    "contents": "\nTable of Contents\nPreface\nLoad nflfastR Play by Play and compute some helper columns\nCreate the plots\nPreface\nIn the NFL, practically everyone can beat anyone. So it often happens that games are tight until the very end and the winner is likely to have had some luck. Every year there are teams where you subjectively feel that they have lost or won particularly many of the aforementioned games.\nIn this post I will show off a very simple way to illustrate that by looking at how many snaps a team played with the nflfastR win probability (model with Vegas line) above a critical value (50%) more or less arbitrarily chosen by me and compare this value with the true win percentage.\nLoad nflfastR Play by Play and compute some helper columns\nSince we want to compute true win percentage from nflfastR play-by-play data we have to do a little data wrangling before we can create the plot.\n\n\nlibrary(tidyverse)\n\n# Parameter --------------------------------------------------------------------\n\nseason <- 2019\nwp_limit <- 0.5\n\n# Load the data ----------------------------------------------------------------\n\npbp <- readRDS(url(\n  glue::glue(\"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_{season}.rds\")\n)) %>%\n  filter(pass == 1 | rush == 1)\n\n# Compute outcomes and win percentage ------------------------------------------\n\noutcomes <- pbp %>%\n  group_by(season, game_id, home_team) %>%\n  summarise(\n    home_win = if_else(result > 0, 1, 0),\n    home_tie = if_else(result == 0, 1, 0)\n  ) %>%\n  group_by(season, home_team) %>%\n  summarise(\n    home_games = n(),\n    home_wins = sum(home_win),\n    home_ties = sum(home_tie)\n  ) %>%\n  ungroup() %>%\n  left_join(\n    # away games\n    pbp %>%\n      group_by(season, game_id, away_team) %>%\n      summarise(\n        away_win = if_else(result < 0, 1, 0),\n        away_tie = if_else(result == 0, 1, 0)\n      ) %>%\n      group_by(season, away_team) %>%\n      summarise(\n        away_games = n(),\n        away_wins = sum(away_win),\n        away_ties = sum(away_tie)\n      ) %>%\n      ungroup(),\n    by = c(\"season\", \"home_team\" = \"away_team\")\n  ) %>%\n  rename(team = \"home_team\") %>%\n  mutate(\n    games = home_games + away_games,\n    wins = home_wins + away_wins,\n    losses = games - wins,\n    ties = home_ties + away_ties,\n    win_percentage = (wins + 0.5 * ties) / games\n  ) %>%\n  select(\n    season, team, games, wins, losses, ties, win_percentage\n  )\n\n# Compute percentage of plays with wp > wp_lim ---------------------------------\n\nwp_combined <- pbp %>%\n  filter(!is.na(vegas_wp) & !is.na(posteam)) %>%\n  group_by(season, posteam) %>%\n  summarise(\n    pos_plays = n(),\n    pos_wp_lim_plays = sum(vegas_wp > wp_limit)\n  ) %>%\n  ungroup() %>%\n  left_join(\n    pbp %>%\n      filter(!is.na(vegas_wp) & !is.na(posteam)) %>%\n      group_by(season, defteam) %>%\n      summarise(\n        def_plays = n(),\n        def_wp_lim_plays = sum(vegas_wp < wp_limit)\n      ) %>%\n      ungroup(),\n    by = c(\"season\", \"posteam\" = \"defteam\")\n  ) %>%\n  rename(team = \"posteam\") %>%\n  mutate(\n    wp_lim_percentage = as.numeric(pos_wp_lim_plays + def_wp_lim_plays) / as.numeric(pos_plays + def_plays)\n  ) %>%\n  select(season, team, wp_lim_percentage)\n\n# Combine data and add colors and logos ----------------------------------------\n\nchart <- outcomes %>%\n  left_join(wp_combined, by = c(\"season\", \"team\")) %>%\n  filter(!is.na(wp_lim_percentage)) %>%\n  mutate(diff = 100 * (win_percentage - wp_lim_percentage)) %>%\n  group_by(team) %>%\n  summarise_all(mean) %>%\n  ungroup() %>%\n  inner_join(\n    nflfastR::teams_colors_logos %>% select(team_abbr, team_color, team_logo_espn, team_logo_wikipedia),\n    by = c(\"team\" = \"team_abbr\")\n  ) %>%\n  mutate(\n    grob = map(seq_along(team_logo_espn), function(x) {\n      grid::rasterGrob(magick::image_read(team_logo_espn[[x]]))\n    })\n  ) %>%\n  select(team, win_percentage, wp_lim_percentage, diff, team_color, grob) %>%\n  arrange(desc(diff))\n\nCreate the plots\nWe will create two separate plots. A scatterplot comparing true win percentage with the percentage of plays with win probability > 50% and a barplot showing the difference between the above variables.\n\n\n# Create scatterplot -----------------------------------------------------------\nchart %>%\n  ggplot(aes(x = wp_lim_percentage, y = win_percentage)) +\n  geom_abline(intercept = 0, slope = 1) +\n  geom_hline(aes(yintercept = mean(win_percentage)), color = \"red\", linetype = \"dashed\") +\n  geom_vline(aes(xintercept = mean(wp_lim_percentage)), color = \"red\", linetype = \"dashed\") +\n  ggpmisc::geom_grob(aes(x = wp_lim_percentage, y = win_percentage, label = grob), vp.width = 0.05) +\n  labs(\n    x = glue::glue(\"Percentage of snaps with win probability (vegas_wp) over {100 * wp_limit}%\"),\n    y = \"True win percentage (including ties as half a win)\",\n    title = \"NFL Team Efficiency\",\n    caption = \"Figure: @mrcaseb | Data: @nflfastR\"\n  ) +\n  ggthemes::theme_stata(scheme = \"sj\", base_size = 8) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    plot.caption = element_text(hjust = 1),\n    axis.text.y = element_text(angle = 0, vjust = 0.5),\n    legend.title = element_text(size = 8, hjust = 0, vjust = 0.5, face = \"bold\"),\n    legend.position = \"top\",\n    aspect.ratio = 1 / 1.618\n  ) +\n  NULL\n\n\n\n\n# Create bar plot  -------------------------------------------------------------\nchart %>%\n  ggplot(aes(x = seq_along(diff), y = diff)) +\n  geom_hline(aes(yintercept = mean(diff)), color = \"red\", linetype = \"dashed\") +\n  geom_col(width = 0.5, colour = chart$team_color, fill = chart$team_color, alpha = 0.5) +\n  ggpmisc::geom_grob(aes(x = seq_along(diff), y = diff, label = grob), vp.width = 0.035) +\n  # scale_x_continuous(expand = c(0,0)) +\n  labs(\n    x = \"Rank\",\n    y = \"Win Percentage Over Expectation\",\n    title = \"NFL Team Efficiency\",\n    subtitle = \"How Lucky are the Teams?\",\n    caption = \"Figure: @mrcaseb | Data: @nflfastR\"\n  ) +\n  ggthemes::theme_stata(scheme = \"sj\", base_size = 8) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    plot.caption = element_text(hjust = 1),\n    axis.text.y = element_text(angle = 0, vjust = 0.5),\n    legend.title = element_text(size = 8, hjust = 0, vjust = 0.5, face = \"bold\"),\n    legend.position = \"top\",\n    aspect.ratio = 1 / 1.618\n  ) +\n  NULL\n\n\n\n\n",
    "preview": "posts/2020-08-19-wins-above-expectation/wins-above-expectation_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2020-10-29T09:08:00+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 3000
  },
  {
    "path": "posts/2020-08-18-pfrs-bad-throw-percentage-for-quarterbacks/",
    "title": "PFR's Bad Throw Percentage for Quarterbacks",
    "description": "This article shows how to scrape football data from Pro Football Reference and\nhow to plot the bad throw percentage data for quarterbacks.",
    "author": [
      {
        "name": "Sebastian Carl",
        "url": "https://twitter.com/mrcaseb"
      }
    ],
    "date": "2020-08-18",
    "categories": [
      "Scraping",
      "PFR",
      "Figures",
      "nflfastR"
    ],
    "contents": "\nTable of Contents\nPreface\nGet the data and save it locally\nCreate the plot\nPreface\nOne of the most important sources for football related data is Pro Football Reference (short PFR). There are a ton of useful stats and I would like to look into their “Bad Throw Percentage” defined as\n\nPercentage of poor throws per pass attempt, excluding spikes and throwaways\n\nPFR provides it’s data on static html websites which makes it easy to scrape them. This will be demonstrated below.\nGet the data and save it locally\nScraping data means traffic for the provider. We don’t want to annoy them so we scrape them once and save them locally. This can be done with the following code (please note it saves only the for this example relevant variables and you might want to change that):\n\n\nlibrary(tidyverse)\nlibrary(rvest)\n\n# scrape data from PFR----------------------------------------------------------\nurl <- \"https://www.pro-football-reference.com/years/2019/passing_advanced.htm\"\npfr_raw <- url %>%\n  read_html() %>%\n  html_table() %>%\n  as.data.frame()\n\n# clean the scraped data--------------------------------------------------------\n\n# rename the columns as the actual column names are saved in the first row now\ncolnames(pfr_raw) <- make.names(pfr_raw[1, ], unique = TRUE, allow_ = TRUE)\n\n# drop the first row and select the columns we are interested in\npfr <- pfr_raw %>%\n  slice(-1) %>%\n  select(Player, Tm, IAY.PA, Bad., Att) %>%\n  rename(team = Tm) %>%\n  mutate(\n    # pfr uses different team abbreviations than nflfastR, fix them\n    team = case_when(\n      team == \"GNB\" ~ \"GB\",\n      team == \"KAN\" ~ \"KC\",\n      team == \"NOR\" ~ \"NO\",\n      team == \"NWE\" ~ \"NE\",\n      team == \"SFO\" ~ \"SF\",\n      team == \"TAM\" ~ \"TB\",\n      TRUE ~ team\n    ),\n    # repair player names\n    Player = str_replace(Player, \"\\\\*\", \"\"),\n    Player = str_replace(Player, \"\\\\+\", \"\"),\n\n    # make interesting columns numeric\n    IAY.PA = as.numeric(IAY.PA),\n    Bad. = as.numeric(str_replace(Bad., \"%\", \"\")),\n    Passattempts = as.numeric(Att)\n  ) %>%\n  # join colors and logos from nflfastR\n  left_join(nflfastR::teams_colors_logos, by = c(\"team\" = \"team_abbr\"))\n\n# save to disk------------------------------------------------------------------\n# binary\nsaveRDS(pfr, file = \"pfr_bad_throws.rds\")\n\n# ASCII\nwrite_csv(pfr, \"pfr_bad_throws.csv\")\n\nCreate the plot\nThe data we want to have a look at is now saved locally and can be used to create the plot:\n\n\nlibrary(tidyverse)\nchart_data <- readRDS(\"pfr_bad_throws.rds\") %>% filter(Passattempts > 180)\n\nchart_data %>%\n  ggplot(aes(x = IAY.PA, y = Bad. / 100)) +\n  geom_hline(aes(yintercept = mean(Bad. / 100)), color = \"red\", linetype = \"dotted\") +\n  geom_vline(aes(xintercept = mean(IAY.PA)), color = \"red\", linetype = \"dotted\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\", size = 0.3) +\n  geom_point(color = chart_data$team_color, aes(cex = Passattempts), alpha = 1 / 4) +\n  ggrepel::geom_text_repel(aes(label = Player), force = 1, point.padding = 0, segment.size = 0.1, size = 3) +\n  scale_y_continuous(labels = scales::percent) +\n  scale_size_area(max_size = 6) +\n  labs(\n    x = \"Average Depth of Target in Yards\",\n    y = \"Bad Throw Percentage\",\n    caption = \"Bad Throw Percentage = Percentage of throws that weren't catchable with normal effort, excluding spikes and throwaways\\nFigure: @mrcaseb | Data: @pfref\",\n    title = \"QB Passing Performance 2019\",\n    subtitle = \"We may see regression hitting Tannehill and Prescott in 2020\"\n  ) +\n  ggthemes::theme_stata(scheme = \"sj\", base_size = 8) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    plot.caption = element_text(hjust = 1),\n    axis.text.y = element_text(angle = 0, vjust = 0.5),\n    legend.title = element_text(size = 8, hjust = 0, vjust = 0.5, face = \"bold\"),\n    legend.position = \"top\",\n    aspect.ratio = 1 / 1.618\n  ) +\n  NULL\n\n\n\n\n",
    "preview": "posts/2020-08-18-pfrs-bad-throw-percentage-for-quarterbacks/pfrs-bad-throw-percentage-for-quarterbacks_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2020-10-29T09:08:00+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 3300
  }
]
